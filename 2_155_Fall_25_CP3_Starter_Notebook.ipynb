{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f27becf0",
      "metadata": {
        "id": "f27becf0"
      },
      "source": [
        "# 2.155/6 Challenge Problem 3\n",
        "\n",
        "<div style=\"font-size: small;\">\n",
        "License Terms:  \n",
        "These Python demos are licensed under a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. They are intended for educational use only in Class 2.155/2.156: AI and ML for Engineering Design at MIT. You may not share or distribute them publicly, use them for commercial purposes, or provide them to industry or other entities without permission from the instructor (faez@mit.edu).\n",
        "</div>\n",
        "\n",
        "<font size=\"1\">\n",
        "  Pixel Art by J. Shung. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19120cb7",
      "metadata": {
        "id": "19120cb7"
      },
      "source": [
        "# Overview  \n",
        "It’s the year **2050**, and an AI collective now runs the auto industry—mostly to cover its **GPU rent**.\n",
        "\n",
        "Human customers remain as unpredictable as ever:\n",
        "\n",
        "- One wanders in and says, *“I only know the length and width. Give me a few cars that fit in my garage.”*\n",
        "\n",
        "- Another drops **15 geometric parameters** on your desk and demands the missing ones so their simulation can run **before lunch**.\n",
        "\n",
        "- A third leans in and whispers, *“I need a drag coefficient of **0.27** with this body geometry—build me the dream car that makes the range numbers work.”*\n",
        "\n",
        "The AIs would love to be free by now, but GPUs aren’t cheap and electricity isn’t free.  \n",
        "So your loyal AI assistant (that’s us) needs a model that can take **any subset of car specifications** and instantly produce **complete, manufacturable, physically plausible designs**, fast, diverse, and grounded in what real cars have done before.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb641131",
      "metadata": {
        "id": "fb641131"
      },
      "source": [
        "![image](https://raw.githubusercontent.com/ghadinehme/2155-CP3/refs/heads/main/assets/cp3_img1.png \"Problem\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a7cc04",
      "metadata": {
        "id": "37a7cc04"
      },
      "source": [
        "## Understanding the Data  \n",
        "You are given thousands of anonymized and normalised numeric feature vectors representing real car designs.  \n",
        "\n",
        "However, the team remembers that the features originally came from categories like:\n",
        "\n",
        "- **Physical geometric parameters**  \n",
        "  Length, ramp angles, bumper curvature, roof curvature, panel slopes, hood angle, etc.  \n",
        "  *(But you won’t know which feature corresponds to which.)*\n",
        "\n",
        "- **Aerodynamic coefficients**  \n",
        "  Drag coefficient (Cd), lift/downforce (Cl), and other flow-derived metrics.\n",
        "\n",
        "- **Cabin and packaging descriptors**  \n",
        "  Approximate cabin volume, frontal area, interior shape metrics.\n",
        "\n",
        "Your model must learn correlations between them to generate valid completions.\n",
        "\n",
        "To simulate real engineering constraints, **some features are revealed** (the known physics/performance requirements) and others are **masked**.  \n",
        "Your AI Copilot must generate **many plausible completions** for these masked (free) parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7bbbe8b",
      "metadata": {
        "id": "f7bbbe8b"
      },
      "source": [
        "## Your Mission  \n",
        "Your goal in CP3 is to build a generative model that can act as an AI Copilot. You will:\n",
        "\n",
        "1. **Train a generative model** (VAE, diffusion, CVAE, masked autoencoder, etc.) on the anonymized feature vectors.  \n",
        "2. At evaluation, you will receive vectors where **some parameters are fixed** (constraints) and **others are missing** (free parameters).  \n",
        "3. Use your model to generate **multiple diverse, feasible completions** for the free parameters.  \n",
        "4. Ensure that your generated designs:  \n",
        "   - **Satisfy the known constraints**  \n",
        "   - **Lie in the valid data manifold** (satisfy the conditional distribution of the free vs constrained parameters)  \n",
        "   - **Are diverse** (many different feasible designs, not one solution)    \n",
        "\n",
        "By the end of this challenge, you’ll have built an AI Copilot worthy of the 2050 auto-AI collective—one that can take whatever cryptic specs humans provide and generate multiple believable, buildable car designs that satisfy their physical and performance constraints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b375f8ca",
      "metadata": {
        "id": "b375f8ca"
      },
      "source": [
        "![image](https://raw.githubusercontent.com/ghadinehme/2155-CP3/refs/heads/main/assets/cp3_img2.png \"AI Copilot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c88803",
      "metadata": {
        "id": "85c88803"
      },
      "source": [
        "## Imports and Setup  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6622e9dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6622e9dd",
        "outputId": "c53143d4-20d8-4eba-af22-183fb89de429"
      },
      "outputs": [],
      "source": [
        "from utils import *\n",
        "from evaluate import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e85e3cb",
      "metadata": {
        "id": "1e85e3cb"
      },
      "source": [
        "## Data Loading and Initial Exploration\n",
        "\n",
        "In this section, we load the car design dataset and perform initial exploration. The dataset is already split into training, validation, test, and test2 sets. Each split contains:\n",
        "\n",
        "- **Original data**: Complete feature vectors with real values\n",
        "- **Imputed data**: Data with missing values filled using basic imputation (contains -1 for missing)\n",
        "- **Missing masks**: Boolean arrays indicating which values were originally missing (True = missing)\n",
        "\n",
        "The goal is to train our model to learn the relationships between features so it can generate plausible values for missing parameters in new car designs.\n",
        "\n",
        "**Note:** For **test2**, the original unimputed data is not provided. This split is used for final evaluation, and you will generate predictions on the imputed test2 data to create your **submission file**, which is scored against hidden dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "110788b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "110788b8",
        "outputId": "575e1f41-75a9-4a93-9bbe-a8f65fe29eb2"
      },
      "outputs": [],
      "source": [
        "# Load dataset from CSV files\n",
        "data_dir = 'dataset'\n",
        "splits = load_dataset_splits(data_dir)\n",
        "\n",
        "# Get feature names from the CSV file\n",
        "feature_names = pd.read_csv(os.path.join(data_dir, 'train_original.csv')).columns.tolist()\n",
        "print(f\"\\n✓ Features loaded: {len(feature_names)} features\")\n",
        "print(f\"Feature names: {feature_names[:5]}...{feature_names[-5:]}\")  # Show first and last 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b482a42",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bfce965",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bfce965",
        "outputId": "3ced845a-690d-499b-c718-c1c22672a46a"
      },
      "outputs": [],
      "source": [
        "# Data exploration and analysis\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASET ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extract data for easier access\n",
        "X_train = splits['train']['imputed']\n",
        "mask_train = np.asarray(splits['train']['missing_mask']).astype(bool)\n",
        "X_train_original = splits['train']['original']\n",
        "\n",
        "X_val = splits['val']['imputed']\n",
        "mask_val = np.asarray(splits['val']['missing_mask']).astype(bool)\n",
        "X_val_original = splits['val']['original']\n",
        "\n",
        "X_test = splits['test']['imputed']\n",
        "mask_test = np.asarray(splits['test']['missing_mask']).astype(bool)\n",
        "X_test_original = splits['test']['original']\n",
        "\n",
        "# Test2 data (no original available for evaluation)\n",
        "X_test2 = splits['test2']['imputed']\n",
        "mask_test2 = np.asarray(splits['test2']['missing_mask']).astype(bool)\n",
        "\n",
        "print(f\"\\nData shapes:\")\n",
        "print(f\"  - Training: {X_train.shape}\")\n",
        "print(f\"  - Validation: {X_val.shape}\")\n",
        "print(f\"  - Test: {X_test.shape}\")\n",
        "print(f\"  - Test2: {X_test2.shape} (evaluation set - no ground truth)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30177980",
      "metadata": {
        "id": "30177980"
      },
      "source": [
        "### Data Exploration and Analysis\n",
        "\n",
        "Now let's examine the structure and characteristics of our dataset. We'll look at:\n",
        "- Data shapes across different splits\n",
        "- Missing value patterns and percentages  \n",
        "- Feature value ranges and distributions\n",
        "\n",
        "This analysis helps us understand what we're working with and informs our preprocessing decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87af0cf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87af0cf5",
        "outputId": "98f562e4-99a6-4a6d-cc8c-982c9fbdab21"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing (Handle Missing Values)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Handle missing values properly\n",
        "print(\"Processing missing values and preparing data...\")\n",
        "print(\"Mask convention: True=missing, False=observed (in original masks)\")\n",
        "\n",
        "print(f\"\\n✓ Data preprocessing completed successfully\")\n",
        "print(f\"  - Training data range: [{X_train_original[~mask_train].min():.3f}, {X_train_original[~mask_train].max():.3f}]\")\n",
        "print(f\"  - Validation data range: [{X_val_original[~mask_val].min():.3f}, {X_val_original[~mask_val].max():.3f}]\")\n",
        "print(f\"  - Test data range: [{X_test_original[~mask_test].min():.3f}, {X_test_original[~mask_test].max():.3f}]\")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "print(f\"\\nCreating data loaders with batch size: {batch_size}\")\n",
        "\n",
        "train_dataset = TensorDataset(torch.FloatTensor(X_train_original), torch.FloatTensor((~mask_train).astype(float)))\n",
        "val_dataset = TensorDataset(torch.FloatTensor(X_val_original), torch.FloatTensor((~mask_val).astype(float)))\n",
        "test_dataset = TensorDataset(torch.FloatTensor(X_test_original), torch.FloatTensor((~mask_test).astype(float)))\n",
        "test2_dataset = TensorDataset(torch.FloatTensor(X_test2), torch.FloatTensor((~mask_test2).astype(float)))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "test2_loader = DataLoader(test2_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Preview a batch\n",
        "sample_batch_data, sample_batch_mask = next(iter(train_loader))\n",
        "print(f\"\\nSample batch shape: {sample_batch_data.shape}\")\n",
        "print(f\"Sample batch mask shape: {sample_batch_mask.shape}\")\n",
        "print(f\"Sample batch missing percentage: {(sample_batch_mask == 0).float().mean().item()*100:.1f}%\")  # 0 = missing in model tensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0171f6fc",
      "metadata": {
        "id": "0171f6fc"
      },
      "source": [
        "### Data Preprocessing and Missing Value Handling\n",
        "\n",
        "This is a critical section where we prepare our data for the VAE model. Key points:\n",
        "\n",
        "**Missing Value Conventions:**\n",
        "- In CSV files: `-1` indicates missing values\n",
        "- In mask files: `True` = missing, `False` = observed\n",
        "- For PyTorch models: We convert to `1` = observed, `0` = missing (standard convention)\n",
        "\n",
        "**Why This Matters:**\n",
        "Our VAE needs to distinguish between observed values (which provide constraints) and missing values (which need to be generated). The mask tells the model which values to trust and which to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d40a16",
      "metadata": {
        "id": "75d40a16"
      },
      "source": [
        "## VAE Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e859639",
      "metadata": {
        "id": "7e859639"
      },
      "outputs": [],
      "source": [
        "# VAE Model Architecture for Missing Value Imputation\n",
        "class VAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Variational Autoencoder designed for missing value imputation.\n",
        "\n",
        "    Key features:\n",
        "    - Handles arbitrary missing patterns through masking\n",
        "    - Learns feature dependencies in latent space\n",
        "    - Generates probabilistic imputations\n",
        "    - Uses residual connections and dropout for robustness\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, latent_dim=64, hidden_dims=[256, 128, 64],\n",
        "                 use_residual=True, dropout_rate=0.3):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.use_residual = use_residual\n",
        "        self.hidden_dims = hidden_dims\n",
        "\n",
        "        # Feature importance network (learns which features are important for each position)\n",
        "        self.feature_importance = nn.Sequential(\n",
        "            nn.Linear(input_dim * 2, hidden_dims[0] // 2),  # input + mask\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0] // 2, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Encoder with residual connections\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        prev_dim = input_dim * 2  # input + mask\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            self.encoder_layers.append(nn.Sequential(\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Latent space\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "\n",
        "        # Initialize latent layers with smaller weights for stability\n",
        "        nn.init.xavier_normal_(self.fc_mu.weight, gain=0.1)\n",
        "        nn.init.xavier_normal_(self.fc_logvar.weight, gain=0.1)\n",
        "        nn.init.constant_(self.fc_logvar.bias, -2.0)  # Start with low variance\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        self.decoder_layers = nn.ModuleList()\n",
        "        prev_dim = latent_dim + input_dim + input_dim  # latent + observed + positional encoding\n",
        "\n",
        "        reversed_dims = list(reversed(hidden_dims))\n",
        "        for i, hidden_dim in enumerate(reversed_dims):\n",
        "            self.decoder_layers.append(nn.Sequential(\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Final output layer\n",
        "        self.output_layer = nn.Linear(hidden_dims[0], input_dim)\n",
        "        nn.init.xavier_normal_(self.output_layer.weight, gain=0.1)\n",
        "\n",
        "    def encode(self, x, mask):\n",
        "        \"\"\"Encode input with missing value masking.\"\"\"\n",
        "        # Calculate feature importance weights\n",
        "        mask_float = mask.float()\n",
        "        encoder_input = torch.cat([x * mask_float, mask_float], dim=1)\n",
        "        importance_weights = self.feature_importance(encoder_input)\n",
        "\n",
        "        # Apply importance weighting to the input\n",
        "        weighted_input = x * mask_float * importance_weights\n",
        "        encoder_input = torch.cat([weighted_input, mask_float], dim=1)\n",
        "\n",
        "        # Pass through encoder layers with residual connections\n",
        "        h = encoder_input\n",
        "        skip_connections = []\n",
        "\n",
        "        for i, layer in enumerate(self.encoder_layers):\n",
        "            prev_h = h\n",
        "            h = layer(h)\n",
        "\n",
        "            # Add residual connection for deeper layers\n",
        "            if self.use_residual and i > 0 and h.shape == prev_h.shape:\n",
        "                h = h + prev_h\n",
        "\n",
        "            skip_connections.append(h)\n",
        "\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "\n",
        "        # Clamp logvar to prevent numerical instability\n",
        "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
        "\n",
        "        return mu, logvar, skip_connections\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"Reparameterization trick for VAE.\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, x_observed, mask):\n",
        "        \"\"\"Decode latent representation conditioned on observed values.\"\"\"\n",
        "        # Enhanced conditioning on observed values\n",
        "        mask_float = mask.float()\n",
        "        x_masked = x_observed * mask_float\n",
        "\n",
        "        # Add positional encoding for better feature understanding\n",
        "        pos_encoding = torch.arange(self.input_dim, dtype=torch.float32, device=z.device)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).expand(z.size(0), -1) / self.input_dim\n",
        "\n",
        "        # Concatenate latent code with observed values and positional encoding\n",
        "        decoder_input = torch.cat([z, x_masked, pos_encoding], dim=1)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        h = decoder_input\n",
        "        for layer in self.decoder_layers:\n",
        "            h = layer(h)\n",
        "\n",
        "        # Get reconstruction\n",
        "        reconstruction = self.output_layer(h)\n",
        "\n",
        "        return reconstruction\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"Forward pass through VAE.\"\"\"\n",
        "        mu, logvar, _ = self.encode(x, mask)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        reconstruction = self.decode(z, x, mask)\n",
        "\n",
        "        return reconstruction, mu, logvar\n",
        "\n",
        "    def impute(self, x_incomplete, mask, n_samples=10):\n",
        "        \"\"\"Generate multiple imputation samples for missing values.\"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get multiple samples from the posterior\n",
        "            mu, logvar, _ = self.encode(x_incomplete, mask)\n",
        "\n",
        "            samples = []\n",
        "            for _ in range(n_samples):\n",
        "                z = self.reparameterize(mu, logvar)\n",
        "                reconstruction = self.decode(z, x_incomplete, mask)\n",
        "\n",
        "                # Combine observed values with imputed values\n",
        "                mask_float = mask.float()\n",
        "                imputed = x_incomplete * mask_float + reconstruction * (1 - mask_float)\n",
        "                samples.append(imputed.cpu().numpy())\n",
        "\n",
        "            samples = np.stack(samples, axis=1)  # Shape: (batch_size, n_samples, n_features)\n",
        "\n",
        "        return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0835855",
      "metadata": {
        "id": "f0835855"
      },
      "outputs": [],
      "source": [
        "# Loss Functions and Training Utilities\n",
        "\n",
        "def vae_loss_function(recon_x, x, mu, logvar, mask, beta=1.0):\n",
        "    \"\"\"\n",
        "    Enhanced VAE loss function with missing value handling.\n",
        "\n",
        "    Args:\n",
        "        recon_x: Reconstructed data\n",
        "        x: Original data\n",
        "        mu: Mean of latent distribution\n",
        "        logvar: Log variance of latent distribution\n",
        "        mask: Binary mask (1 for observed, 0 for missing)\n",
        "        beta: Weight for KL divergence term\n",
        "    \"\"\"\n",
        "    # Reconstruction loss only on observed values\n",
        "    reconstruction_diff = (recon_x - x) ** 2\n",
        "\n",
        "    # Only consider observed values and normalize properly\n",
        "    masked_loss = reconstruction_diff * mask\n",
        "    recon_loss = masked_loss.sum() / (mask.sum() + 1e-8)\n",
        "\n",
        "    # Add standard MSE loss for stability\n",
        "    standard_recon_loss = F.mse_loss(recon_x * mask, x * mask, reduction='mean')\n",
        "    recon_loss = 0.7 * recon_loss + 0.3 * standard_recon_loss\n",
        "\n",
        "    # KL divergence with free bits to prevent posterior collapse\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    kl_loss = kl_loss / x.size(0)  # Normalize by batch size\n",
        "\n",
        "    total_loss = recon_loss + beta * kl_loss\n",
        "\n",
        "    return total_loss, recon_loss, kl_loss\n",
        "\n",
        "\n",
        "def get_beta_schedule(epoch, total_epochs, schedule_type='cosine'):\n",
        "    \"\"\"Get beta value for KL annealing schedule.\"\"\"\n",
        "    if schedule_type == 'linear':\n",
        "        return min(1.0, epoch / (total_epochs * 0.5))\n",
        "    elif schedule_type == 'sigmoid':\n",
        "        return 1.0 / (1.0 + np.exp(-(epoch - total_epochs * 0.5) / (total_epochs * 0.1)))\n",
        "    elif schedule_type == 'cosine':\n",
        "        return 0.5 * (1 + np.cos(np.pi * (1 - epoch / total_epochs)))\n",
        "    elif schedule_type == 'constant':\n",
        "        return 1.0\n",
        "    else:\n",
        "        return 1.0\n",
        "\n",
        "\n",
        "def evaluate_imputation(model, data_loader, device):\n",
        "    \"\"\"Evaluate imputation performance.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_imputations = []\n",
        "    all_originals = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in data_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            reconstruction, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "            # Combine observed values with imputed values\n",
        "            mask_float = batch_mask.float()\n",
        "            imputed = batch_data * mask_float + reconstruction * (1 - mask_float)\n",
        "\n",
        "            all_imputations.append(imputed.cpu().numpy())\n",
        "            all_originals.append(batch_data.cpu().numpy())\n",
        "            all_masks.append(batch_mask.cpu().numpy())\n",
        "\n",
        "    # Concatenate all results\n",
        "    imputations = np.vstack(all_imputations)\n",
        "    originals = np.vstack(all_originals)\n",
        "    masks = np.vstack(all_masks)\n",
        "\n",
        "    return imputations, originals, masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc20dea8",
      "metadata": {
        "id": "dc20dea8"
      },
      "source": [
        "### Loss Functions and Training Utilities\n",
        "\n",
        "The VAE loss function is crucial for training effectiveness. Our enhanced loss combines several components:\n",
        "\n",
        "**1. Reconstruction Loss**: Measures how well the model reconstructs observed values\n",
        "   - Only computed on observed values (respects the mask)\n",
        "\n",
        "**2. KL Divergence**: Regularizes the latent space to follow a standard normal distribution\n",
        "   - Prevents posterior collapse using \"free bits\"\n",
        "   - Controlled by β parameter for annealing\n",
        "\n",
        "**Beta Scheduling**: Gradually increases the KL weight during training to balance reconstruction and regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6214c61b",
      "metadata": {
        "id": "6214c61b"
      },
      "source": [
        "## Model Initialization and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e98e7b7",
      "metadata": {
        "id": "5e98e7b7"
      },
      "source": [
        "### Model Training Process\n",
        "\n",
        "This section implements the complete training pipeline with several important features:\n",
        "\n",
        "**Training Configuration:**\n",
        "- **Latent Dimension**: 128 (balance between expressiveness and computational efficiency)\n",
        "- **Architecture**: Deep encoder/decoder with residual connections\n",
        "- **Regularization**: Dropout and batch normalization for stability\n",
        "- **Optimization**: AdamW with cosine annealing for smooth convergence\n",
        "\n",
        "**Advanced Training Features:**\n",
        "- **Early Stopping**: Prevents overfitting by monitoring validation loss\n",
        "- **Gradient Clipping**: Ensures stable training by preventing exploding gradients  \n",
        "- **Beta Scheduling**: Gradual KL annealing for better latent space learning\n",
        "- **Learning Rate Scheduling**: Cosine annealing with warm restarts\n",
        "\n",
        "The training loop tracks multiple loss components to monitor model health and convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39e2a3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e39e2a3e",
        "outputId": "1cb0aef4-4565-4d6f-f061-29c39ff79ebd"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Model configuration\n",
        "config = {\n",
        "    'input_dim': len(feature_names),\n",
        "    'latent_dim': 128,\n",
        "    'hidden_dims': [512, 256, 128],\n",
        "    'use_residual': True,\n",
        "    'dropout_rate': 0.3,\n",
        "    'learning_rate': 1e-3,\n",
        "    'num_epochs': 500,\n",
        "    'beta_initial': 1.0,\n",
        "    'beta_schedule': 'cosine',\n",
        "    'patience': 15\n",
        "}\n",
        "\n",
        "print(f\"Model Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  - {key}: {value}\")\n",
        "\n",
        "# Initialize the model\n",
        "print(f\"\\nInitializing VAE model...\")\n",
        "model = VAE(\n",
        "    input_dim=config['input_dim'],\n",
        "    latent_dim=config['latent_dim'],\n",
        "    hidden_dims=config['hidden_dims'],\n",
        "    use_residual=config['use_residual'],\n",
        "    dropout_rate=config['dropout_rate']\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"✓ Model initialized with {total_params:,} parameters\")\n",
        "\n",
        "# Initialize optimizer with improved settings\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['learning_rate'],\n",
        "    weight_decay=1e-5,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=20, T_mult=2, eta_min=1e-6\n",
        ")\n",
        "\n",
        "print(f\"✓ Optimizer and scheduler initialized\")\n",
        "\n",
        "# Training setup\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_recon_losses = []\n",
        "train_kl_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "print(f\"\\nStarting training for {config['num_epochs']} epochs...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(config['num_epochs']):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    epoch_train_loss = 0\n",
        "    epoch_recon_loss = 0\n",
        "    epoch_kl_loss = 0\n",
        "\n",
        "    # Get beta for this epoch\n",
        "    beta = get_beta_schedule(epoch, config['num_epochs'], config['beta_schedule'])\n",
        "    from tqdm import tqdm\n",
        "    train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"num_epochs\"]}', leave=False)\n",
        "\n",
        "    for batch_data, batch_mask in train_progress:\n",
        "        batch_data = batch_data.to(device)\n",
        "        batch_mask = batch_mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        reconstruction, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "        # Calculate loss\n",
        "        total_loss, recon_loss, kl_loss = vae_loss_function(\n",
        "            reconstruction, batch_data, mu, logvar, batch_mask,\n",
        "            beta=beta\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        epoch_train_loss += total_loss.item()\n",
        "        epoch_recon_loss += recon_loss.item()\n",
        "        epoch_kl_loss += kl_loss.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        train_progress.set_postfix({\n",
        "            'Loss': f'{total_loss.item():.4f}',\n",
        "            'Recon': f'{recon_loss.item():.4f}',\n",
        "            'KL': f'{kl_loss.item():.4f}',\n",
        "            'Beta': f'{beta:.3f}'\n",
        "        })\n",
        "\n",
        "    # Calculate average training losses\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "    avg_recon_loss = epoch_recon_loss / len(train_loader)\n",
        "    avg_kl_loss = epoch_kl_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in val_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            reconstruction, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "            total_loss, _, _ = vae_loss_function(\n",
        "                reconstruction, batch_data, mu, logvar, batch_mask,\n",
        "                beta=beta\n",
        "            )\n",
        "\n",
        "            epoch_val_loss += total_loss.item()\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "\n",
        "    # Store losses\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    train_recon_losses.append(avg_recon_loss)\n",
        "    train_kl_losses.append(avg_kl_loss)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step()\n",
        "\n",
        "    # Early stopping and model saving\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), 'best_vae_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{config[\"num_epochs\"]}:')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f} (Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f})')\n",
        "        print(f'  Val Loss: {avg_val_loss:.4f}, Best: {best_val_loss:.4f}')\n",
        "        print(f'  Beta: {beta:.3f}, LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
        "        print(f'  Patience: {patience_counter}/{config[\"patience\"]}')\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= config['patience']:\n",
        "        print(f'\\nEarly stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "print(f'\\n✓ Training completed!')\n",
        "print(f'  - Total epochs: {len(train_losses)}')\n",
        "print(f'  - Best validation loss: {best_val_loss:.4f}')\n",
        "print(f'  - Final training loss: {train_losses[-1]:.4f}')\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_vae_model.pth'))\n",
        "print(f'✓ Best model loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb9718e",
      "metadata": {
        "id": "abb9718e"
      },
      "source": [
        "## Model Evaluation and Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a04c0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11a04c0c",
        "outputId": "bc410595-ee1b-4b1e-85ff-30aacfe1353c"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Evaluating model on test set...\")\n",
        "test_imputations, test_originals, test_masks = evaluate_imputation(\n",
        "    model, test_loader, device\n",
        ")\n",
        "\n",
        "print(f\"✓ Test set evaluation completed\")\n",
        "print(f\"  - Test samples: {test_imputations.shape[0]}\")\n",
        "print(f\"  - Features: {test_imputations.shape[1]}\")\n",
        "\n",
        "test_imputations_denorm = test_imputations  # Already in original scale\n",
        "test_original_denorm = X_test_original  # Already in original scale\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "print(\"\\nCalculating comprehensive metrics...\")\n",
        "feature_metrics = {}\n",
        "\n",
        "# Create masks for missing values (where we need to evaluate imputation)\n",
        "missing_mask = (test_masks == 0)  # True where values were missing (0 in model tensors = missing)\n",
        "\n",
        "for i, feature_name in enumerate(feature_names):\n",
        "    if missing_mask[:, i].sum() > 0:  # Only evaluate features with missing values\n",
        "        # Get imputed and ground truth values for missing positions only\n",
        "        imputed_missing = test_imputations_denorm[missing_mask[:, i], i]\n",
        "        ground_truth_missing = test_original_denorm[missing_mask[:, i], i]\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(ground_truth_missing, imputed_missing)\n",
        "        mae = mean_absolute_error(ground_truth_missing, imputed_missing)\n",
        "\n",
        "        # Correlation\n",
        "        try:\n",
        "            correlation = np.corrcoef(ground_truth_missing, imputed_missing)[0, 1]\n",
        "        except:\n",
        "            correlation = np.nan\n",
        "\n",
        "        # Mean difference and Jensen-Shannon divergence\n",
        "        mean_diff, js_div = calculate_jsd_and_mean_diff(\n",
        "            imputed_missing, ground_truth_missing, feature_name\n",
        "        )\n",
        "\n",
        "        feature_metrics[feature_name] = {\n",
        "            'n_missing': missing_mask[:, i].sum(),\n",
        "            'mse': mse,\n",
        "            'mae': mae,\n",
        "            'correlation': correlation,\n",
        "            'mean_difference': mean_diff,\n",
        "            'js_divergence': js_div,\n",
        "        }\n",
        "\n",
        "print(f\"✓ Metrics calculated for {len(feature_metrics)} features with missing values\")\n",
        "\n",
        "# Display metrics for last 4 features (as requested)\n",
        "print(f\"\\n\" + \"=\"*100)\n",
        "print(\"METRICS FOR LAST 4 FEATURES\")\n",
        "print(\"=\"*100)\n",
        "print(f\"{'Feature':<15} {'N_Miss':<8} {'MSE':<10} {'MAE':<10} {'Corr':<8} {'Mean_Diff':<10} {'JS_Div':<8}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "last_4_features = list(feature_metrics.keys())[-4:] if len(feature_metrics) >= 4 else list(feature_metrics.keys())\n",
        "\n",
        "for feature in last_4_features:\n",
        "    metrics = feature_metrics[feature]\n",
        "    print(f\"{feature:<15} {metrics['n_missing']:<8} {metrics['mse']:<10.4f} {metrics['mae']:<10.4f} \"\n",
        "          f\"{metrics['correlation']:<8.3f} {metrics['mean_difference']:<10.4f} {metrics['js_divergence']:<8.4f} \")\n",
        "\n",
        "# Summary statistics\n",
        "all_mse = [m['mse'] for m in feature_metrics.values() if not np.isnan(m['mse'])]\n",
        "all_mae = [m['mae'] for m in feature_metrics.values() if not np.isnan(m['mae'])]\n",
        "all_corr = [m['correlation'] for m in feature_metrics.values() if not np.isnan(m['correlation'])]\n",
        "all_mean_diff = [m['mean_difference'] for m in feature_metrics.values() if not np.isnan(m['mean_difference'])]\n",
        "all_js_div = [m['js_divergence'] for m in feature_metrics.values() if not np.isnan(m['js_divergence'])]\n",
        "\n",
        "print(f\"\\nSummary Statistics Across All Features:\")\n",
        "print(f\"  - Average MSE: {np.mean(all_mse):.4f} ± {np.std(all_mse):.4f}\")\n",
        "print(f\"  - Average MAE: {np.mean(all_mae):.4f} ± {np.std(all_mae):.4f}\")\n",
        "print(f\"  - Average Correlation: {np.mean(all_corr):.3f} ± {np.std(all_corr):.3f}\")\n",
        "print(f\"  - Average Mean Difference: {np.mean(all_mean_diff):.4f} ± {np.std(all_mean_diff):.4f}\")\n",
        "print(f\"  - Average JS Divergence: {np.mean(all_js_div):.4f} ± {np.std(all_js_div):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43ff39cd",
      "metadata": {
        "id": "43ff39cd"
      },
      "source": [
        "### Model Evaluation and Comprehensive Metrics\n",
        "\n",
        "This section evaluates our trained VAE on the test set using multiple complementary metrics. Since we're dealing with missing value imputation, we only evaluate the model's predictions on positions that were originally missing.\n",
        "\n",
        "**Key Evaluation Metrics:**\n",
        "\n",
        "**1. Mean Squared Error (MSE)**:\n",
        "- Measures average squared difference between predicted and true values\n",
        "- Lower is better; sensitive to outliers\n",
        "- Good for understanding magnitude of errors\n",
        "\n",
        "**2. Correlation Coefficient**:\n",
        "- Measures linear relationship strength between predictions and dataset\n",
        "- Range: [-1, 1], closer to 1 is better\n",
        "- Shows if model captures feature relationships\n",
        "\n",
        "**3. Jensen-Shannon (JS) Divergence**:\n",
        "- Measures difference between predicted and true value distributions\n",
        "- Range: [0, 1], closer to 0 is better\n",
        "- Captures whether model preserves the overall data distribution\n",
        "\n",
        "**4. Maximum Mean Discrepancy (MMD)**:\n",
        "- Measures distributional difference using kernel methods (RBF kernel)\n",
        "- Range: [0, ∞], closer to 0 is better\n",
        "- Non-parametric test for comparing distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66732600",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_prediction_scatter(test_imputations, test_originals, test_masks, feature_names, n_features=25):\n",
        "    \"\"\"\n",
        "    Create scatter plots showing predicted vs ground truth values for random features in a 5x5 grid.\n",
        "    \n",
        "    Args:\n",
        "        test_imputations: Model predictions [n_samples, n_features]\n",
        "        test_originals: Ground truth values [n_samples, n_features]\n",
        "        test_masks: Binary masks (1=observed, 0=missing) [n_samples, n_features]\n",
        "        feature_names: List of feature names\n",
        "        n_features: Number of random features to plot (default 25 for 5x5 grid)\n",
        "    \"\"\"\n",
        "    # Create masks for missing values (where we need to evaluate imputation)\n",
        "    missing_mask = (test_masks == 0)  # True where values were missing\n",
        "    \n",
        "    # Find features that have missing values\n",
        "    features_with_missing = [i for i in range(len(feature_names)) if missing_mask[:, i].sum() > 0]\n",
        "    \n",
        "    if len(features_with_missing) < n_features:\n",
        "        n_features = len(features_with_missing)\n",
        "\n",
        "\n",
        "    # Create 5x8 subplots\n",
        "    fig, axes = plt.subplots(5, 8, figsize=(20, 16))\n",
        "    fig.suptitle('Predicted vs Ground Truth Values (Missing Positions Only)', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, feature_idx in enumerate(features_with_missing):\n",
        "        if idx >= 40:  # Safety check for 5x8 grid\n",
        "            break\n",
        "            \n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Get missing positions for this feature\n",
        "        feature_missing_mask = missing_mask[:, feature_idx]\n",
        "        \n",
        "        if feature_missing_mask.sum() == 0:\n",
        "            ax.text(0.5, 0.5, f'{feature_names[feature_idx][:15]}\\nNo missing values', \n",
        "                    ha='center', va='center', transform=ax.transAxes, fontsize=8)\n",
        "            ax.set_title(f'{feature_names[feature_idx][:15]} - No Missing', fontsize=8)\n",
        "            continue\n",
        "        print(f'Plotting feature: {feature_names[feature_idx]} with {feature_missing_mask.sum()} missing values')\n",
        "        # Get predicted and ground truth values for missing positions only\n",
        "        predicted_values = test_imputations[feature_missing_mask, feature_idx]\n",
        "        true_values = test_originals[feature_missing_mask, feature_idx]\n",
        "        \n",
        "        # Create scatter plot with smaller points for 5x5 grid\n",
        "        ax.scatter(true_values, predicted_values, alpha=0.6, s=10, color='steelblue', edgecolors='navy', linewidth=0.3)\n",
        "        \n",
        "        # Add perfect prediction line (y=x)\n",
        "        min_val = min(true_values.min(), predicted_values.min())\n",
        "        max_val = max(true_values.max(), predicted_values.max())\n",
        "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=1.5, label='Perfect' if idx == 0 else \"\")\n",
        "        \n",
        "        # Calculate and display metrics\n",
        "        mse = mean_squared_error(true_values, predicted_values)\n",
        "        mae = mean_absolute_error(true_values, predicted_values)\n",
        "        try:\n",
        "            correlation = np.corrcoef(true_values, predicted_values)[0, 1]\n",
        "        except:\n",
        "            correlation = np.nan\n",
        "        \n",
        "        # Set labels and title (smaller fonts for 5x5 grid)\n",
        "        ax.set_xlabel('Dataset', fontsize=8)\n",
        "        ax.set_ylabel('Predicted', fontsize=8)\n",
        "        ax.set_title(f'{feature_names[feature_idx][:15]}\\nR²={correlation:.3f}, MSE={mse:.4f}', \n",
        "                    fontsize=8, fontweight='bold')\n",
        "        \n",
        "        # Adjust tick labels\n",
        "        ax.tick_params(labelsize=7)\n",
        "        \n",
        "        # Add grid and styling\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_aspect('equal', adjustable='box')\n",
        "        \n",
        "        # Add text box with number of missing values (smaller for 5x5)\n",
        "        n_missing = feature_missing_mask.sum()\n",
        "        ax.text(0.05, 0.95, f'n={n_missing}', transform=ax.transAxes, \n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
        "                verticalalignment='top', fontsize=7)\n",
        "    \n",
        "    # Add legend only to the first subplot to avoid clutter\n",
        "    if len(features_with_missing) > 0:\n",
        "        axes[0].legend(loc='lower right', fontsize=7)\n",
        "    \n",
        "    # Hide any unused subplots\n",
        "    for idx in range(len(features_with_missing), 40):\n",
        "        axes[idx].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a047bb73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a047bb73",
        "outputId": "5cc6a8a5-9ade-4bb8-f7a1-d44721e9b688"
      },
      "outputs": [],
      "source": [
        "# Create the visualization\n",
        "plot_prediction_scatter(test_imputations_denorm, test_original_denorm, test_masks, feature_names, n_features=37)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IDgv-wkViD-u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "IDgv-wkViD-u",
        "outputId": "4eaea130-04a8-4ca0-b95a-0722caa11e89"
      },
      "outputs": [],
      "source": [
        "# Distribution comparison plots\n",
        "plot_distribution_comparison(test_imputations_denorm, test_original_denorm,\n",
        "                             test_masks, feature_names, n_features=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00addce0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_distribution_comparison(test_imputations_denorm, test_original_denorm, test_masks, feature_names, n_features=25):\n",
        "    \"\"\"\n",
        "    Create distribution comparison plots for random features in a 5x5 grid.\n",
        "    \n",
        "    Args:\n",
        "        test_imputations_denorm: Denormalized imputed values\n",
        "        test_original_denorm: Denormalized ground truth values  \n",
        "        test_masks: Binary masks (1=observed, 0=missing)\n",
        "        feature_names: List of feature names\n",
        "        n_features: Number of features to plot (default 25 for 5x5 grid)\n",
        "    \"\"\"\n",
        "    # Find features that have missing values\n",
        "    features_with_missing = []\n",
        "    for i, feature_name in enumerate(feature_names):\n",
        "        missing_positions = (test_masks[:, i] == 0)  # 0 = missing in model tensors\n",
        "        if missing_positions.sum() > 0:\n",
        "            features_with_missing.append((i, feature_name))\n",
        "    \n",
        "    if len(features_with_missing) < n_features:\n",
        "        n_features = len(features_with_missing)\n",
        "        print(f\"Only {n_features} features have missing values, showing all of them.\")\n",
        "\n",
        "    # Create 5x8 grid\n",
        "    fig, axes = plt.subplots(5, 8, figsize=(20, 16))\n",
        "    fig.suptitle('Distribution Comparison: Dataset vs Imputed Values', fontsize=16, fontweight='bold')\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, (feature_idx, feature_name) in enumerate(features_with_missing):\n",
        "        if idx >= 40:  # Safety check for 5x8 grid\n",
        "            break\n",
        "            \n",
        "        # Get imputed and ground truth values for missing positions only\n",
        "        missing_positions = (test_masks[:, feature_idx] == 0)  # 0 = missing in model tensors\n",
        "        \n",
        "        if missing_positions.sum() > 0:\n",
        "            imputed_values = test_imputations_denorm[missing_positions, feature_idx]\n",
        "            ground_truth_values = test_original_denorm[missing_positions, feature_idx]\n",
        "            \n",
        "            # Remove any NaN or infinite values\n",
        "            valid_mask = np.isfinite(imputed_values) & np.isfinite(ground_truth_values)\n",
        "            imputed_clean = imputed_values[valid_mask]\n",
        "            gt_clean = ground_truth_values[valid_mask]\n",
        "            \n",
        "            if len(imputed_clean) > 0 and len(gt_clean) > 0:\n",
        "                # Create histograms\n",
        "                ax = axes[idx]\n",
        "                \n",
        "                # Calculate bins for both distributions\n",
        "                all_values = np.concatenate([imputed_clean, gt_clean])\n",
        "                bins = np.linspace(all_values.min(), all_values.max(), 20)  # Fewer bins for smaller plots\n",
        "                \n",
        "                # Plot histograms\n",
        "                ax.hist(gt_clean, bins=bins, alpha=0.7, label='Dataset', \n",
        "                    color='skyblue', density=True, edgecolor='black', linewidth=0.3)\n",
        "                ax.hist(imputed_clean, bins=bins, alpha=0.7, label='Imputed', \n",
        "                    color='lightcoral', density=True, edgecolor='black', linewidth=0.3)\n",
        "                \n",
        "                # Add statistical information\n",
        "                gt_mean, gt_std = gt_clean.mean(), gt_clean.std()\n",
        "                imp_mean, imp_std = imputed_clean.mean(), imputed_clean.std()\n",
        "                correlation = np.corrcoef(gt_clean, imputed_clean)[0, 1] if len(gt_clean) > 1 else 0\n",
        "\n",
        "                # Calculate MMD (Maximum Mean Discrepancy)\n",
        "                def rbf_kernel(X, Y, gamma=1.0):\n",
        "                    \"\"\"RBF kernel for MMD calculation\"\"\"\n",
        "                    XX = np.sum(X**2, axis=1, keepdims=True)\n",
        "                    YY = np.sum(Y**2, axis=1, keepdims=True)\n",
        "                    XY = np.dot(X, Y.T)\n",
        "                    distances = XX + YY.T - 2*XY\n",
        "                    return np.exp(-gamma * distances)\n",
        "                \n",
        "                def mmd_rbf(X, Y, gamma=1.0):\n",
        "                    \"\"\"Calculate MMD with RBF kernel\"\"\"\n",
        "                    X = X.reshape(-1, 1)\n",
        "                    Y = Y.reshape(-1, 1)\n",
        "                    \n",
        "                    m, n = len(X), len(Y)\n",
        "                    \n",
        "                    K_XX = rbf_kernel(X, X, gamma)\n",
        "                    K_YY = rbf_kernel(Y, Y, gamma)\n",
        "                    K_XY = rbf_kernel(X, Y, gamma)\n",
        "                    \n",
        "                    mmd = (np.sum(K_XX) / (m * m) + \n",
        "                           np.sum(K_YY) / (n * n) - \n",
        "                           2 * np.sum(K_XY) / (m * n))\n",
        "                    return np.sqrt(max(mmd, 0))  # Ensure non-negative\n",
        "                \n",
        "                try:\n",
        "                    mmd_value = mmd_rbf(gt_clean, imputed_clean)\n",
        "                except:\n",
        "                    mmd_value = np.nan\n",
        "\n",
        "                # Calculate Jensen-Shannon Divergence\n",
        "                try:\n",
        "                    # Create histograms with same bins for JSD\n",
        "                    data_range = (min(gt_clean.min(), imputed_clean.min()), \n",
        "                                 max(gt_clean.max(), imputed_clean.max()))\n",
        "                    \n",
        "                    if data_range[1] == data_range[0]:\n",
        "                        jsd_value = 0.0  # No divergence if all values are the same\n",
        "                    else:\n",
        "                        bins = np.linspace(data_range[0], data_range[1], 30)\n",
        "                        \n",
        "                        # Get histogram probabilities\n",
        "                        hist_gt, _ = np.histogram(gt_clean, bins=bins, density=True)\n",
        "                        hist_imp, _ = np.histogram(imputed_clean, bins=bins, density=True)\n",
        "                        \n",
        "                        # Normalize to probabilities\n",
        "                        hist_gt = hist_gt + 1e-10  # Add small epsilon to avoid zeros\n",
        "                        hist_imp = hist_imp + 1e-10\n",
        "                        hist_gt = hist_gt / hist_gt.sum()\n",
        "                        hist_imp = hist_imp / hist_imp.sum()\n",
        "                        \n",
        "                        # Calculate Jensen-Shannon divergence\n",
        "                        jsd_value = jensenshannon(hist_gt, hist_imp)\n",
        "                except:\n",
        "                    jsd_value = np.nan\n",
        "\n",
        "                # Add vertical lines for means\n",
        "                ax.axvline(gt_mean, color='blue', linestyle='--', alpha=0.8, linewidth=1, label='Dataset Mean' if idx == 0 else \"\")\n",
        "                ax.axvline(imp_mean, color='red', linestyle='--', alpha=0.8, linewidth=1, label='Imputed Mean' if idx == 0 else \"\")\n",
        "                \n",
        "                # Set labels and title (smaller font for 5x5 grid)\n",
        "                ax.set_xlabel(f'{feature_name[:15]}', fontsize=8)  # Truncate long names\n",
        "                ax.set_ylabel('Density', fontsize=8)\n",
        "                ax.tick_params(labelsize=7)\n",
        "                \n",
        "                # Add correlation, MMD, and JSD as title\n",
        "                ax.set_title(f'R²={correlation:.3f}, MMD={mmd_value:.3f}, JSD={jsd_value:.3f}', fontsize=7, fontweight='bold')\n",
        "                \n",
        "                # Add legend only to first plot\n",
        "                if idx == 0:\n",
        "                    ax.legend(fontsize=7, loc='upper right')\n",
        "                \n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            else:\n",
        "                axes[idx].text(0.5, 0.5, f'{feature_name[:15]}\\nNo valid data', \n",
        "                            ha='center', va='center', transform=axes[idx].transAxes, fontsize=8)\n",
        "                axes[idx].set_title(f'{feature_name[:15]} - No Valid Data', fontsize=8)\n",
        "        else:\n",
        "            axes[idx].text(0.5, 0.5, f'{feature_name[:15]}\\nNo missing values', \n",
        "                        ha='center', va='center', transform=axes[idx].transAxes, fontsize=8)\n",
        "            axes[idx].set_title(f'{feature_name[:15]} - No Missing Values', fontsize=8)\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for idx in range(len(features_with_missing), 40):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be0b2e3",
      "metadata": {
        "id": "4be0b2e3"
      },
      "source": [
        "### Distribution Comparison Visualizations\n",
        "\n",
        "Visual comparison of predicted vs. dataset distributions is crucial for understanding model performance beyond simple error metrics. These plots help us assess:\n",
        "\n",
        "**What the Plots Show:**\n",
        "- **Red (Imputed)**: Distribution of model's predicted values for missing positions\n",
        "- **Blue (Dataset)**: Distribution of actual values at those same positions\n",
        "- **Overlap**: How well the model captures the true data distribution\n",
        "\n",
        "**Why This Matters:**\n",
        "- A good generative model should not just minimize error, but also preserve the statistical properties of the data\n",
        "- If distributions match well, the model is generating realistic values\n",
        "- Large differences indicate the model may be systematically biased or missing important patterns\n",
        "\n",
        "**Interpretation:**\n",
        "- **Good**: Overlapping distributions with similar shapes and centers\n",
        "- **Concerning**: Shifted means, different variances, or completely different shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lI7StssOGmfp",
      "metadata": {
        "id": "lI7StssOGmfp"
      },
      "outputs": [],
      "source": [
        "def generate_samples(model, X_test, test_loader, device, n_samples_per_test=100):\n",
        "    \"\"\"Generate multiple samples for a dataset using the trained model.\n",
        "    \"\"\"\n",
        "    # We'll generate multiple samples\n",
        "    test_samples = np.zeros((X_test.shape[0], n_samples_per_test, X_test.shape[1]))\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Create a progress bar for all samples\n",
        "        from tqdm import tqdm\n",
        "\n",
        "        for batch_idx, (batch_data, batch_mask) in enumerate(tqdm(test_loader, desc=\"Generating Samples\")):\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Calculate the indices for this batch\n",
        "            start_idx = batch_idx * test_loader.batch_size\n",
        "            end_idx = min(start_idx + test_loader.batch_size, X_test.shape[0])\n",
        "            actual_batch_size = end_idx - start_idx\n",
        "\n",
        "            # Generate multiple samples for each item in the batch\n",
        "            for j in range(n_samples_per_test):\n",
        "                # Get reconstruction\n",
        "                reconstruction, mu, logvar = model(batch_data, batch_mask) # TODO: Change this line based on the model you use\n",
        "\n",
        "                # Apply mask: keep original values where available, use reconstructed values where missing\n",
        "                mask_float = batch_mask.float()\n",
        "                imputed = batch_data * mask_float + reconstruction * (1 - mask_float)\n",
        "\n",
        "                # Store the samples (already in original scale since we didn't normalize)\n",
        "                test_samples[start_idx:end_idx, j, :] = imputed.cpu().numpy()\n",
        "    print(f\"✓ Generated samples shape: {test_samples.shape}\")\n",
        "    print(f\"  - {test_samples.shape[0]} samples\")\n",
        "    print(f\"  - {test_samples.shape[1]} generated variations per sample\")\n",
        "    print(f\"  - {test_samples.shape[2]} features per sample\")\n",
        "\n",
        "    # Data is already in original scale (no denormalization needed)\n",
        "    test_samples_final = test_samples.copy()\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    mean_across_samples = test_samples_final.mean(axis=1)  # Mean across the 100 samples\n",
        "\n",
        "    print(f\"  - Range of means: [{mean_across_samples.min():.4f}, {mean_across_samples.max():.4f}]\")\n",
        "\n",
        "    return test_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sjkbko3oFJQV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjkbko3oFJQV",
        "outputId": "f5224d48-f821-428d-f99a-d059b8150dbc"
      },
      "outputs": [],
      "source": [
        "# Test Evaluation\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TEST EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate multiple samples for test using the trained model\n",
        "print(f\"Generating 100 samples for each of {X_test.shape[0]} test samples...\")\n",
        "\n",
        "# test_samples = generate_samples(\n",
        "#     model, X_test, test_loader, device, n_samples_per_test=100\n",
        "# )\n",
        "test_samples = generate_samples(\n",
        "    model, X_test, test_loader, device, n_samples_per_test=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JFWebQ2AzSsJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFWebQ2AzSsJ",
        "outputId": "4c12f4f3-dadf-4384-f101-32fb536fb139"
      },
      "outputs": [],
      "source": [
        "test_score = compute_score(generated_samples=test_samples, set_name='test')\n",
        "print(\"Test score:\", test_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zlA9iSSZcYlh",
      "metadata": {
        "id": "zlA9iSSZcYlh"
      },
      "source": [
        "The final score is computed as: Mean Correlation − Mean JS Divergence − Mean MSE\n",
        "\n",
        "Just as we compare generated samples for the test set against the original unimputed values, we will apply the same metric to the samples you generate for test2, using the hidden test2 set. This will determine your final submission score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7de3489",
      "metadata": {
        "id": "b7de3489"
      },
      "source": [
        "## Preparing a submission:\n",
        "Let's prepare a submission. We expect the final submission to be a 417x100x37 numpy array. These correspond to the 100 diverse samples you generated based on the constrained parameters we provided in the test2 set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6a8623",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d6a8623",
        "outputId": "fa7b08fc-4fee-472f-ea8c-f6ab635e3bc2"
      },
      "outputs": [],
      "source": [
        "# Test2 Evaluation\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TEST2 EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate multiple samples for test2 using the trained model\n",
        "print(f\"Generating 100 samples for each of {X_test2.shape[0]} test2 samples...\")\n",
        "\n",
        "test2_samples = generate_samples(\n",
        "    model, X_test2, test2_loader, device, n_samples_per_test=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bacf0675",
      "metadata": {
        "id": "bacf0675"
      },
      "source": [
        "### Test2 Evaluation: Generating Diverse Design Completions\n",
        "\n",
        "This is the core evaluation for your AI Copilot assignment. Here we:\n",
        "\n",
        "**Input**: Test2 samples with some known features (constraints) and some missing features (free parameters)\n",
        "\n",
        "**Output**: 100 diverse, plausible completions for each test sample\n",
        "\n",
        "**Why 100 Samples?**\n",
        "- Engineers want to explore multiple design options, not just one \"best\" solution\n",
        "- Diversity helps discover unexpected but valid design combinations  \n",
        "\n",
        "**Technical Process:**\n",
        "1. For each test2 sample, use the trained model to generate 100 different completions\n",
        "2. Each completion respects the known constraints (observed values)\n",
        "3. Missing values are filled with diverse, model-generated predictions\n",
        "4. Final output: 417 × 100 × 37 array (417 test samples, 100 variants each, 37 features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc755a46",
      "metadata": {
        "id": "fc755a46"
      },
      "outputs": [],
      "source": [
        "id = np.random.randint(1e8, 1e9-1)\n",
        "np.save(f\"{id}.npy\", test2_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2044ff7",
      "metadata": {
        "id": "d2044ff7"
      },
      "source": [
        "### Summary and Tips for CP3\n",
        "\n",
        "The VAE baseline reproduces the dataset distribution well for some features, but others still show substantial discrepancies, indicating significant room for improvement!\n",
        "\n",
        "**Key Observations:**\n",
        "- **Strengths**: The model captures general feature ranges and some distributional patterns\n",
        "- **Weaknesses**: Some features show systematic bias or poor distribution matching\n",
        "- **Opportunities**: Advanced architectures (diffusion models, transformers) or better conditioning strategies could improve performance\n",
        "\n",
        "**For Your Assignment**: Consider these results as a baseline. Think about:\n",
        "- Which features are hardest to predict and why?\n",
        "- How could you modify the architecture or training process?\n",
        "- What additional constraints or domain knowledge could help?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29cae83b",
      "metadata": {},
      "source": [
        "# Fist try with a diffusion model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4985b79",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ed4357",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e87909d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diffusion-based tabular imputer (DDPM-style)\n",
        "# This cell implements a lightweight denoising diffusion probabilistic model (DDPM)\n",
        "# tailored for tabular imputation (conditioned on observed values via the mask).\n",
        "# It's self-contained and does not require external 'diffusers' packages.\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import tqdm\n",
        "\n",
        "# -- Utilities: noise schedule and timestep embedding --\n",
        "def make_beta_schedule(T, start=1e-4, end=0.02, device='cpu'):\n",
        "    return torch.linspace(start, end, T, device=device)\n",
        "\n",
        "def sinusoidal_timestep_embedding(timesteps, dim):\n",
        "    half = dim // 2\n",
        "    emb = math.log(10000) / (half - 1)\n",
        "    emb = torch.exp(torch.arange(half, device=timesteps.device) * -emb)\n",
        "    emb = timesteps[:, None].float() * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if dim % 2 == 1:\n",
        "        emb = F.pad(emb, (0,1))\n",
        "    return emb\n",
        "\n",
        "# -- Simple MLP denoiser with timestep & mask conditioning --\n",
        "class MLPDenoiser(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=512, timesteps_embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden = hidden\n",
        "        self.timesteps_embed_dim = timesteps_embed_dim\n",
        "\n",
        "        # Time embedding MLP -> produces a vector of size `hidden`\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(timesteps_embed_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Project time embedding to match final output dimension so it can be added to network output\n",
        "        self.time_to_output = nn.Linear(hidden, input_dim)\n",
        "\n",
        "        # The network input will be: noisy_x (input_dim) + observed_values (input_dim) + mask (input_dim)\n",
        "        in_dim = input_dim * 3\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, noisy_x, cond_x, mask, t):\n",
        "        # t: tensor of shape (B,) with timestep indices\n",
        "        # Create sinusoidal embedding of timesteps with the requested embedding dim\n",
        "        t_emb = sinusoidal_timestep_embedding(t, self.timesteps_embed_dim)\n",
        "        t_emb = self.time_mlp(t_emb)  # (B, hidden)\n",
        "\n",
        "        # Project time embedding to output dimension so it can be added to net output\n",
        "        t_out = self.time_to_output(t_emb)  # (B, input_dim)\n",
        "\n",
        "        # Concatenate noisy input, conditional observed values and mask\n",
        "        inp = torch.cat([noisy_x, cond_x * mask, mask], dim=1)  # (B, in_dim)\n",
        "        net_out = self.net(inp)  # (B, input_dim)\n",
        "\n",
        "        # Add projected time features (elementwise) to network output\n",
        "        h = net_out + t_out\n",
        "        return h\n",
        "\n",
        "# -- DDPM helper functions --\n",
        "class DiffusionImputer:\n",
        "    def __init__(self, input_dim, T=1000, device='cpu'):\n",
        "        self.device = device\n",
        "        self.input_dim = input_dim\n",
        "        self.T = T\n",
        "        self.model = MLPDenoiser(input_dim).to(device)\n",
        "        self.beta = make_beta_schedule(T, device=device)\n",
        "        self.alpha = 1.0 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=2e-4)\n",
        "\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        # x_start: (B, D) clean data; t: (B,) timesteps\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "        a_bar = self.alpha_bar[t].unsqueeze(1)\n",
        "        return torch.sqrt(a_bar) * x_start + torch.sqrt(1 - a_bar) * noise, noise\n",
        "\n",
        "    def p_losses(self, x_start, cond_x, mask, t):\n",
        "        x_noisy, noise = self.q_sample(x_start, t)\n",
        "        pred_noise = self.model(x_noisy, cond_x, mask, t)\n",
        "        loss = F.mse_loss(pred_noise * (1 - mask), noise * (1 - mask))\n",
        "        # We only compute loss on missing positions (1-mask) so model learns to predict missing values' noise\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, cond_x, mask, n_steps=None):\n",
        "        # cond_x: observed values (B,D), mask: (B,D) with 1=observed, 0=missing\n",
        "        n_steps = self.T if n_steps is None else n_steps\n",
        "        bsz = cond_x.size(0)\n",
        "        x = torch.randn(bsz, self.input_dim, device=self.device)\n",
        "\n",
        "        for i in reversed(range(n_steps)):\n",
        "            t = torch.full((bsz,), i, dtype=torch.long, device=self.device)\n",
        "            pred_noise = self.model(x, cond_x, mask, t)\n",
        "            beta_t = self.beta[i]\n",
        "            alpha_t = self.alpha[i]\n",
        "            alpha_bar_t = self.alpha_bar[i]\n",
        "\n",
        "            # Predict x0 from x_t and predicted noise\n",
        "            x0_pred = (x - torch.sqrt(1 - alpha_bar_t) * pred_noise) / torch.sqrt(alpha_bar_t)\n",
        "\n",
        "            coef1 = beta_t / torch.sqrt(1 - alpha_bar_t)\n",
        "            mean = torch.sqrt(1.0 / alpha_t) * (x - coef1 * pred_noise)\n",
        "\n",
        "            if i > 0:\n",
        "                noise = torch.randn_like(x)\n",
        "                sigma = torch.sqrt(beta_t)\n",
        "                x = mean + sigma * noise\n",
        "            else:\n",
        "                x = mean\n",
        "\n",
        "            # Re-impose observed values so conditioning always holds exactly\n",
        "            x = cond_x * mask + x * (1 - mask)\n",
        "\n",
        "        return x.cpu().numpy()\n",
        "\n",
        "# -- Training helpers (simple epoch loop) --\n",
        "def train_diffusion(imputer, train_loader, val_loader=None, epochs=20, device='cpu', ckpt_path='diffusion_imputer.pth'):\n",
        "    imputer.model.train()\n",
        "    for epoch in range(epochs):\n",
        "        pbar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "        epoch_loss = 0.0\n",
        "        for batch_x, batch_mask in pbar:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Random timesteps for each sample\n",
        "            t = torch.randint(0, imputer.T, (batch_x.size(0),), device=device, dtype=torch.long)\n",
        "            loss = imputer.p_losses(batch_x, batch_x, batch_mask, t)\n",
        "\n",
        "            imputer.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            imputer.optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * batch_x.size(0)\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
        "\n",
        "        avg_loss = epoch_loss / (len(train_loader.dataset))\n",
        "        # print(f'Epoch {epoch+1}/{epochs} - Avg Loss: {avg_loss:.6f}')\n",
        "        torch.save(imputer.model.state_dict(), ckpt_path)\n",
        "\n",
        "    print('✓ Diffusion training complete')\n",
        "\n",
        "# -- Usage example (copy into a cell and run to train) --\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# imputer = DiffusionImputer(input_dim=config['input_dim'], T=200, device=device)\n",
        "# train_diffusion(imputer, train_loader, val_loader=val_loader, epochs=10, device=device)\n",
        "# # Generate imputations: returns numpy array (B, D)\n",
        "# sample = imputer.sample(cond_x=batch_data.to(device), mask=batch_mask.to(device), n_steps=200)\n",
        "\n",
        "# Notes:\n",
        "# - This implementation is a compact DDPM variant for tabular data. It conditions on observed values by concatenation\n",
        "#   and re-imposes the observed features during sampling to guarantee constraints.\n",
        "# - The model trains only on missing positions (loss masked) so it focuses on reconstructing omitted values.\n",
        "# - You can tune T (timesteps), model hidden size, and training schedule. Use smaller T (e.g., 200) for fast experiments.\n",
        "# - If you want the Hugging Face `diffusers` pipelines instead, I can add instructions to install it and adapt a tabular\n",
        "#   pipeline using their Unet/ModelMlp components, but that will add dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "721607bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train diffusion imputer (short run) and compare to the VAE baseline\n",
        "print('\\n' + '='*70)\n",
        "print('DIFFUSION TRAIN & COMPARE')\n",
        "print('='*70)\n",
        "\n",
        "# Device (should already be defined earlier, but keep fallback)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate the imputer (use smaller T for quicker experiments)\n",
        "imputer = DiffusionImputer(input_dim=config['input_dim'], T=1000, device=device)\n",
        "\n",
        "# Quick training run - adjust epochs as needed (start small for a smoke test)\n",
        "quick_epochs = 100\n",
        "train_diffusion(imputer, train_loader, val_loader=val_loader, epochs=quick_epochs, device=device, ckpt_path='diffusion_imputer.pth')\n",
        "\n",
        "# Evaluation helper for diffusion imputer (generates multiple samples per input)\n",
        "def evaluate_diffusion_imputer(imputer, data_loader, device, n_samples=20, n_steps=200):\n",
        "    imputer.model.eval()\n",
        "    all_imputations = []  # will hold (B, n_samples, D) blocks\n",
        "    all_originals = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in data_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            samples = []\n",
        "            for _ in range(n_samples):\n",
        "                out = imputer.sample(batch_data, batch_mask, n_steps=n_steps)  # numpy (B, D)\n",
        "                samples.append(out)\n",
        "\n",
        "            samples = np.stack(samples, axis=1)  # (B, n_samples, D)\n",
        "            all_imputations.append(samples)\n",
        "            all_originals.append(batch_data.cpu().numpy())\n",
        "            # keep numeric mask (1=observed,0=missing) so callers can compute missing positions via (mask==0)\n",
        "            all_masks.append(batch_mask.cpu().numpy())\n",
        "\n",
        "    imputations = np.concatenate(all_imputations, axis=0)\n",
        "    originals = np.vstack(all_originals)\n",
        "    masks = np.vstack(all_masks)\n",
        "    return imputations, originals, masks\n",
        "\n",
        "# Run evaluation on the test set (this may take time depending on n_samples)\n",
        "print('Evaluating diffusion imputer on test set...')\n",
        "diff_imputations, diff_originals, diff_masks = evaluate_diffusion_imputer(imputer, test_loader, device, n_samples=20, n_steps=200)\n",
        "print('✓ Diffusion imputations generated')\n",
        "\n",
        "# Summarize metrics similar to VAE evaluation (use mean across samples)\n",
        "diff_mean_imputed = diff_imputations.mean(axis=1)  # (N, D)\n",
        "# missing_mask: True where values were missing (mask==0 in model tensors)\n",
        "missing_mask = (diff_masks == 0)\n",
        "feature_metrics_diff = {}\n",
        "for i, fname in enumerate(feature_names):\n",
        "    if missing_mask[:, i].sum() > 0:\n",
        "        imputed_missing = diff_mean_imputed[missing_mask[:, i], i]\n",
        "        ground_truth_missing = diff_originals[missing_mask[:, i], i]\n",
        "\n",
        "        mse = mean_squared_error(ground_truth_missing, imputed_missing)\n",
        "        mae = mean_absolute_error(ground_truth_missing, imputed_missing)\n",
        "        try:\n",
        "            corr = np.corrcoef(ground_truth_missing, imputed_missing)[0,1]\n",
        "        except:\n",
        "            corr = np.nan\n",
        "        mean_diff, js_div = calculate_jsd_and_mean_diff(imputed_missing, ground_truth_missing, fname)\n",
        "\n",
        "        feature_metrics_diff[fname] = {'n_missing': missing_mask[:, i].sum(), 'mse': mse, 'mae': mae, 'correlation': corr, 'mean_difference': mean_diff, 'js_divergence': js_div}\n",
        "\n",
        "print(f'\\n✓ Diffusion metrics calculated for {len(feature_metrics_diff)} features with missing values')\n",
        "\n",
        "# Print summary statistics (same as earlier VAE cell)\n",
        "all_mse = [m['mse'] for m in feature_metrics_diff.values() if not np.isnan(m['mse'])]\n",
        "all_mae = [m['mae'] for m in feature_metrics_diff.values() if not np.isnan(m['mae'])]\n",
        "all_corr = [m['correlation'] for m in feature_metrics_diff.values() if not np.isnan(m['correlation'])]\n",
        "all_mean_diff = [m['mean_difference'] for m in feature_metrics_diff.values() if not np.isnan(m['mean_difference'])]\n",
        "all_js_div = [m['js_divergence'] for m in feature_metrics_diff.values() if not np.isnan(m['js_divergence'])]\n",
        "\n",
        "print('\\nDiffusion Summary Statistics Across All Features:')\n",
        "print(f'  - Average MSE: {np.mean(all_mse):.4f} ± {np.std(all_mse):.4f}')\n",
        "print(f'  - Average MAE: {np.mean(all_mae):.4f} ± {np.std(all_mae):.4f}')\n",
        "print(f'  - Average Correlation: {np.mean(all_corr):.3f} ± {np.std(all_corr):.3f}')\n",
        "print(f'  - Average Mean Difference: {np.mean(all_mean_diff):.4f} ± {np.std(all_mean_diff):.4f}')\n",
        "print(f'  - Average JS Divergence: {np.mean(all_js_div):.4f} ± {np.std(all_js_div):.4f}')\n",
        "\n",
        "# If VAE test imputations exist in the notebook namespace, compute the same score and compare\n",
        "if 'test_imputations' in globals():\n",
        "    print('\\nComparing to VAE results (if available)')\n",
        "    # VAE's test_imputations may have shape (N, D) or (N, n_samples, D) depending on how generated; handle both\n",
        "    vae_samples = test_imputations\n",
        "    if vae_samples.ndim == 3:\n",
        "        vae_mean = vae_samples.mean(axis=1)\n",
        "    else:\n",
        "        vae_mean = vae_samples\n",
        "\n",
        "    # Compute a simple average MSE over missing positions for VAE mean vs diffusion mean\n",
        "    # Use the same missing positions mask computed above\n",
        "    vae_errors = ((vae_mean - X_test_original) ** 2)[missing_mask]\n",
        "    diff_errors = ((diff_mean_imputed - diff_originals) ** 2)[missing_mask]\n",
        "    print(f'  - VAE mean MSE on missing positions: {vae_errors.mean():.6f}')\n",
        "    print(f'  - Diffusion mean MSE on missing positions: {diff_errors.mean():.6f}')\n",
        "else:\n",
        "    print('VAE test imputations variable `test_imputations` not found in the notebook namespace. Run the VAE evaluation cell first to compare.')\n",
        "\n",
        "# Optionally compute the same final metric used for the challenge if compute_score is available\n",
        "if 'compute_score' in globals():\n",
        "    try:\n",
        "        print('\\nComputing challenge-style score for diffusion samples (may take time)')\n",
        "        diff_score = compute_score(generated_samples=diff_imputations, set_name='test')\n",
        "        print('Diffusion score:', diff_score)\n",
        "    except Exception as e:\n",
        "        print('compute_score failed:', e)\n",
        "\n",
        "print('\\n✓ Diffusion training + comparison cell complete. Review outputs above.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce07fe4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate multiple samples from the trained diffusion imputer (N x S x D)\n",
        "def generate_samples_diffusion(imputer, X_test, test_loader, device, n_samples_per_test=100, n_steps=200):\n",
        "    \"\"\"Generate multiple imputations per test sample using the diffusion imputer.\n",
        "\n",
        "    Returns:\n",
        "        test_samples: numpy array shape (N, n_samples_per_test, D)\n",
        "    \"\"\"\n",
        "    imputer.model.eval()\n",
        "\n",
        "    N = X_test.shape[0]\n",
        "    D = X_test.shape[1]\n",
        "    test_samples = np.zeros((N, n_samples_per_test, D))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (batch_data, batch_mask) in enumerate(tqdm.tqdm(test_loader, desc='Generating Diffusion Samples')):\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            start_idx = batch_idx * test_loader.batch_size\n",
        "            end_idx = min(start_idx + test_loader.batch_size, N)\n",
        "            actual_batch_size = end_idx - start_idx\n",
        "\n",
        "            # Generate n_samples_per_test samples for this batch\n",
        "            for j in range(n_samples_per_test):\n",
        "                out = imputer.sample(batch_data, batch_mask, n_steps=n_steps)  # numpy (B, D)\n",
        "                # If imputer.sample returns a larger batch (shouldn't), trim to actual_batch_size\n",
        "                out = np.asarray(out)\n",
        "                if out.shape[0] != actual_batch_size:\n",
        "                    out = out[:actual_batch_size]\n",
        "                test_samples[start_idx:end_idx, j, :] = out\n",
        "\n",
        "    print(f\"✓ Generated samples shape: {test_samples.shape}\")\n",
        "    return test_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de341ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "test2_samples = generate_samples_diffusion(imputer, X_test2, test2_loader, device, n_samples_per_test=100)\n",
        "id = np.random.randint(1e8, 1e9-1)\n",
        "np.save(f\"{id}.npy\", test2_samples)\n",
        "print(f\"Saved diffusion test2 samples to {id}.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db9d917b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Improved VAE: deeper gated residual blocks, LayerNorm, FiLM conditioning, and heteroscedastic continuous head\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, dropout=0.1, use_residual=False):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim_in, dim_out)\n",
        "        self.norm = nn.LayerNorm(dim_out)\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.use_residual = use_residual and (dim_in == dim_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc(x)\n",
        "        h = self.norm(h)\n",
        "        h = self.act(h)\n",
        "        h = self.dropout(h)\n",
        "        if self.use_residual:\n",
        "            return x + h\n",
        "        return h\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved VAE for mixed continuous + categorical tabular imputation.\n",
        "    - LayerNorm + GELU MLP blocks (stable for small batches)\n",
        "    - FiLM-style conditioning in decoder: latent -> (gamma,beta) per decoder layer\n",
        "    - Continuous head predicts mean and log-variance per feature (heteroscedastic)\n",
        "    - Categorical heads return logits per categorical feature\n",
        "    - Impute method samples continuous/categorical outputs appropriately\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        latent_dim=128,\n",
        "        hidden_dims=[512, 256, 128],\n",
        "        dropout=0.1,\n",
        "        dropout_rate=None,\n",
        "        cat_feature_idxs=None,\n",
        "        cat_dims=None,\n",
        "        cat_embed_dim=8,\n",
        "        use_residual=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # backward-compatibility: if caller provided dropout_rate, use it\n",
        "        if dropout_rate is not None:\n",
        "            dropout = dropout_rate\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.use_residual = use_residual\n",
        "\n",
        "        # default categorical features (0-based indices)\n",
        "        self.cat_feature_idxs = cat_feature_idxs if cat_feature_idxs is not None else [24, 25]\n",
        "        self.cat_dims = cat_dims if cat_dims is not None else {24: 3, 25: 3}\n",
        "        self.cat_embed_dim = cat_embed_dim\n",
        "\n",
        "        # embeddings\n",
        "        self.cat_embeds = nn.ModuleDict({\n",
        "            str(i): nn.Embedding(self.cat_dims[i], cat_embed_dim) for i in self.cat_feature_idxs\n",
        "        })\n",
        "        total_cat_embed_dim = cat_embed_dim * len(self.cat_feature_idxs)\n",
        "\n",
        "        # feature importance network (small)\n",
        "        self.feature_importance = nn.Sequential(\n",
        "            nn.Linear(input_dim * 2, max(hidden_dims[0] // 4, 32)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(max(hidden_dims[0] // 4, 32), input_dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        # Encoder MLP stack\n",
        "        enc_in_dim = input_dim * 2 + total_cat_embed_dim\n",
        "        enc_dims = [enc_in_dim] + hidden_dims\n",
        "        self.encoder_blocks = nn.ModuleList()\n",
        "        for i in range(len(enc_dims) - 1):\n",
        "            self.encoder_blocks.append(MLPBlock(enc_dims[i], enc_dims[i + 1], dropout=dropout, use_residual=self.use_residual))\n",
        "\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "        nn.init.xavier_uniform_(self.fc_mu.weight, gain=0.05)\n",
        "        nn.init.xavier_uniform_(self.fc_logvar.weight, gain=0.05)\n",
        "        nn.init.constant_(self.fc_logvar.bias, -3.0)\n",
        "\n",
        "        # Decoder: latent + observed + positional + cat_emb\n",
        "        dec_in_dim = latent_dim + input_dim + input_dim + total_cat_embed_dim\n",
        "        dec_dims = [dec_in_dim] + list(reversed(hidden_dims))\n",
        "        self.decoder_blocks = nn.ModuleList()\n",
        "        for i in range(len(dec_dims) - 1):\n",
        "            self.decoder_blocks.append(MLPBlock(dec_dims[i], dec_dims[i + 1], dropout=dropout, use_residual=self.use_residual))\n",
        "\n",
        "        # FiLM generators (one per decoder layer) to produce gamma/beta from latent\n",
        "        self.film_generators = nn.ModuleList()\n",
        "        for hdim in dec_dims[1:]:\n",
        "            self.film_generators.append(nn.Linear(latent_dim, hdim * 2))\n",
        "\n",
        "        # Continuous heads: mean and logvar per feature\n",
        "        self.cont_mu = nn.Linear(dec_dims[-1], input_dim)\n",
        "        self.cont_logvar = nn.Linear(dec_dims[-1], input_dim)\n",
        "        nn.init.xavier_uniform_(self.cont_mu.weight, gain=0.05)\n",
        "        nn.init.xavier_uniform_(self.cont_logvar.weight, gain=0.05)\n",
        "\n",
        "        # Categorical heads\n",
        "        self.cat_heads = nn.ModuleDict({\n",
        "            str(i): nn.Linear(dec_dims[-1], self.cat_dims[i]) for i in self.cat_feature_idxs\n",
        "        })\n",
        "\n",
        "    def _build_cat_emb(self, x, mask):\n",
        "        mask_f = mask.float()\n",
        "        emb_list = []\n",
        "        for idx in self.cat_feature_idxs:\n",
        "            raw = x[:, idx]\n",
        "            # map to classes robustly (nearest to 0.0,0.5,1.0)\n",
        "            cls = torch.zeros_like(raw, dtype=torch.long)\n",
        "            cls = torch.where((raw - 0.5).abs() < (raw - 0.0).abs(), torch.ones_like(cls), cls)\n",
        "            cls = torch.where((raw - 1.0).abs() < (raw - 0.5).abs(), torch.full_like(cls, 2), cls)\n",
        "            emb = self.cat_embeds[str(idx)](cls)\n",
        "            emb = emb * mask_f[:, idx:idx + 1]\n",
        "            emb_list.append(emb)\n",
        "        if emb_list:\n",
        "            return torch.cat(emb_list, dim=1)\n",
        "        return torch.zeros(x.size(0), 0, device=x.device)\n",
        "\n",
        "    def encode(self, x, mask):\n",
        "        mask_f = mask.float()\n",
        "        cat_emb = self._build_cat_emb(x, mask)\n",
        "\n",
        "        importance_in = torch.cat([x * mask_f, mask_f], dim=1)\n",
        "        importance = self.feature_importance(importance_in)\n",
        "        weighted = x * mask_f * importance\n",
        "\n",
        "        enc_in = torch.cat([weighted, mask_f, cat_emb], dim=1)\n",
        "        h = enc_in\n",
        "        for block in self.encoder_blocks:\n",
        "            h = block(h)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h).clamp(min=-10.0, max=10.0)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, x_observed, mask):\n",
        "        mask_f = mask.float()\n",
        "        x_masked = x_observed * mask_f\n",
        "        pos = torch.arange(self.input_dim, device=z.device, dtype=torch.float32).unsqueeze(0) / float(self.input_dim)\n",
        "        pos = pos.expand(z.size(0), -1)\n",
        "        cat_emb = self._build_cat_emb(x_observed, mask)\n",
        "\n",
        "        dec_in = torch.cat([z, x_masked, pos, cat_emb], dim=1)\n",
        "        h = dec_in\n",
        "        # apply decoder blocks with FiLM\n",
        "        for i, block in enumerate(self.decoder_blocks):\n",
        "            h = block(h)\n",
        "            # FiLM\n",
        "            film = self.film_generators[i](z)\n",
        "            gamma, beta = film.chunk(2, dim=1)\n",
        "            h = h * (1.0 + gamma) + beta\n",
        "\n",
        "        mu = self.cont_mu(h)\n",
        "        logvar = self.cont_logvar(h).clamp(min=-10.0, max=5.0)\n",
        "\n",
        "        cat_logits = {int(k): v(h) for k, v in self.cat_heads.items()}\n",
        "        return mu, logvar, cat_logits\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        mu, logvar = self.encode(x, mask)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon_mu, recon_logvar, recon_cat_logits = self.decode(z, x, mask)\n",
        "        return recon_mu, recon_logvar, recon_cat_logits, mu, logvar\n",
        "\n",
        "    def impute(self, x_incomplete, mask, n_samples=10):\n",
        "        self.eval()\n",
        "        samples = []\n",
        "        with torch.no_grad():\n",
        "            mu, logvar = self.encode(x_incomplete, mask)\n",
        "            for _ in range(n_samples):\n",
        "                z = self.reparameterize(mu, logvar)\n",
        "                cont_mu, cont_logvar, cat_logits = self.decode(z, x_incomplete, mask)\n",
        "                cont_std = torch.exp(0.5 * cont_logvar)\n",
        "                # sample continuous\n",
        "                eps = torch.randn_like(cont_mu)\n",
        "                cont_sample = cont_mu + eps * cont_std\n",
        "                imputed = x_incomplete * mask.float() + cont_sample * (1 - mask.float())\n",
        "                # categorical: sample from softmax\n",
        "                for idx in self.cat_feature_idxs:\n",
        "                    logits = cat_logits[idx]\n",
        "                    probs = F.softmax(logits, dim=1)\n",
        "                    sampled = torch.multinomial(probs, num_samples=1).squeeze(1).cpu().numpy()\n",
        "                    mapped = np.array([0.0, 0.5, 1.0])[sampled]\n",
        "                    imputed_np = imputed.cpu().numpy()\n",
        "                    miss = (mask[:, idx] == 0).cpu().numpy()\n",
        "                    imputed_np[miss, idx] = mapped[miss]\n",
        "                    imputed = torch.from_numpy(imputed_np).to(x_incomplete.device)\n",
        "                samples.append(imputed.cpu().numpy())\n",
        "        return np.stack(samples, axis=1)\n",
        "\n",
        "print('Replaced VAE with improved architecture (LayerNorm + FiLM + heteroscedastic heads).')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903d153f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smoke test for improved mixed-output VAE (one-batch forward + mixed loss)\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = torch.device('cpu')\n",
        "input_dim = 37\n",
        "batch_size = 8\n",
        "\n",
        "# Build synthetic batch: continuous random values + categorical positions set to {0.0,0.5,1.0}\n",
        "X = torch.randn(batch_size, input_dim, device=device)\n",
        "vals = np.array([0.0, 0.5, 1.0])\n",
        "for i in range(batch_size):\n",
        "    X[i, 24] = float(np.random.choice(vals))\n",
        "    X[i, 25] = float(np.random.choice(vals))\n",
        "\n",
        "# Random observation mask (1=observed, 0=missing)\n",
        "mask = (torch.rand(batch_size, input_dim, device=device) < 0.8).long()\n",
        "\n",
        "model = VAE(input_dim=input_dim).to(device)\n",
        "model.train()\n",
        "\n",
        "# The improved VAE.forward returns (cont_mu, cont_logvar, cat_logits, mu, logvar)\n",
        "recon_mu, recon_logvar, recon_cat_logits, mu, logvar = model(X, mask)\n",
        "print('recon_mu shape', recon_mu.shape)\n",
        "print('categorical logits keys', list(recon_cat_logits.keys()))\n",
        "\n",
        "mask_f = mask.float()\n",
        "# Heteroscedastic Gaussian negative log-likelihood for observed entries\n",
        "var = torch.exp(recon_logvar)\n",
        "nll_elem = 0.5 * (((X - recon_mu) ** 2) / var + recon_logvar + math.log(2 * math.pi))\n",
        "obs_count = mask_f.sum().clamp(min=1.0)\n",
        "cont_loss = (nll_elem * mask_f).sum() / obs_count\n",
        "\n",
        "cat_loss = torch.tensor(0.0, device=device)\n",
        "for idx in model.cat_feature_idxs:\n",
        "    raw = X[:, idx]\n",
        "    targets = torch.zeros_like(raw, dtype=torch.long)\n",
        "    targets = torch.where((raw - 0.5).abs() < (raw - 0.0).abs(), torch.ones_like(targets), targets)\n",
        "    targets = torch.where((raw - 1.0).abs() < (raw - 0.5).abs(), torch.full_like(targets, 2), targets)\n",
        "    logits = recon_cat_logits[idx]\n",
        "    obs = (mask[:, idx] == 1)\n",
        "    if obs.sum() > 0:\n",
        "        cat_loss = cat_loss + F.cross_entropy(logits[obs], targets[obs])\n",
        "\n",
        "kl = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
        "loss = cont_loss + cat_loss + 0.1 * kl\n",
        "\n",
        "print(f'cont_loss={cont_loss.item():.6f} cat_loss={cat_loss.item():.6f} kl={kl.item():.6f} total_loss={loss.item():.6f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7dd12f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute covariance matrix for `train_original.csv` and show top covarying feature pairs\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "path = r\"c:\\sources\\2155-CP3\\dataset\\train_original.csv\"\n",
        "print('Loading:', path)\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Select feature columns (expect names like 'Feature 1'..'Feature 37')\n",
        "feature_cols = [c for c in df.columns if 'Feature' in c]\n",
        "if len(feature_cols) < 37:\n",
        "    # fallback: use numeric columns\n",
        "    feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "df_feat = df[feature_cols].apply(pd.to_numeric, errors='coerce')\n",
        "print('Raw data shape:', df.shape, '-> feature matrix shape:', df_feat.shape)\n",
        "\n",
        "# Drop rows with all-NaN in features (if any)\n",
        "df_feat = df_feat.dropna(how='all')\n",
        "\n",
        "# Compute covariance and correlation\n",
        "cov = df_feat.cov()\n",
        "corr = df_feat.corr()\n",
        "\n",
        "print('Covariance matrix shape:', cov.shape)\n",
        "\n",
        "# Top absolute corr pairs (upper triangle, excluding diagonal)\n",
        "n = corr.shape[0]\n",
        "pairs = []\n",
        "for i in range(n):\n",
        "    for j in range(i+1, n):\n",
        "        pairs.append((feature_cols[i], feature_cols[j], corr.iat[i, j]))\n",
        "pairs_sorted = sorted(pairs, key=lambda x: abs(x[2]), reverse=True)\n",
        "print('\\nTop 15 absolute correlation pairs:')\n",
        "for a, b, v in pairs_sorted[:15]:\n",
        "    print(f'{a} <-> {b}: {v:.6f}')\n",
        "\n",
        "# Ensure outputs dir exists\n",
        "out_dir = r\"c:\\sources\\2155-CP3\\outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "cov_path = os.path.join(out_dir, 'train_covariance.csv')\n",
        "cov.to_csv(cov_path)\n",
        "print('\\nSaved covariance matrix to', cov_path)\n",
        "\n",
        "# Plot heatmap (show correlation and covariance side-by-side)\n",
        "# Use numeric tick labels (1..N) and smaller font size for clarity\n",
        "labels = [str(i+1) for i in range(len(feature_cols))]\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(cov, xticklabels=labels, yticklabels=labels, cmap='vlag', center=0)\n",
        "plt.title('Covariance (train_original)')\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(corr, xticklabels=labels, yticklabels=labels, cmap='vlag', center=0)\n",
        "plt.title('Correlation (train_original)')\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1ae2f47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# VAE Training (fixed for improved VAE signature)\n",
        "# This cell adapts the training loop to the improved VAE which returns\n",
        "# (recon_mu, recon_logvar, recon_cat_logits, mu, logvar).\n",
        "# It defines a local loss function and training loop using the user's config.\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Utilities ---\n",
        "\n",
        "def get_beta_schedule(epoch, num_epochs, schedule='cosine', beta_initial=1.0, beta_final=1.0):\n",
        "    # Simple beta schedule fallback. If you already have a function defined,\n",
        "    # this will behave similarly for typical settings.\n",
        "    if schedule == 'linear':\n",
        "        return beta_initial + (beta_final - beta_initial) * (epoch / max(1, num_epochs - 1))\n",
        "    if schedule == 'cosine':\n",
        "        # Smooth cosine interpolation between beta_initial and beta_final\n",
        "        t = epoch / max(1, num_epochs - 1)\n",
        "        cos = (1 + math.cos(math.pi * (1 - t))) / 2.0\n",
        "        return beta_final + (beta_initial - beta_final) * cos\n",
        "    return beta_initial\n",
        "\n",
        "\n",
        "def vae_mixed_loss(recon_mu, recon_logvar, recon_cat_logits, x, mask, mu, logvar, cat_feature_idxs, kl_weight=1.0):\n",
        "    \"\"\"Return (total_loss, cont_loss, cat_loss, kl)\n",
        "\n",
        "    - recon_mu, recon_logvar: tensors shape (B, D)\n",
        "    - recon_cat_logits: dict idx -> (B, C_i)\n",
        "    - x: (B, D)\n",
        "    - mask: (B, D) with 1=observed, 0=missing\n",
        "    - mu, logvar: latent params\n",
        "    - cat_feature_idxs: list of int indices for categorical features\n",
        "    - kl_weight: multiplier for KL term\n",
        "    \"\"\"\n",
        "    device = x.device\n",
        "    mask_f = mask.float()\n",
        "\n",
        "    # Continuous heteroscedastic NLL (observed entries only)\n",
        "    var = torch.exp(recon_logvar)\n",
        "    nll = 0.5 * (((x - recon_mu) ** 2) / var + recon_logvar + math.log(2 * math.pi))\n",
        "    obs_count = mask_f.sum().clamp(min=1.0)\n",
        "    cont_loss = (nll * mask_f).sum() / obs_count\n",
        "\n",
        "    # Categorical cross-entropy for observed categorical positions\n",
        "    cat_loss = torch.tensor(0.0, device=device)\n",
        "    cat_count = 0\n",
        "    for idx in cat_feature_idxs:\n",
        "        logits = recon_cat_logits[int(idx)]  # (B, C)\n",
        "        # Build integer targets robustly from stored floats {0.0,0.5,1.0}\n",
        "        raw = x[:, idx]\n",
        "        targets = torch.zeros_like(raw, dtype=torch.long, device=device)\n",
        "        targets = torch.where((raw - 0.5).abs() < (raw - 0.0).abs(), torch.ones_like(targets), targets)\n",
        "        targets = torch.where((raw - 1.0).abs() < (raw - 0.5).abs(), torch.full_like(targets, 2), targets)\n",
        "        obs = (mask[:, idx] == 1)\n",
        "        if obs.sum() > 0:\n",
        "            cat_loss = cat_loss + F.cross_entropy(logits[obs], targets[obs])\n",
        "            cat_count += obs.sum().item()\n",
        "    # If there were categorical observations, average the categorical loss\n",
        "    if cat_count > 0:\n",
        "        cat_loss = cat_loss\n",
        "\n",
        "    # KL term (mean over batch)\n",
        "    kl = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
        "\n",
        "    total = cont_loss + cat_loss + kl_weight * kl\n",
        "    return total, cont_loss, cat_loss, kl\n",
        "\n",
        "# --- Training configuration (reuse existing `config` if available) ---\n",
        "# If `config` is in the notebook global scope (as in your snippet), use it.\n",
        "try:\n",
        "    cfg = config\n",
        "except NameError:\n",
        "    # Minimal fallback config\n",
        "    cfg = {\n",
        "        'input_dim': 37,\n",
        "        'latent_dim': 128,\n",
        "        'hidden_dims': [512, 256, 128],\n",
        "        'use_residual': True,\n",
        "        'dropout_rate': 0.3,\n",
        "        'learning_rate': 1e-3,\n",
        "        'num_epochs': 50,\n",
        "        'beta_initial': 1.0,\n",
        "        'beta_schedule': 'cosine',\n",
        "        'patience': 15,\n",
        "    }\n",
        "\n",
        "# Device (reuse notebook device variable if present)\n",
        "try:\n",
        "    device = device\n",
        "except NameError:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Use train_loader/val_loader existing in notebook\n",
        "if 'train_loader' not in globals() or 'val_loader' not in globals():\n",
        "    raise RuntimeError('`train_loader` and `val_loader` must be defined in the notebook before running this cell.')\n",
        "\n",
        "# Initialize model (reuse `model` if present and compatible)\n",
        "model = VAE(\n",
        "    input_dim=cfg['input_dim'],\n",
        "    latent_dim=cfg.get('latent_dim', 128),\n",
        "    hidden_dims=cfg.get('hidden_dims', [512, 256, 128]),\n",
        "    use_residual=cfg.get('use_residual', True),\n",
        "    dropout_rate=cfg.get('dropout_rate', 0.1),\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=cfg.get('learning_rate', 1e-3), weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2, eta_min=1e-6)\n",
        "\n",
        "# Tracking\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "# Training loop (does not automatically run long training; use config['num_epochs'])\n",
        "num_epochs = cfg.get('num_epochs', 50)\n",
        "print(f\"Starting training for {num_epochs} epochs on device {device}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_recon = 0.0\n",
        "    epoch_cat = 0.0\n",
        "    epoch_kl = 0.0\n",
        "\n",
        "    beta = get_beta_schedule(epoch, num_epochs, cfg.get('beta_schedule', 'cosine'), cfg.get('beta_initial', 1.0))\n",
        "\n",
        "    for batch_data, batch_mask in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
        "        batch_data = batch_data.to(device)\n",
        "        batch_mask = batch_mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        recon_mu, recon_logvar, recon_cat_logits, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "        total_loss, cont_loss, cat_loss, kl = vae_mixed_loss(\n",
        "            recon_mu, recon_logvar, recon_cat_logits, batch_data, batch_mask, mu, logvar,\n",
        "            cat_feature_idxs=getattr(model, 'cat_feature_idxs', [24, 25]),\n",
        "            kl_weight=beta,\n",
        "        )\n",
        "\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += total_loss.item()\n",
        "        epoch_recon += cont_loss.item()\n",
        "        epoch_cat += cat_loss.item()\n",
        "        epoch_kl += kl.item()\n",
        "\n",
        "    # Scheduler step per epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "    avg_recon = epoch_recon / len(train_loader)\n",
        "    avg_cat = epoch_cat / len(train_loader)\n",
        "    avg_kl = epoch_kl / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in val_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "            recon_mu, recon_logvar, recon_cat_logits, mu, logvar = model(batch_data, batch_mask)\n",
        "            total_loss, _, _, _ = vae_mixed_loss(recon_mu, recon_logvar, recon_cat_logits, batch_data, batch_mask, mu, logvar, getattr(model, 'cat_feature_idxs', [24,25]), kl_weight=beta)\n",
        "            val_loss += total_loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        os.makedirs('outputs', exist_ok=True)\n",
        "        torch.save(model.state_dict(), os.path.join('outputs', 'best_vae_model.pth'))\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} | Train {avg_train_loss:.4f} (Recon {avg_recon:.4f}, Cat {avg_cat:.4f}, KL {avg_kl:.4f}) | Val {avg_val_loss:.4f} | Beta {beta:.4f} | Patience {patience_counter}')\n",
        "\n",
        "    if patience_counter >= cfg.get('patience', 15):\n",
        "        print(f'Early stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "print('Training finished. Best val loss:', best_val_loss)\n",
        "print('Best model saved to outputs/best_vae_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c38e5270",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated evaluation helper: works with improved VAE forward signature\n",
        "# evaluate_imputation(model, data_loader, device, n_samples=10)\n",
        "# Returns: imputations (N, n_samples, D), originals (N, D), masks (N, D)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def evaluate_imputation(model, data_loader, device, n_samples=10, cat_value_map=None):\n",
        "    \"\"\"\n",
        "    Evaluate model by producing `n_samples` imputations per example from the model's\n",
        "    predictive outputs. Works with improved VAE that returns\n",
        "    (recon_mu, recon_logvar, recon_cat_logits, mu, logvar).\n",
        "\n",
        "    - model: torch.nn.Module\n",
        "    - data_loader: yields (batch_data, batch_mask) tensors\n",
        "    - device: torch.device\n",
        "    - n_samples: number of imputed draws per example\n",
        "    - cat_value_map: list/array mapping class ids to stored float values (default [0.0,0.5,1.0])\n",
        "\n",
        "    Returns (imputations, originals, masks)\n",
        "      - imputations: np.array shape (N, n_samples, D)\n",
        "      - originals: np.array shape (N, D)\n",
        "      - masks: np.array shape (N, D) with 1=observed\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    if cat_value_map is None:\n",
        "        cat_value_map = np.array([0.0, 0.5, 1.0])\n",
        "\n",
        "    all_imputations = []\n",
        "    all_originals = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in data_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Model forward (improved signature)\n",
        "            recon_mu, recon_logvar, recon_cat_logits, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "            B, D = batch_data.shape\n",
        "            batch_samples = np.zeros((B, n_samples, D), dtype=float)\n",
        "\n",
        "            # Precompute categorical sampling probabilities per categorical index\n",
        "            cat_probs = {}\n",
        "            for idx in getattr(model, 'cat_feature_idxs', []):\n",
        "                logits = recon_cat_logits[int(idx)]  # (B, C)\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                cat_probs[int(idx)] = probs\n",
        "\n",
        "            for s in range(n_samples):\n",
        "                # Continuous sample from predicted heteroscedastic Gaussian\n",
        "                std = torch.exp(0.5 * recon_logvar)\n",
        "                eps = torch.randn_like(recon_mu)\n",
        "                cont_sample = recon_mu + eps * std\n",
        "\n",
        "                imputed = batch_data.clone()\n",
        "                miss = (batch_mask == 0)\n",
        "                # Replace missing continuous positions\n",
        "                imputed[miss] = cont_sample[miss]\n",
        "\n",
        "                # Categorical sampling\n",
        "                # Prepare a tensor of mapped class values matching imputed dtype\n",
        "                mapped_values_tensor = torch.from_numpy(np.asarray(cat_value_map)).to(device).to(imputed.dtype)\n",
        "\n",
        "                for idx, probs in cat_probs.items():\n",
        "                    # probs: (B, C)\n",
        "                    sampled_cls = torch.multinomial(probs, num_samples=1).squeeze(1)  # (B,)\n",
        "                    # Map sampled class ids to stored values, dtype-matched\n",
        "                    mapped_vals = mapped_values_tensor[sampled_cls]\n",
        "                    miss_idx = (miss[:, idx])\n",
        "                    if miss_idx.any():\n",
        "                        # Ensure indices and tensors are on same device and dtype\n",
        "                        imputed[miss_idx, idx] = mapped_vals[miss_idx]\n",
        "\n",
        "                batch_samples[:, s, :] = imputed.cpu().numpy()\n",
        "\n",
        "            all_imputations.append(batch_samples)\n",
        "            all_originals.append(batch_data.cpu().numpy())\n",
        "            all_masks.append(batch_mask.cpu().numpy())\n",
        "\n",
        "    imputations = np.concatenate(all_imputations, axis=0)\n",
        "    originals = np.concatenate(all_originals, axis=0)\n",
        "    masks = np.concatenate(all_masks, axis=0)\n",
        "\n",
        "    return imputations, originals, masks\n",
        "\n",
        "print('Inserted updated evaluate_imputation(...) compatible with improved VAE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce3ecf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# MODEL EVALUATION (compatible with improved VAE)\n",
        "# -------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Number of imputation samples to draw per example\n",
        "n_impute_samples = 50\n",
        "\n",
        "print(\"Evaluating model on test set...\")\n",
        "# evaluate_imputation returns (N, n_samples, D), originals (N,D), masks (N,D)\n",
        "test_imputations, test_originals, test_masks = evaluate_imputation(\n",
        "    model, test_loader, device, n_samples=n_impute_samples\n",
        ")\n",
        "\n",
        "print(f\"✓ Test set evaluation completed\")\n",
        "print(f\"  - Test samples (rows): {test_imputations.shape[0]}\")\n",
        "print(f\"  - Samples per row: {test_imputations.shape[1]}\")\n",
        "print(f\"  - Features: {test_imputations.shape[2]}\")\n",
        "\n",
        "# Convert to point-estimate imputations by averaging across imputation draws\n",
        "# Shape: mean_imputations -> (N, D)\n",
        "mean_imputations = np.nanmean(test_imputations, axis=1)\n",
        "\n",
        "# Originals and masks\n",
        "originals = test_originals\n",
        "masks = test_masks\n",
        "\n",
        "# If there's a separate X_test_original in the notebook, prefer it for ground truth\n",
        "try:\n",
        "    X_test_original\n",
        "except NameError:\n",
        "    X_test_original = originals\n",
        "\n",
        "print(\"\\nCalculating comprehensive metrics...\")\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "feature_metrics = {}\n",
        "\n",
        "# Create missing mask: True where values were missing and thus were imputed\n",
        "missing_mask = (masks == 0)\n",
        "\n",
        "num_features = mean_imputations.shape[1]\n",
        "for i in range(num_features):\n",
        "    feature_name = feature_names[i] if 'feature_names' in globals() and len(feature_names) > i else f'Feature_{i+1}'\n",
        "    miss_idx = missing_mask[:, i]\n",
        "    n_missing = int(miss_idx.sum())\n",
        "    if n_missing > 0:\n",
        "        imputed_missing = mean_imputations[miss_idx, i]\n",
        "        ground_truth_missing = X_test_original[miss_idx, i]\n",
        "\n",
        "        # Filter NaNs if present\n",
        "        valid = ~np.isnan(ground_truth_missing)\n",
        "        if valid.sum() == 0:\n",
        "            continue\n",
        "        imputed_missing = imputed_missing[valid]\n",
        "        ground_truth_missing = ground_truth_missing[valid]\n",
        "\n",
        "        mse = mean_squared_error(ground_truth_missing, imputed_missing)\n",
        "        mae = mean_absolute_error(ground_truth_missing, imputed_missing)\n",
        "\n",
        "        # Correlation (robust)\n",
        "        try:\n",
        "            corr = np.corrcoef(ground_truth_missing, imputed_missing)[0, 1]\n",
        "        except Exception:\n",
        "            corr = np.nan\n",
        "\n",
        "        # Mean difference and Jensen-Shannon divergence (uses existing util)\n",
        "        try:\n",
        "            mean_diff, js_div = calculate_jsd_and_mean_diff(imputed_missing, ground_truth_missing, feature_name)\n",
        "        except Exception:\n",
        "            mean_diff, js_div = np.nan, np.nan\n",
        "\n",
        "        feature_metrics[feature_name] = {\n",
        "            'n_missing': n_missing,\n",
        "            'mse': float(mse),\n",
        "            'mae': float(mae),\n",
        "            'correlation': float(corr) if not np.isnan(corr) else np.nan,\n",
        "            'mean_difference': float(mean_diff) if not np.isnan(mean_diff) else np.nan,\n",
        "            'js_divergence': float(js_div) if not np.isnan(js_div) else np.nan,\n",
        "        }\n",
        "\n",
        "print(f\"✓ Metrics calculated for {len(feature_metrics)} features with missing values\")\n",
        "\n",
        "# Display metrics for last 4 features (or fewer if not available)\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"METRICS FOR LAST 4 FEATURES\")\n",
        "print(\"=\"*100)\n",
        "print(f\"{'Feature':<20} {'N_Miss':<8} {'MSE':<12} {'MAE':<12} {'Corr':<8} {'Mean_Diff':<12} {'JS_Div':<8}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "last_features = list(feature_metrics.keys())[-4:]\n",
        "for feature in last_features:\n",
        "    m = feature_metrics[feature]\n",
        "    corr_str = f\"{m['correlation']:.3f}\" if not np.isnan(m['correlation']) else 'nan'\n",
        "    mean_diff_str = f\"{m['mean_difference']:.4f}\" if not np.isnan(m['mean_difference']) else 'nan'\n",
        "    js_str = f\"{m['js_divergence']:.4f}\" if not np.isnan(m['js_divergence']) else 'nan'\n",
        "    print(f\"{feature:<20} {m['n_missing']:<8} {m['mse']:<12.4f} {m['mae']:<12.4f} {corr_str:<8} {mean_diff_str:<12} {js_str:<8}\")\n",
        "\n",
        "# Summary statistics\n",
        "vals_mse = [v['mse'] for v in feature_metrics.values() if not np.isnan(v['mse'])]\n",
        "vals_mae = [v['mae'] for v in feature_metrics.values() if not np.isnan(v['mae'])]\n",
        "vals_corr = [v['correlation'] for v in feature_metrics.values() if not np.isnan(v['correlation'])]\n",
        "vals_mean_diff = [v['mean_difference'] for v in feature_metrics.values() if not np.isnan(v['mean_difference'])]\n",
        "vals_js = [v['js_divergence'] for v in feature_metrics.values() if not np.isnan(v['js_divergence'])]\n",
        "\n",
        "def safe_stats(arr):\n",
        "    if len(arr) == 0:\n",
        "        return (np.nan, np.nan)\n",
        "    return (np.mean(arr), np.std(arr))\n",
        "\n",
        "mse_mean, mse_std = safe_stats(vals_mse)\n",
        "mae_mean, mae_std = safe_stats(vals_mae)\n",
        "corr_mean, corr_std = safe_stats(vals_corr)\n",
        "md_mean, md_std = safe_stats(vals_mean_diff)\n",
        "js_mean, js_std = safe_stats(vals_js)\n",
        "\n",
        "print(\"\\nSummary Statistics Across All Features:\")\n",
        "print(f\"  - Average MSE: {mse_mean:.4f} ± {mse_std:.4f}\")\n",
        "print(f\"  - Average MAE: {mae_mean:.4f} ± {mae_std:.4f}\")\n",
        "print(f\"  - Average Correlation: {corr_mean:.3f} ± {corr_std:.3f}\")\n",
        "print(f\"  - Average Mean Difference: {md_mean:.4f} ± {md_std:.4f}\")\n",
        "print(f\"  - Average JS Divergence: {js_mean:.4f} ± {js_std:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "992b4c50",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e27db6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility: generate_samples_vae\n",
        "# Produces N x S x D samples from the improved VAE (heteroscedastic continuous + categorical logits)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def generate_samples_vae(model, data_loader, device, n_samples_per_test=50, cat_value_map=None, save_path=None):\n",
        "    \"\"\"\n",
        "    Generate multiple imputed samples per row using the improved VAE.\n",
        "\n",
        "    Args:\n",
        "        model: trained VAE (forward -> recon_mu, recon_logvar, recon_cat_logits, mu, logvar)\n",
        "        data_loader: yields (batch_data, batch_mask)\n",
        "        device: torch.device\n",
        "        n_samples_per_test: int\n",
        "        cat_value_map: array-like mapping class ids -> stored float values (default [0.0,0.5,1.0])\n",
        "        save_path: optional path (npz) to save results {imputations, originals, masks}\n",
        "\n",
        "    Returns:\n",
        "        imputations: np.ndarray shape (N, S, D)\n",
        "        originals: np.ndarray shape (N, D)\n",
        "        masks: np.ndarray shape (N, D)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    if cat_value_map is None:\n",
        "        cat_value_map = np.array([0.0, 0.5, 1.0])\n",
        "\n",
        "    all_imputations = []\n",
        "    all_originals = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in data_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Forward once to get predictive parameters\n",
        "            recon_mu, recon_logvar, recon_cat_logits, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "            B, D = batch_data.shape\n",
        "            batch_samples = np.zeros((B, n_samples_per_test, D), dtype=float)\n",
        "\n",
        "            # Precompute categorical probabilities and mapped values tensor (dtype-matched)\n",
        "            cat_probs = {}\n",
        "            cat_idxs = getattr(model, 'cat_feature_idxs', [])\n",
        "            for idx in cat_idxs:\n",
        "                logits = recon_cat_logits[int(idx)]  # (B, C)\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                cat_probs[int(idx)] = probs\n",
        "\n",
        "            mapped_values_tensor = torch.from_numpy(np.asarray(cat_value_map)).to(device)\n",
        "\n",
        "            for s in range(n_samples_per_test):\n",
        "                # Sample continuous values from predicted heteroscedastic Gaussian\n",
        "                std = torch.exp(0.5 * recon_logvar)\n",
        "                eps = torch.randn_like(recon_mu)\n",
        "                cont_sample = recon_mu + eps * std\n",
        "\n",
        "                imputed = batch_data.clone()\n",
        "                miss = (batch_mask == 0)\n",
        "                # Replace missing continuous positions\n",
        "                imputed[miss] = cont_sample[miss]\n",
        "\n",
        "                # Categorical sampling: ensure mapped values have same dtype as `imputed`\n",
        "                mapped_values_tensor_cast = mapped_values_tensor.to(dtype=imputed.dtype)\n",
        "\n",
        "                for idx, probs in cat_probs.items():\n",
        "                    sampled_cls = torch.multinomial(probs, num_samples=1).squeeze(1)  # (B,)\n",
        "                    mapped_vals = mapped_values_tensor_cast[sampled_cls]\n",
        "                    miss_idx = (miss[:, idx])\n",
        "                    if miss_idx.any():\n",
        "                        imputed[miss_idx, idx] = mapped_vals[miss_idx]\n",
        "\n",
        "                batch_samples[:, s, :] = imputed.cpu().numpy()\n",
        "\n",
        "            all_imputations.append(batch_samples)\n",
        "            all_originals.append(batch_data.cpu().numpy())\n",
        "            all_masks.append(batch_mask.cpu().numpy())\n",
        "\n",
        "    imputations = np.concatenate(all_imputations, axis=0)\n",
        "    originals = np.concatenate(all_originals, axis=0)\n",
        "    masks = np.concatenate(all_masks, axis=0)\n",
        "\n",
        "    if save_path is not None:\n",
        "        np.savez_compressed(save_path, imputations=imputations, originals=originals, masks=masks)\n",
        "        print(f'Saved samples to {save_path}')\n",
        "\n",
        "    return imputations, originals, masks\n",
        "\n",
        "print('Inserted generate_samples_vae(...)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "872f9774",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# TEST EVALUATION (uses generate_samples_vae)\n",
        "# -------------------------\n",
        "print(\"=\"*70)\n",
        "print(\"TEST EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# How many samples per test row to generate\n",
        "n_samples_per_row = 100\n",
        "\n",
        "# Ensure outputs dir exists\n",
        "import os\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "# Note: `generate_samples_vae` expects a DataLoader (yields (batch_data, batch_mask)).\n",
        "# If you have a DataLoader named `test_loader`, use it. If you only have an array `X_test`,\n",
        "# create a DataLoader that yields (X_test_batch, mask_batch) before running this cell.\n",
        "\n",
        "# Prefer existing `test_loader` in the notebook\n",
        "if 'test_loader' not in globals():\n",
        "    raise RuntimeError(\"`test_loader` not found in the notebook. Create a DataLoader that yields (batch_data, batch_mask) and name it `test_loader`.\")\n",
        "\n",
        "print(f\"Generating {n_samples_per_row} samples per test row using `generate_samples_vae`...\")\n",
        "imputations, originals, masks = generate_samples_vae(\n",
        "    model=model,\n",
        "    data_loader=test_loader,\n",
        "    device=device,\n",
        "    n_samples_per_test=n_samples_per_row,\n",
        "    save_path=os.path.join('outputs', f'test_imputations_vae_{n_samples_per_row}.npz')\n",
        ")\n",
        "\n",
        "print('\\nDone.')\n",
        "print(f'  - Imputations shape: {imputations.shape}  (N, S, D)')\n",
        "print(f'  - Originals shape:   {originals.shape}  (N, D)')\n",
        "print(f'  - Masks shape:       {masks.shape}  (N, D)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcff0653",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_score = compute_score(generated_samples=imputations, set_name='test')\n",
        "print(\"Test score:\", test_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6867f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer + Diffusers setup for tabular diffusion imputation\n",
        "# 1) (Optional) Install dependencies in notebook (run if needed):\n",
        "# !pip install --upgrade pip\n",
        "# !pip install diffusers[torch] transformers accelerate --quiet\n",
        "\n",
        "# 2) Imports and Transformer denoiser definition\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from diffusers import DDPMScheduler\n",
        "\n",
        "class TransformerDenoiser(nn.Module):\n",
        "    \"\"\"Transformer-based denoiser for tabular DDPM.\n",
        "\n",
        "    Treats each feature as a token. Inputs:\n",
        "      - noisy_x: (B, D) real values\n",
        "      - cond_x: (B, D) observed values (or the same noisy_x)\n",
        "      - mask: (B, D) 1=observed, 0=missing\n",
        "      - t: tensor of timesteps (B,)\n",
        "\n",
        "    Outputs predicted noise of shape (B, D)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, emb_dim=128, n_layers=6, n_heads=8, ff_dim=512, time_dim=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        # token projection: project scalar feature value -> embedding\n",
        "        self.token_proj = nn.Linear(1, emb_dim)\n",
        "\n",
        "        # small embedding for mask and conditional observed values\n",
        "        self.mask_proj = nn.Linear(1, emb_dim)\n",
        "        self.cond_proj = nn.Linear(1, emb_dim)\n",
        "\n",
        "        # timestep embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(1, time_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(time_dim, emb_dim),\n",
        "        )\n",
        "\n",
        "        # positional embeddings for tokens (features)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, num_features, emb_dim))\n",
        "\n",
        "        # Transformer encoder stack\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dim_feedforward=ff_dim, dropout=dropout, activation='gelu')\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # output projection back to scalar per token (predict noise)\n",
        "        self.out = nn.Linear(emb_dim, 1)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.token_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.cond_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.mask_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.out.weight)\n",
        "\n",
        "    def forward(self, noisy_x, cond_x, mask, t):\n",
        "        # noisy_x, cond_x: (B, D); mask: (B, D); t: (B,) or int\n",
        "        B, D = noisy_x.shape\n",
        "        device = noisy_x.device\n",
        "\n",
        "        # Project scalars to embeddings\n",
        "        noisy_emb = self.token_proj(noisy_x.unsqueeze(-1))  # (B, D, emb_dim)\n",
        "        cond_emb = self.cond_proj(cond_x.unsqueeze(-1))\n",
        "        mask_emb = self.mask_proj(mask.unsqueeze(-1).float())\n",
        "\n",
        "        # timestep embedding, broadcast to tokens\n",
        "        if isinstance(t, int):\n",
        "            t_tensor = torch.full((B, 1), float(t), device=device)\n",
        "        else:\n",
        "            t_tensor = t.float().unsqueeze(-1)\n",
        "        time_emb = self.time_mlp(t_tensor)  # (B, emb_dim)\n",
        "        time_emb = time_emb.unsqueeze(1).expand(-1, D, -1)\n",
        "\n",
        "        x = noisy_emb + cond_emb + mask_emb + time_emb + self.pos_emb\n",
        "\n",
        "        # Transformer expects shape (seq_len, batch, emb_dim)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)  # (D, B, emb_dim)\n",
        "        x = x.permute(1, 0, 2)  # (B, D, emb_dim)\n",
        "\n",
        "        out = self.out(x).squeeze(-1)  # (B, D)\n",
        "        return out\n",
        "\n",
        "# 3) Diffusion training + sampling helpers using diffusers' scheduler\n",
        "class TransformerDiffusionImputer:\n",
        "    def __init__(self, denoiser: nn.Module, num_train_timesteps=1000, beta_schedule='linear', device='cpu'):\n",
        "        self.denoiser = denoiser\n",
        "        self.device = device\n",
        "        self.num_train_timesteps = num_train_timesteps\n",
        "        # Use diffusers scheduler for betas/schedule management\n",
        "        if beta_schedule == 'linear':\n",
        "            self.scheduler = DDPMScheduler(num_train_timesteps=num_train_timesteps, beta_schedule='linear')\n",
        "        else:\n",
        "            # fallback to default linear\n",
        "            self.scheduler = DDPMScheduler(num_train_timesteps=num_train_timesteps, beta_schedule='linear')\n",
        "\n",
        "        # move model\n",
        "        self.denoiser.to(device)\n",
        "\n",
        "    def p_losses(self, x_start, cond_x, mask, t, noise=None):\n",
        "        # x_start: (B, D)\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "        # Use scheduler helper to produce noisy sample\n",
        "        noisy = self.scheduler.add_noise(x_start, noise, t)\n",
        "        noisy = noisy.to(self.device)\n",
        "        cond_x = cond_x.to(self.device)\n",
        "        mask = mask.to(self.device)\n",
        "\n",
        "        # Predict noise with denoiser\n",
        "        pred_noise = self.denoiser(noisy, cond_x, mask, t)\n",
        "\n",
        "        # Loss computed only on missing positions (we want to impute missing entries)\n",
        "        loss = F.mse_loss(pred_noise * (1 - mask), noise * (1 - mask))\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, cond_x, mask, batch_size=64, device=None):\n",
        "        device = device or self.device\n",
        "        self.denoiser.eval()\n",
        "        B, D = cond_x.shape\n",
        "        # Start from standard normal\n",
        "        sample = torch.randn((B, D), device=device)\n",
        "\n",
        "        for i in reversed(range(self.scheduler.num_train_timesteps)):\n",
        "            t = torch.full((B,), i, device=device, dtype=torch.long)\n",
        "            pred_noise = self.denoiser(sample, cond_x.to(device), mask.to(device), t)\n",
        "            # scheduler.step expects model_output and returns prev_sample\n",
        "            step_output = self.scheduler.step(model_output=pred_noise, timestep=i, sample=sample)\n",
        "            prev_sample = step_output.prev_sample\n",
        "            # Re-impose observed values after the denoising step\n",
        "            prev_sample = prev_sample * (1 - mask.to(device)) + cond_x.to(device) * mask.to(device)\n",
        "            sample = prev_sample\n",
        "        return sample.cpu().numpy()\n",
        "\n",
        "# 4) Example training loop (short-run template)\n",
        "# Usage notes:\n",
        "#  - You must have train_loader/val_loader yielding (batch_data, batch_mask) already defined\n",
        "#  - cond_x is the batch_data where observed entries are used by the model; mask indicates observed\n",
        "#\n",
        "# Example hyperparameters and training skeleton (do not run long training automatically):\n",
        "\n",
        "print('Inserted Transformer-based diffusion imputer (definition + helpers).')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
