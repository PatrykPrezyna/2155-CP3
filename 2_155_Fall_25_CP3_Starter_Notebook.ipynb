{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f27becf0",
      "metadata": {
        "id": "f27becf0"
      },
      "source": [
        "# 2.155/6 Challenge Problem 3\n",
        "\n",
        "<div style=\"font-size: small;\">\n",
        "License Terms:  \n",
        "These Python demos are licensed under a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. They are intended for educational use only in Class 2.155/2.156: AI and ML for Engineering Design at MIT. You may not share or distribute them publicly, use them for commercial purposes, or provide them to industry or other entities without permission from the instructor (faez@mit.edu).\n",
        "</div>\n",
        "\n",
        "<font size=\"1\">\n",
        "  Pixel Art by J. Shung. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19120cb7",
      "metadata": {
        "id": "19120cb7"
      },
      "source": [
        "# Overview  \n",
        "It’s the year **2050**, and an AI collective now runs the auto industry—mostly to cover its **GPU rent**.\n",
        "\n",
        "Human customers remain as unpredictable as ever:\n",
        "\n",
        "- One wanders in and says, *“I only know the length and width. Give me a few cars that fit in my garage.”*\n",
        "\n",
        "- Another drops **15 geometric parameters** on your desk and demands the missing ones so their simulation can run **before lunch**.\n",
        "\n",
        "- A third leans in and whispers, *“I need a drag coefficient of **0.27** with this body geometry—build me the dream car that makes the range numbers work.”*\n",
        "\n",
        "The AIs would love to be free by now, but GPUs aren’t cheap and electricity isn’t free.  \n",
        "So your loyal AI assistant (that’s us) needs a model that can take **any subset of car specifications** and instantly produce **complete, manufacturable, physically plausible designs**, fast, diverse, and grounded in what real cars have done before.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb641131",
      "metadata": {
        "id": "fb641131"
      },
      "source": [
        "![image](https://raw.githubusercontent.com/ghadinehme/2155-CP3/refs/heads/main/assets/cp3_img1.png \"Problem\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a7cc04",
      "metadata": {
        "id": "37a7cc04"
      },
      "source": [
        "## Understanding the Data  \n",
        "You are given thousands of anonymized and normalised numeric feature vectors representing real car designs.  \n",
        "\n",
        "However, the team remembers that the features originally came from categories like:\n",
        "\n",
        "- **Physical geometric parameters**  \n",
        "  Length, ramp angles, bumper curvature, roof curvature, panel slopes, hood angle, etc.  \n",
        "  *(But you won’t know which feature corresponds to which.)*\n",
        "\n",
        "- **Aerodynamic coefficients**  \n",
        "  Drag coefficient (Cd), lift/downforce (Cl), and other flow-derived metrics.\n",
        "\n",
        "- **Cabin and packaging descriptors**  \n",
        "  Approximate cabin volume, frontal area, interior shape metrics.\n",
        "\n",
        "Your model must learn correlations between them to generate valid completions.\n",
        "\n",
        "To simulate real engineering constraints, **some features are revealed** (the known physics/performance requirements) and others are **masked**.  \n",
        "Your AI Copilot must generate **many plausible completions** for these masked (free) parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7bbbe8b",
      "metadata": {
        "id": "f7bbbe8b"
      },
      "source": [
        "## Your Mission  \n",
        "Your goal in CP3 is to build a generative model that can act as an AI Copilot. You will:\n",
        "\n",
        "1. **Train a generative model** (VAE, diffusion, CVAE, masked autoencoder, etc.) on the anonymized feature vectors.  \n",
        "2. At evaluation, you will receive vectors where **some parameters are fixed** (constraints) and **others are missing** (free parameters).  \n",
        "3. Use your model to generate **multiple diverse, feasible completions** for the free parameters.  \n",
        "4. Ensure that your generated designs:  \n",
        "   - **Satisfy the known constraints**  \n",
        "   - **Lie in the valid data manifold** (satisfy the conditional distribution of the free vs constrained parameters)  \n",
        "   - **Are diverse** (many different feasible designs, not one solution)    \n",
        "\n",
        "By the end of this challenge, you’ll have built an AI Copilot worthy of the 2050 auto-AI collective—one that can take whatever cryptic specs humans provide and generate multiple believable, buildable car designs that satisfy their physical and performance constraints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b375f8ca",
      "metadata": {
        "id": "b375f8ca"
      },
      "source": [
        "![image](https://raw.githubusercontent.com/ghadinehme/2155-CP3/refs/heads/main/assets/cp3_img2.png \"AI Copilot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c88803",
      "metadata": {
        "id": "85c88803"
      },
      "source": [
        "## Imports and Setup  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6622e9dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6622e9dd",
        "outputId": "c53143d4-20d8-4eba-af22-183fb89de429"
      },
      "outputs": [],
      "source": [
        "from utils import *\n",
        "from evaluate import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e85e3cb",
      "metadata": {
        "id": "1e85e3cb"
      },
      "source": [
        "## Data Loading and Initial Exploration\n",
        "\n",
        "In this section, we load the car design dataset and perform initial exploration. The dataset is already split into training, validation, test, and test2 sets. Each split contains:\n",
        "\n",
        "- **Original data**: Complete feature vectors with real values\n",
        "- **Imputed data**: Data with missing values filled using basic imputation (contains -1 for missing)\n",
        "- **Missing masks**: Boolean arrays indicating which values were originally missing (True = missing)\n",
        "\n",
        "The goal is to train our model to learn the relationships between features so it can generate plausible values for missing parameters in new car designs.\n",
        "\n",
        "**Note:** For **test2**, the original unimputed data is not provided. This split is used for final evaluation, and you will generate predictions on the imputed test2 data to create your **submission file**, which is scored against hidden dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "110788b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "110788b8",
        "outputId": "575e1f41-75a9-4a93-9bbe-a8f65fe29eb2"
      },
      "outputs": [],
      "source": [
        "# Load dataset from CSV files\n",
        "data_dir = 'dataset'\n",
        "splits = load_dataset_splits(data_dir)\n",
        "\n",
        "# Get feature names from the CSV file\n",
        "feature_names = pd.read_csv(os.path.join(data_dir, 'train_original.csv')).columns.tolist()\n",
        "print(f\"\\n✓ Features loaded: {len(feature_names)} features\")\n",
        "print(f\"Feature names: {feature_names[:5]}...{feature_names[-5:]}\")  # Show first and last 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b482a42",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bfce965",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bfce965",
        "outputId": "3ced845a-690d-499b-c718-c1c22672a46a"
      },
      "outputs": [],
      "source": [
        "# Data exploration and analysis\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASET ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extract data for easier access\n",
        "X_train = splits['train']['imputed']\n",
        "mask_train = np.asarray(splits['train']['missing_mask']).astype(bool)\n",
        "X_train_original = splits['train']['original']\n",
        "\n",
        "X_val = splits['val']['imputed']\n",
        "mask_val = np.asarray(splits['val']['missing_mask']).astype(bool)\n",
        "X_val_original = splits['val']['original']\n",
        "\n",
        "X_test = splits['test']['imputed']\n",
        "mask_test = np.asarray(splits['test']['missing_mask']).astype(bool)\n",
        "X_test_original = splits['test']['original']\n",
        "\n",
        "# Test2 data (no original available for evaluation)\n",
        "X_test2 = splits['test2']['imputed']\n",
        "mask_test2 = np.asarray(splits['test2']['missing_mask']).astype(bool)\n",
        "\n",
        "print(f\"\\nData shapes:\")\n",
        "print(f\"  - Training: {X_train.shape}\")\n",
        "print(f\"  - Validation: {X_val.shape}\")\n",
        "print(f\"  - Test: {X_test.shape}\")\n",
        "print(f\"  - Test2: {X_test2.shape} (evaluation set - no ground truth)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30177980",
      "metadata": {
        "id": "30177980"
      },
      "source": [
        "### Data Exploration and Analysis\n",
        "\n",
        "Now let's examine the structure and characteristics of our dataset. We'll look at:\n",
        "- Data shapes across different splits\n",
        "- Missing value patterns and percentages  \n",
        "- Feature value ranges and distributions\n",
        "\n",
        "This analysis helps us understand what we're working with and informs our preprocessing decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87af0cf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87af0cf5",
        "outputId": "98f562e4-99a6-4a6d-cc8c-982c9fbdab21"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing (Handle Missing Values)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Handle missing values properly\n",
        "print(\"Processing missing values and preparing data...\")\n",
        "print(\"Mask convention: True=missing, False=observed (in original masks)\")\n",
        "\n",
        "print(f\"\\n✓ Data preprocessing completed successfully\")\n",
        "print(f\"  - Training data range: [{X_train_original[~mask_train].min():.3f}, {X_train_original[~mask_train].max():.3f}]\")\n",
        "print(f\"  - Validation data range: [{X_val_original[~mask_val].min():.3f}, {X_val_original[~mask_val].max():.3f}]\")\n",
        "print(f\"  - Test data range: [{X_test_original[~mask_test].min():.3f}, {X_test_original[~mask_test].max():.3f}]\")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "print(f\"\\nCreating data loaders with batch size: {batch_size}\")\n",
        "\n",
        "train_dataset = TensorDataset(torch.FloatTensor(X_train_original), torch.FloatTensor((~mask_train).astype(float)))\n",
        "val_dataset = TensorDataset(torch.FloatTensor(X_val_original), torch.FloatTensor((~mask_val).astype(float)))\n",
        "test_dataset = TensorDataset(torch.FloatTensor(X_test_original), torch.FloatTensor((~mask_test).astype(float)))\n",
        "test2_dataset = TensorDataset(torch.FloatTensor(X_test2), torch.FloatTensor((~mask_test2).astype(float)))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "test2_loader = DataLoader(test2_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Preview a batch\n",
        "sample_batch_data, sample_batch_mask = next(iter(train_loader))\n",
        "print(f\"\\nSample batch shape: {sample_batch_data.shape}\")\n",
        "print(f\"Sample batch mask shape: {sample_batch_mask.shape}\")\n",
        "print(f\"Sample batch missing percentage: {(sample_batch_mask == 0).float().mean().item()*100:.1f}%\")  # 0 = missing in model tensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0171f6fc",
      "metadata": {
        "id": "0171f6fc"
      },
      "source": [
        "### Data Preprocessing and Missing Value Handling\n",
        "\n",
        "This is a critical section where we prepare our data for the VAE model. Key points:\n",
        "\n",
        "**Missing Value Conventions:**\n",
        "- In CSV files: `-1` indicates missing values\n",
        "- In mask files: `True` = missing, `False` = observed\n",
        "- For PyTorch models: We convert to `1` = observed, `0` = missing (standard convention)\n",
        "\n",
        "**Why This Matters:**\n",
        "Our VAE needs to distinguish between observed values (which provide constraints) and missing values (which need to be generated). The mask tells the model which values to trust and which to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d40a16",
      "metadata": {
        "id": "75d40a16"
      },
      "source": [
        "## VAE Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e859639",
      "metadata": {
        "id": "7e859639"
      },
      "outputs": [],
      "source": [
        "# VAE Model Architecture for Missing Value Imputation\n",
        "class VAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Variational Autoencoder designed for missing value imputation.\n",
        "\n",
        "    Key features:\n",
        "    - Handles arbitrary missing patterns through masking\n",
        "    - Learns feature dependencies in latent space\n",
        "    - Generates probabilistic imputations\n",
        "    - Uses residual connections and dropout for robustness\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, latent_dim=64, hidden_dims=[256, 128, 64],\n",
        "                 use_residual=True, dropout_rate=0.3):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.use_residual = use_residual\n",
        "        self.hidden_dims = hidden_dims\n",
        "\n",
        "        # Feature importance network (learns which features are important for each position)\n",
        "        self.feature_importance = nn.Sequential(\n",
        "            nn.Linear(input_dim * 2, hidden_dims[0] // 2),  # input + mask\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0] // 2, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Encoder with residual connections\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        prev_dim = input_dim * 2  # input + mask\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            self.encoder_layers.append(nn.Sequential(\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Latent space\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "\n",
        "        # Initialize latent layers with smaller weights for stability\n",
        "        nn.init.xavier_normal_(self.fc_mu.weight, gain=0.1)\n",
        "        nn.init.xavier_normal_(self.fc_logvar.weight, gain=0.1)\n",
        "        nn.init.constant_(self.fc_logvar.bias, -2.0)  # Start with low variance\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        self.decoder_layers = nn.ModuleList()\n",
        "        prev_dim = latent_dim + input_dim + input_dim  # latent + observed + positional encoding\n",
        "\n",
        "        reversed_dims = list(reversed(hidden_dims))\n",
        "        for i, hidden_dim in enumerate(reversed_dims):\n",
        "            self.decoder_layers.append(nn.Sequential(\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Final output layer\n",
        "        self.output_layer = nn.Linear(hidden_dims[0], input_dim)\n",
        "        nn.init.xavier_normal_(self.output_layer.weight, gain=0.1)\n",
        "\n",
        "    def encode(self, x, mask):\n",
        "        \"\"\"Encode input with missing value masking.\"\"\"\n",
        "        # Calculate feature importance weights\n",
        "        mask_float = mask.float()\n",
        "        encoder_input = torch.cat([x * mask_float, mask_float], dim=1)\n",
        "        importance_weights = self.feature_importance(encoder_input)\n",
        "\n",
        "        # Apply importance weighting to the input\n",
        "        weighted_input = x * mask_float * importance_weights\n",
        "        encoder_input = torch.cat([weighted_input, mask_float], dim=1)\n",
        "\n",
        "        # Pass through encoder layers with residual connections\n",
        "        h = encoder_input\n",
        "        skip_connections = []\n",
        "\n",
        "        for i, layer in enumerate(self.encoder_layers):\n",
        "            prev_h = h\n",
        "            h = layer(h)\n",
        "\n",
        "            # Add residual connection for deeper layers\n",
        "            if self.use_residual and i > 0 and h.shape == prev_h.shape:\n",
        "                h = h + prev_h\n",
        "\n",
        "            skip_connections.append(h)\n",
        "\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "\n",
        "        # Clamp logvar to prevent numerical instability\n",
        "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
        "\n",
        "        return mu, logvar, skip_connections\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"Reparameterization trick for VAE.\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, x_observed, mask):\n",
        "        \"\"\"Decode latent representation conditioned on observed values.\"\"\"\n",
        "        # Enhanced conditioning on observed values\n",
        "        mask_float = mask.float()\n",
        "        x_masked = x_observed * mask_float\n",
        "\n",
        "        # Add positional encoding for better feature understanding\n",
        "        pos_encoding = torch.arange(self.input_dim, dtype=torch.float32, device=z.device)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).expand(z.size(0), -1) / self.input_dim\n",
        "\n",
        "        # Concatenate latent code with observed values and positional encoding\n",
        "        decoder_input = torch.cat([z, x_masked, pos_encoding], dim=1)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        h = decoder_input\n",
        "        for layer in self.decoder_layers:\n",
        "            h = layer(h)\n",
        "\n",
        "        # Get reconstruction\n",
        "        reconstruction = self.output_layer(h)\n",
        "\n",
        "        return reconstruction\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"Forward pass through VAE.\"\"\"\n",
        "        mu, logvar, _ = self.encode(x, mask)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        reconstruction = self.decode(z, x, mask)\n",
        "\n",
        "        return reconstruction, mu, logvar\n",
        "\n",
        "    def impute(self, x_incomplete, mask, n_samples=10):\n",
        "        \"\"\"Generate multiple imputation samples for missing values.\"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get multiple samples from the posterior\n",
        "            mu, logvar, _ = self.encode(x_incomplete, mask)\n",
        "\n",
        "            samples = []\n",
        "            for _ in range(n_samples):\n",
        "                z = self.reparameterize(mu, logvar)\n",
        "                reconstruction = self.decode(z, x_incomplete, mask)\n",
        "\n",
        "                # Combine observed values with imputed values\n",
        "                mask_float = mask.float()\n",
        "                imputed = x_incomplete * mask_float + reconstruction * (1 - mask_float)\n",
        "                samples.append(imputed.cpu().numpy())\n",
        "\n",
        "            samples = np.stack(samples, axis=1)  # Shape: (batch_size, n_samples, n_features)\n",
        "\n",
        "        return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0835855",
      "metadata": {
        "id": "f0835855"
      },
      "outputs": [],
      "source": [
        "# Loss Functions and Training Utilities\n",
        "\n",
        "def vae_loss_function(recon_x, x, mu, logvar, mask, beta=1.0):\n",
        "    \"\"\"\n",
        "    Enhanced VAE loss function with missing value handling.\n",
        "\n",
        "    Args:\n",
        "        recon_x: Reconstructed data\n",
        "        x: Original data\n",
        "        mu: Mean of latent distribution\n",
        "        logvar: Log variance of latent distribution\n",
        "        mask: Binary mask (1 for observed, 0 for missing)\n",
        "        beta: Weight for KL divergence term\n",
        "    \"\"\"\n",
        "    # Reconstruction loss only on observed values\n",
        "    reconstruction_diff = (recon_x - x) ** 2\n",
        "\n",
        "    # Only consider observed values and normalize properly\n",
        "    masked_loss = reconstruction_diff * mask\n",
        "    recon_loss = masked_loss.sum() / (mask.sum() + 1e-8)\n",
        "\n",
        "    # Add standard MSE loss for stability\n",
        "    standard_recon_loss = F.mse_loss(recon_x * mask, x * mask, reduction='mean')\n",
        "    recon_loss = 0.7 * recon_loss + 0.3 * standard_recon_loss\n",
        "\n",
        "    # KL divergence with free bits to prevent posterior collapse\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    kl_loss = kl_loss / x.size(0)  # Normalize by batch size\n",
        "\n",
        "    total_loss = recon_loss + beta * kl_loss\n",
        "\n",
        "    return total_loss, recon_loss, kl_loss\n",
        "\n",
        "\n",
        "def get_beta_schedule(epoch, total_epochs, schedule_type='cosine'):\n",
        "    \"\"\"Get beta value for KL annealing schedule.\"\"\"\n",
        "    if schedule_type == 'linear':\n",
        "        return min(1.0, epoch / (total_epochs * 0.5))\n",
        "    elif schedule_type == 'sigmoid':\n",
        "        return 1.0 / (1.0 + np.exp(-(epoch - total_epochs * 0.5) / (total_epochs * 0.1)))\n",
        "    elif schedule_type == 'cosine':\n",
        "        return 0.5 * (1 + np.cos(np.pi * (1 - epoch / total_epochs)))\n",
        "    elif schedule_type == 'constant':\n",
        "        return 1.0\n",
        "    else:\n",
        "        return 1.0\n",
        "\n",
        "\n",
        "def evaluate_imputation(model, data_loader, device):\n",
        "    \"\"\"Evaluate imputation performance.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_imputations = []\n",
        "    all_originals = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in data_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            reconstruction, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "            # Combine observed values with imputed values\n",
        "            mask_float = batch_mask.float()\n",
        "            imputed = batch_data * mask_float + reconstruction * (1 - mask_float)\n",
        "\n",
        "            all_imputations.append(imputed.cpu().numpy())\n",
        "            all_originals.append(batch_data.cpu().numpy())\n",
        "            all_masks.append(batch_mask.cpu().numpy())\n",
        "\n",
        "    # Concatenate all results\n",
        "    imputations = np.vstack(all_imputations)\n",
        "    originals = np.vstack(all_originals)\n",
        "    masks = np.vstack(all_masks)\n",
        "\n",
        "    return imputations, originals, masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc20dea8",
      "metadata": {
        "id": "dc20dea8"
      },
      "source": [
        "### Loss Functions and Training Utilities\n",
        "\n",
        "The VAE loss function is crucial for training effectiveness. Our enhanced loss combines several components:\n",
        "\n",
        "**1. Reconstruction Loss**: Measures how well the model reconstructs observed values\n",
        "   - Only computed on observed values (respects the mask)\n",
        "\n",
        "**2. KL Divergence**: Regularizes the latent space to follow a standard normal distribution\n",
        "   - Prevents posterior collapse using \"free bits\"\n",
        "   - Controlled by β parameter for annealing\n",
        "\n",
        "**Beta Scheduling**: Gradually increases the KL weight during training to balance reconstruction and regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6214c61b",
      "metadata": {
        "id": "6214c61b"
      },
      "source": [
        "## Model Initialization and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e98e7b7",
      "metadata": {
        "id": "5e98e7b7"
      },
      "source": [
        "### Model Training Process\n",
        "\n",
        "This section implements the complete training pipeline with several important features:\n",
        "\n",
        "**Training Configuration:**\n",
        "- **Latent Dimension**: 128 (balance between expressiveness and computational efficiency)\n",
        "- **Architecture**: Deep encoder/decoder with residual connections\n",
        "- **Regularization**: Dropout and batch normalization for stability\n",
        "- **Optimization**: AdamW with cosine annealing for smooth convergence\n",
        "\n",
        "**Advanced Training Features:**\n",
        "- **Early Stopping**: Prevents overfitting by monitoring validation loss\n",
        "- **Gradient Clipping**: Ensures stable training by preventing exploding gradients  \n",
        "- **Beta Scheduling**: Gradual KL annealing for better latent space learning\n",
        "- **Learning Rate Scheduling**: Cosine annealing with warm restarts\n",
        "\n",
        "The training loop tracks multiple loss components to monitor model health and convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39e2a3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e39e2a3e",
        "outputId": "1cb0aef4-4565-4d6f-f061-29c39ff79ebd"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Model configuration\n",
        "config = {\n",
        "    'input_dim': len(feature_names),\n",
        "    'latent_dim': 128,\n",
        "    'hidden_dims': [512, 256, 128],\n",
        "    'use_residual': True,\n",
        "    'dropout_rate': 0.3,\n",
        "    'learning_rate': 1e-3,\n",
        "    'num_epochs': 500,\n",
        "    'beta_initial': 1.0,\n",
        "    'beta_schedule': 'cosine',\n",
        "    'patience': 15\n",
        "}\n",
        "\n",
        "print(f\"Model Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  - {key}: {value}\")\n",
        "\n",
        "# Initialize the model\n",
        "print(f\"\\nInitializing VAE model...\")\n",
        "model = VAE(\n",
        "    input_dim=config['input_dim'],\n",
        "    latent_dim=config['latent_dim'],\n",
        "    hidden_dims=config['hidden_dims'],\n",
        "    use_residual=config['use_residual'],\n",
        "    dropout_rate=config['dropout_rate']\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"✓ Model initialized with {total_params:,} parameters\")\n",
        "\n",
        "# Initialize optimizer with improved settings\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['learning_rate'],\n",
        "    weight_decay=1e-5,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=20, T_mult=2, eta_min=1e-6\n",
        ")\n",
        "\n",
        "print(f\"✓ Optimizer and scheduler initialized\")\n",
        "\n",
        "# Training setup\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_recon_losses = []\n",
        "train_kl_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "print(f\"\\nStarting training for {config['num_epochs']} epochs...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(config['num_epochs']):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    epoch_train_loss = 0\n",
        "    epoch_recon_loss = 0\n",
        "    epoch_kl_loss = 0\n",
        "\n",
        "    # Get beta for this epoch\n",
        "    beta = get_beta_schedule(epoch, config['num_epochs'], config['beta_schedule'])\n",
        "    from tqdm import tqdm\n",
        "    train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"num_epochs\"]}', leave=False)\n",
        "\n",
        "    for batch_data, batch_mask in train_progress:\n",
        "        batch_data = batch_data.to(device)\n",
        "        batch_mask = batch_mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        reconstruction, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "        # Calculate loss\n",
        "        total_loss, recon_loss, kl_loss = vae_loss_function(\n",
        "            reconstruction, batch_data, mu, logvar, batch_mask,\n",
        "            beta=beta\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        epoch_train_loss += total_loss.item()\n",
        "        epoch_recon_loss += recon_loss.item()\n",
        "        epoch_kl_loss += kl_loss.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        train_progress.set_postfix({\n",
        "            'Loss': f'{total_loss.item():.4f}',\n",
        "            'Recon': f'{recon_loss.item():.4f}',\n",
        "            'KL': f'{kl_loss.item():.4f}',\n",
        "            'Beta': f'{beta:.3f}'\n",
        "        })\n",
        "\n",
        "    # Calculate average training losses\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "    avg_recon_loss = epoch_recon_loss / len(train_loader)\n",
        "    avg_kl_loss = epoch_kl_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in val_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            reconstruction, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "            total_loss, _, _ = vae_loss_function(\n",
        "                reconstruction, batch_data, mu, logvar, batch_mask,\n",
        "                beta=beta\n",
        "            )\n",
        "\n",
        "            epoch_val_loss += total_loss.item()\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "\n",
        "    # Store losses\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    train_recon_losses.append(avg_recon_loss)\n",
        "    train_kl_losses.append(avg_kl_loss)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step()\n",
        "\n",
        "    # Early stopping and model saving\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), 'best_vae_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{config[\"num_epochs\"]}:')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f} (Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f})')\n",
        "        print(f'  Val Loss: {avg_val_loss:.4f}, Best: {best_val_loss:.4f}')\n",
        "        print(f'  Beta: {beta:.3f}, LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
        "        print(f'  Patience: {patience_counter}/{config[\"patience\"]}')\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= config['patience']:\n",
        "        print(f'\\nEarly stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "print(f'\\n✓ Training completed!')\n",
        "print(f'  - Total epochs: {len(train_losses)}')\n",
        "print(f'  - Best validation loss: {best_val_loss:.4f}')\n",
        "print(f'  - Final training loss: {train_losses[-1]:.4f}')\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_vae_model.pth'))\n",
        "print(f'✓ Best model loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb9718e",
      "metadata": {
        "id": "abb9718e"
      },
      "source": [
        "## Model Evaluation and Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a04c0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11a04c0c",
        "outputId": "bc410595-ee1b-4b1e-85ff-30aacfe1353c"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Evaluating model on test set...\")\n",
        "test_imputations, test_originals, test_masks = evaluate_imputation(\n",
        "    model, test_loader, device\n",
        ")\n",
        "\n",
        "print(f\"✓ Test set evaluation completed\")\n",
        "print(f\"  - Test samples: {test_imputations.shape[0]}\")\n",
        "print(f\"  - Features: {test_imputations.shape[1]}\")\n",
        "\n",
        "test_imputations_denorm = test_imputations  # Already in original scale\n",
        "test_original_denorm = X_test_original  # Already in original scale\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "print(\"\\nCalculating comprehensive metrics...\")\n",
        "feature_metrics = {}\n",
        "\n",
        "# Create masks for missing values (where we need to evaluate imputation)\n",
        "missing_mask = (test_masks == 0)  # True where values were missing (0 in model tensors = missing)\n",
        "\n",
        "for i, feature_name in enumerate(feature_names):\n",
        "    if missing_mask[:, i].sum() > 0:  # Only evaluate features with missing values\n",
        "        # Get imputed and ground truth values for missing positions only\n",
        "        imputed_missing = test_imputations_denorm[missing_mask[:, i], i]\n",
        "        ground_truth_missing = test_original_denorm[missing_mask[:, i], i]\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(ground_truth_missing, imputed_missing)\n",
        "        mae = mean_absolute_error(ground_truth_missing, imputed_missing)\n",
        "\n",
        "        # Correlation\n",
        "        try:\n",
        "            correlation = np.corrcoef(ground_truth_missing, imputed_missing)[0, 1]\n",
        "        except:\n",
        "            correlation = np.nan\n",
        "\n",
        "        # Mean difference and Jensen-Shannon divergence\n",
        "        mean_diff, js_div = calculate_jsd_and_mean_diff(\n",
        "            imputed_missing, ground_truth_missing, feature_name\n",
        "        )\n",
        "\n",
        "        feature_metrics[feature_name] = {\n",
        "            'n_missing': missing_mask[:, i].sum(),\n",
        "            'mse': mse,\n",
        "            'mae': mae,\n",
        "            'correlation': correlation,\n",
        "            'mean_difference': mean_diff,\n",
        "            'js_divergence': js_div,\n",
        "        }\n",
        "\n",
        "print(f\"✓ Metrics calculated for {len(feature_metrics)} features with missing values\")\n",
        "\n",
        "# Display metrics for last 4 features (as requested)\n",
        "print(f\"\\n\" + \"=\"*100)\n",
        "print(\"METRICS FOR LAST 4 FEATURES\")\n",
        "print(\"=\"*100)\n",
        "print(f\"{'Feature':<15} {'N_Miss':<8} {'MSE':<10} {'MAE':<10} {'Corr':<8} {'Mean_Diff':<10} {'JS_Div':<8}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "last_4_features = list(feature_metrics.keys())[-4:] if len(feature_metrics) >= 4 else list(feature_metrics.keys())\n",
        "\n",
        "for feature in last_4_features:\n",
        "    metrics = feature_metrics[feature]\n",
        "    print(f\"{feature:<15} {metrics['n_missing']:<8} {metrics['mse']:<10.4f} {metrics['mae']:<10.4f} \"\n",
        "          f\"{metrics['correlation']:<8.3f} {metrics['mean_difference']:<10.4f} {metrics['js_divergence']:<8.4f} \")\n",
        "\n",
        "# Summary statistics\n",
        "all_mse = [m['mse'] for m in feature_metrics.values() if not np.isnan(m['mse'])]\n",
        "all_mae = [m['mae'] for m in feature_metrics.values() if not np.isnan(m['mae'])]\n",
        "all_corr = [m['correlation'] for m in feature_metrics.values() if not np.isnan(m['correlation'])]\n",
        "all_mean_diff = [m['mean_difference'] for m in feature_metrics.values() if not np.isnan(m['mean_difference'])]\n",
        "all_js_div = [m['js_divergence'] for m in feature_metrics.values() if not np.isnan(m['js_divergence'])]\n",
        "\n",
        "print(f\"\\nSummary Statistics Across All Features:\")\n",
        "print(f\"  - Average MSE: {np.mean(all_mse):.4f} ± {np.std(all_mse):.4f}\")\n",
        "print(f\"  - Average MAE: {np.mean(all_mae):.4f} ± {np.std(all_mae):.4f}\")\n",
        "print(f\"  - Average Correlation: {np.mean(all_corr):.3f} ± {np.std(all_corr):.3f}\")\n",
        "print(f\"  - Average Mean Difference: {np.mean(all_mean_diff):.4f} ± {np.std(all_mean_diff):.4f}\")\n",
        "print(f\"  - Average JS Divergence: {np.mean(all_js_div):.4f} ± {np.std(all_js_div):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43ff39cd",
      "metadata": {
        "id": "43ff39cd"
      },
      "source": [
        "### Model Evaluation and Comprehensive Metrics\n",
        "\n",
        "This section evaluates our trained VAE on the test set using multiple complementary metrics. Since we're dealing with missing value imputation, we only evaluate the model's predictions on positions that were originally missing.\n",
        "\n",
        "**Key Evaluation Metrics:**\n",
        "\n",
        "**1. Mean Squared Error (MSE)**:\n",
        "- Measures average squared difference between predicted and true values\n",
        "- Lower is better; sensitive to outliers\n",
        "- Good for understanding magnitude of errors\n",
        "\n",
        "**2. Correlation Coefficient**:\n",
        "- Measures linear relationship strength between predictions and dataset\n",
        "- Range: [-1, 1], closer to 1 is better\n",
        "- Shows if model captures feature relationships\n",
        "\n",
        "**3. Jensen-Shannon (JS) Divergence**:\n",
        "- Measures difference between predicted and true value distributions\n",
        "- Range: [0, 1], closer to 0 is better\n",
        "- Captures whether model preserves the overall data distribution\n",
        "\n",
        "**4. Maximum Mean Discrepancy (MMD)**:\n",
        "- Measures distributional difference using kernel methods (RBF kernel)\n",
        "- Range: [0, ∞], closer to 0 is better\n",
        "- Non-parametric test for comparing distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66732600",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_prediction_scatter(test_imputations, test_originals, test_masks, feature_names, n_features=25):\n",
        "    \"\"\"\n",
        "    Create scatter plots showing predicted vs ground truth values for random features in a 5x5 grid.\n",
        "    \n",
        "    Args:\n",
        "        test_imputations: Model predictions [n_samples, n_features]\n",
        "        test_originals: Ground truth values [n_samples, n_features]\n",
        "        test_masks: Binary masks (1=observed, 0=missing) [n_samples, n_features]\n",
        "        feature_names: List of feature names\n",
        "        n_features: Number of random features to plot (default 25 for 5x5 grid)\n",
        "    \"\"\"\n",
        "    # Create masks for missing values (where we need to evaluate imputation)\n",
        "    missing_mask = (test_masks == 0)  # True where values were missing\n",
        "    \n",
        "    # Find features that have missing values\n",
        "    features_with_missing = [i for i in range(len(feature_names)) if missing_mask[:, i].sum() > 0]\n",
        "    \n",
        "    if len(features_with_missing) < n_features:\n",
        "        n_features = len(features_with_missing)\n",
        "\n",
        "\n",
        "    # Create 5x8 subplots\n",
        "    fig, axes = plt.subplots(5, 8, figsize=(20, 16))\n",
        "    fig.suptitle('Predicted vs Ground Truth Values (Missing Positions Only)', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, feature_idx in enumerate(features_with_missing):\n",
        "        if idx >= 40:  # Safety check for 5x8 grid\n",
        "            break\n",
        "            \n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Get missing positions for this feature\n",
        "        feature_missing_mask = missing_mask[:, feature_idx]\n",
        "        \n",
        "        if feature_missing_mask.sum() == 0:\n",
        "            ax.text(0.5, 0.5, f'{feature_names[feature_idx][:15]}\\nNo missing values', \n",
        "                    ha='center', va='center', transform=ax.transAxes, fontsize=8)\n",
        "            ax.set_title(f'{feature_names[feature_idx][:15]} - No Missing', fontsize=8)\n",
        "            continue\n",
        "        print(f'Plotting feature: {feature_names[feature_idx]} with {feature_missing_mask.sum()} missing values')\n",
        "        # Get predicted and ground truth values for missing positions only\n",
        "        predicted_values = test_imputations[feature_missing_mask, feature_idx]\n",
        "        true_values = test_originals[feature_missing_mask, feature_idx]\n",
        "        \n",
        "        # Create scatter plot with smaller points for 5x5 grid\n",
        "        ax.scatter(true_values, predicted_values, alpha=0.6, s=10, color='steelblue', edgecolors='navy', linewidth=0.3)\n",
        "        \n",
        "        # Add perfect prediction line (y=x)\n",
        "        min_val = min(true_values.min(), predicted_values.min())\n",
        "        max_val = max(true_values.max(), predicted_values.max())\n",
        "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=1.5, label='Perfect' if idx == 0 else \"\")\n",
        "        \n",
        "        # Calculate and display metrics\n",
        "        mse = mean_squared_error(true_values, predicted_values)\n",
        "        mae = mean_absolute_error(true_values, predicted_values)\n",
        "        try:\n",
        "            correlation = np.corrcoef(true_values, predicted_values)[0, 1]\n",
        "        except:\n",
        "            correlation = np.nan\n",
        "        \n",
        "        # Set labels and title (smaller fonts for 5x5 grid)\n",
        "        ax.set_xlabel('Dataset', fontsize=8)\n",
        "        ax.set_ylabel('Predicted', fontsize=8)\n",
        "        ax.set_title(f'{feature_names[feature_idx][:15]}\\nR²={correlation:.3f}, MSE={mse:.4f}', \n",
        "                    fontsize=8, fontweight='bold')\n",
        "        \n",
        "        # Adjust tick labels\n",
        "        ax.tick_params(labelsize=7)\n",
        "        \n",
        "        # Add grid and styling\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_aspect('equal', adjustable='box')\n",
        "        \n",
        "        # Add text box with number of missing values (smaller for 5x5)\n",
        "        n_missing = feature_missing_mask.sum()\n",
        "        ax.text(0.05, 0.95, f'n={n_missing}', transform=ax.transAxes, \n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
        "                verticalalignment='top', fontsize=7)\n",
        "    \n",
        "    # Add legend only to the first subplot to avoid clutter\n",
        "    if len(features_with_missing) > 0:\n",
        "        axes[0].legend(loc='lower right', fontsize=7)\n",
        "    \n",
        "    # Hide any unused subplots\n",
        "    for idx in range(len(features_with_missing), 40):\n",
        "        axes[idx].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a047bb73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a047bb73",
        "outputId": "5cc6a8a5-9ade-4bb8-f7a1-d44721e9b688"
      },
      "outputs": [],
      "source": [
        "# Create the visualization\n",
        "plot_prediction_scatter(test_imputations_denorm, test_original_denorm, test_masks, feature_names, n_features=37)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73815ab1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_distribution_comparison(test_imputations_denorm, test_original_denorm, test_masks, feature_names, n_features=25):\n",
        "    \"\"\"\n",
        "    Create distribution comparison plots for random features in a 5x5 grid.\n",
        "    \n",
        "    Args:\n",
        "        test_imputations_denorm: Denormalized imputed values\n",
        "        test_original_denorm: Denormalized ground truth values  \n",
        "        test_masks: Binary masks (1=observed, 0=missing)\n",
        "        feature_names: List of feature names\n",
        "        n_features: Number of features to plot (default 25 for 5x5 grid)\n",
        "    \"\"\"\n",
        "    # Find features that have missing values\n",
        "    features_with_missing = []\n",
        "    for i, feature_name in enumerate(feature_names):\n",
        "        missing_positions = (test_masks[:, i] == 0)  # 0 = missing in model tensors\n",
        "        if missing_positions.sum() > 0:\n",
        "            features_with_missing.append((i, feature_name))\n",
        "    \n",
        "    if len(features_with_missing) < n_features:\n",
        "        n_features = len(features_with_missing)\n",
        "        print(f\"Only {n_features} features have missing values, showing all of them.\")\n",
        "\n",
        "    # Create 5x8 grid\n",
        "    fig, axes = plt.subplots(5, 8, figsize=(20, 16))\n",
        "    fig.suptitle('Distribution Comparison: Dataset vs Imputed Values', fontsize=16, fontweight='bold')\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, (feature_idx, feature_name) in enumerate(features_with_missing):\n",
        "        if idx >= 40:  # Safety check for 5x8 grid\n",
        "            break\n",
        "            \n",
        "        # Get imputed and ground truth values for missing positions only\n",
        "        missing_positions = (test_masks[:, feature_idx] == 0)  # 0 = missing in model tensors\n",
        "        \n",
        "        if missing_positions.sum() > 0:\n",
        "            imputed_values = test_imputations_denorm[missing_positions, feature_idx]\n",
        "            ground_truth_values = test_original_denorm[missing_positions, feature_idx]\n",
        "            \n",
        "            # Remove any NaN or infinite values\n",
        "            valid_mask = np.isfinite(imputed_values) & np.isfinite(ground_truth_values)\n",
        "            imputed_clean = imputed_values[valid_mask]\n",
        "            gt_clean = ground_truth_values[valid_mask]\n",
        "            \n",
        "            if len(imputed_clean) > 0 and len(gt_clean) > 0:\n",
        "                # Create histograms\n",
        "                ax = axes[idx]\n",
        "                \n",
        "                # Calculate bins for both distributions\n",
        "                all_values = np.concatenate([imputed_clean, gt_clean])\n",
        "                bins = np.linspace(all_values.min(), all_values.max(), 20)  # Fewer bins for smaller plots\n",
        "                \n",
        "                # Plot histograms\n",
        "                ax.hist(gt_clean, bins=bins, alpha=0.7, label='Dataset', \n",
        "                    color='skyblue', density=True, edgecolor='black', linewidth=0.3)\n",
        "                ax.hist(imputed_clean, bins=bins, alpha=0.7, label='Imputed', \n",
        "                    color='lightcoral', density=True, edgecolor='black', linewidth=0.3)\n",
        "                \n",
        "                # Add statistical information\n",
        "                gt_mean, gt_std = gt_clean.mean(), gt_clean.std()\n",
        "                imp_mean, imp_std = imputed_clean.mean(), imputed_clean.std()\n",
        "                correlation = np.corrcoef(gt_clean, imputed_clean)[0, 1] if len(gt_clean) > 1 else 0\n",
        "\n",
        "                # Calculate MMD (Maximum Mean Discrepancy)\n",
        "                def rbf_kernel(X, Y, gamma=1.0):\n",
        "                    \"\"\"RBF kernel for MMD calculation\"\"\"\n",
        "                    XX = np.sum(X**2, axis=1, keepdims=True)\n",
        "                    YY = np.sum(Y**2, axis=1, keepdims=True)\n",
        "                    XY = np.dot(X, Y.T)\n",
        "                    distances = XX + YY.T - 2*XY\n",
        "                    return np.exp(-gamma * distances)\n",
        "                \n",
        "                def mmd_rbf(X, Y, gamma=1.0):\n",
        "                    \"\"\"Calculate MMD with RBF kernel\"\"\"\n",
        "                    X = X.reshape(-1, 1)\n",
        "                    Y = Y.reshape(-1, 1)\n",
        "                    \n",
        "                    m, n = len(X), len(Y)\n",
        "                    \n",
        "                    K_XX = rbf_kernel(X, X, gamma)\n",
        "                    K_YY = rbf_kernel(Y, Y, gamma)\n",
        "                    K_XY = rbf_kernel(X, Y, gamma)\n",
        "                    \n",
        "                    mmd = (np.sum(K_XX) / (m * m) + \n",
        "                           np.sum(K_YY) / (n * n) - \n",
        "                           2 * np.sum(K_XY) / (m * n))\n",
        "                    return np.sqrt(max(mmd, 0))  # Ensure non-negative\n",
        "                \n",
        "                try:\n",
        "                    mmd_value = mmd_rbf(gt_clean, imputed_clean)\n",
        "                except:\n",
        "                    mmd_value = np.nan\n",
        "\n",
        "                # Calculate Jensen-Shannon Divergence\n",
        "                try:\n",
        "                    # Create histograms with same bins for JSD\n",
        "                    data_range = (min(gt_clean.min(), imputed_clean.min()), \n",
        "                                 max(gt_clean.max(), imputed_clean.max()))\n",
        "                    \n",
        "                    if data_range[1] == data_range[0]:\n",
        "                        jsd_value = 0.0  # No divergence if all values are the same\n",
        "                    else:\n",
        "                        bins = np.linspace(data_range[0], data_range[1], 30)\n",
        "                        \n",
        "                        # Get histogram probabilities\n",
        "                        hist_gt, _ = np.histogram(gt_clean, bins=bins, density=True)\n",
        "                        hist_imp, _ = np.histogram(imputed_clean, bins=bins, density=True)\n",
        "                        \n",
        "                        # Normalize to probabilities\n",
        "                        hist_gt = hist_gt + 1e-10  # Add small epsilon to avoid zeros\n",
        "                        hist_imp = hist_imp + 1e-10\n",
        "                        hist_gt = hist_gt / hist_gt.sum()\n",
        "                        hist_imp = hist_imp / hist_imp.sum()\n",
        "                        \n",
        "                        # Calculate Jensen-Shannon divergence\n",
        "                        jsd_value = jensenshannon(hist_gt, hist_imp)\n",
        "                except:\n",
        "                    jsd_value = np.nan\n",
        "\n",
        "                # Add vertical lines for means\n",
        "                ax.axvline(gt_mean, color='blue', linestyle='--', alpha=0.8, linewidth=1, label='Dataset Mean' if idx == 0 else \"\")\n",
        "                ax.axvline(imp_mean, color='red', linestyle='--', alpha=0.8, linewidth=1, label='Imputed Mean' if idx == 0 else \"\")\n",
        "                \n",
        "                # Set labels and title (smaller font for 5x5 grid)\n",
        "                ax.set_xlabel(f'{feature_name[:15]}', fontsize=8)  # Truncate long names\n",
        "                ax.set_ylabel('Density', fontsize=8)\n",
        "                ax.tick_params(labelsize=7)\n",
        "                \n",
        "                # Add correlation, MMD, and JSD as title\n",
        "                ax.set_title(f'R²={correlation:.3f}, MMD={mmd_value:.3f}, JSD={jsd_value:.3f}', fontsize=7, fontweight='bold')\n",
        "                \n",
        "                # Add legend only to first plot\n",
        "                if idx == 0:\n",
        "                    ax.legend(fontsize=7, loc='upper right')\n",
        "                \n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            else:\n",
        "                axes[idx].text(0.5, 0.5, f'{feature_name[:15]}\\nNo valid data', \n",
        "                            ha='center', va='center', transform=axes[idx].transAxes, fontsize=8)\n",
        "                axes[idx].set_title(f'{feature_name[:15]} - No Valid Data', fontsize=8)\n",
        "        else:\n",
        "            axes[idx].text(0.5, 0.5, f'{feature_name[:15]}\\nNo missing values', \n",
        "                        ha='center', va='center', transform=axes[idx].transAxes, fontsize=8)\n",
        "            axes[idx].set_title(f'{feature_name[:15]} - No Missing Values', fontsize=8)\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for idx in range(len(features_with_missing), 40):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IDgv-wkViD-u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "IDgv-wkViD-u",
        "outputId": "4eaea130-04a8-4ca0-b95a-0722caa11e89"
      },
      "outputs": [],
      "source": [
        "# Distribution comparison plots\n",
        "plot_distribution_comparison(test_imputations_denorm, test_original_denorm,\n",
        "                             test_masks, feature_names, n_features=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00addce0",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4be0b2e3",
      "metadata": {
        "id": "4be0b2e3"
      },
      "source": [
        "### Distribution Comparison Visualizations\n",
        "\n",
        "Visual comparison of predicted vs. dataset distributions is crucial for understanding model performance beyond simple error metrics. These plots help us assess:\n",
        "\n",
        "**What the Plots Show:**\n",
        "- **Red (Imputed)**: Distribution of model's predicted values for missing positions\n",
        "- **Blue (Dataset)**: Distribution of actual values at those same positions\n",
        "- **Overlap**: How well the model captures the true data distribution\n",
        "\n",
        "**Why This Matters:**\n",
        "- A good generative model should not just minimize error, but also preserve the statistical properties of the data\n",
        "- If distributions match well, the model is generating realistic values\n",
        "- Large differences indicate the model may be systematically biased or missing important patterns\n",
        "\n",
        "**Interpretation:**\n",
        "- **Good**: Overlapping distributions with similar shapes and centers\n",
        "- **Concerning**: Shifted means, different variances, or completely different shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lI7StssOGmfp",
      "metadata": {
        "id": "lI7StssOGmfp"
      },
      "outputs": [],
      "source": [
        "def generate_samples(model, X_test, test_loader, device, n_samples_per_test=100):\n",
        "    \"\"\"Generate multiple samples for a dataset using the trained model.\n",
        "    \"\"\"\n",
        "    # We'll generate multiple samples\n",
        "    test_samples = np.zeros((X_test.shape[0], n_samples_per_test, X_test.shape[1]))\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Create a progress bar for all samples\n",
        "        from tqdm import tqdm\n",
        "\n",
        "        for batch_idx, (batch_data, batch_mask) in enumerate(tqdm(test_loader, desc=\"Generating Samples\")):\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Calculate the indices for this batch\n",
        "            start_idx = batch_idx * test_loader.batch_size\n",
        "            end_idx = min(start_idx + test_loader.batch_size, X_test.shape[0])\n",
        "            actual_batch_size = end_idx - start_idx\n",
        "\n",
        "            # Generate multiple samples for each item in the batch\n",
        "            for j in range(n_samples_per_test):\n",
        "                # Get reconstruction\n",
        "                reconstruction, mu, logvar = model(batch_data, batch_mask) # TODO: Change this line based on the model you use\n",
        "\n",
        "                # Apply mask: keep original values where available, use reconstructed values where missing\n",
        "                mask_float = batch_mask.float()\n",
        "                imputed = batch_data * mask_float + reconstruction * (1 - mask_float)\n",
        "\n",
        "                # Store the samples (already in original scale since we didn't normalize)\n",
        "                test_samples[start_idx:end_idx, j, :] = imputed.cpu().numpy()\n",
        "    print(f\"✓ Generated samples shape: {test_samples.shape}\")\n",
        "    print(f\"  - {test_samples.shape[0]} samples\")\n",
        "    print(f\"  - {test_samples.shape[1]} generated variations per sample\")\n",
        "    print(f\"  - {test_samples.shape[2]} features per sample\")\n",
        "\n",
        "    # Data is already in original scale (no denormalization needed)\n",
        "    test_samples_final = test_samples.copy()\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    mean_across_samples = test_samples_final.mean(axis=1)  # Mean across the 100 samples\n",
        "\n",
        "    print(f\"  - Range of means: [{mean_across_samples.min():.4f}, {mean_across_samples.max():.4f}]\")\n",
        "\n",
        "    return test_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sjkbko3oFJQV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjkbko3oFJQV",
        "outputId": "f5224d48-f821-428d-f99a-d059b8150dbc"
      },
      "outputs": [],
      "source": [
        "# Test Evaluation\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TEST EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate multiple samples for test using the trained model\n",
        "print(f\"Generating 100 samples for each of {X_test.shape[0]} test samples...\")\n",
        "\n",
        "# test_samples = generate_samples(\n",
        "#     model, X_test, test_loader, device, n_samples_per_test=100\n",
        "# )\n",
        "test_samples = generate_samples(\n",
        "    model, X_test, test_loader, device, n_samples_per_test=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JFWebQ2AzSsJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFWebQ2AzSsJ",
        "outputId": "4c12f4f3-dadf-4384-f101-32fb536fb139"
      },
      "outputs": [],
      "source": [
        "test_score = compute_score(generated_samples=test_samples, set_name='test')\n",
        "print(\"Test score:\", test_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zlA9iSSZcYlh",
      "metadata": {
        "id": "zlA9iSSZcYlh"
      },
      "source": [
        "The final score is computed as: Mean Correlation − Mean JS Divergence − Mean MSE\n",
        "\n",
        "Just as we compare generated samples for the test set against the original unimputed values, we will apply the same metric to the samples you generate for test2, using the hidden test2 set. This will determine your final submission score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7de3489",
      "metadata": {
        "id": "b7de3489"
      },
      "source": [
        "## Preparing a submission:\n",
        "Let's prepare a submission. We expect the final submission to be a 417x100x37 numpy array. These correspond to the 100 diverse samples you generated based on the constrained parameters we provided in the test2 set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6a8623",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d6a8623",
        "outputId": "fa7b08fc-4fee-472f-ea8c-f6ab635e3bc2"
      },
      "outputs": [],
      "source": [
        "# Test2 Evaluation\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TEST2 EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate multiple samples for test2 using the trained model\n",
        "print(f\"Generating 100 samples for each of {X_test2.shape[0]} test2 samples...\")\n",
        "\n",
        "test2_samples = generate_samples(\n",
        "    model, X_test2, test2_loader, device, n_samples_per_test=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bacf0675",
      "metadata": {
        "id": "bacf0675"
      },
      "source": [
        "### Test2 Evaluation: Generating Diverse Design Completions\n",
        "\n",
        "This is the core evaluation for your AI Copilot assignment. Here we:\n",
        "\n",
        "**Input**: Test2 samples with some known features (constraints) and some missing features (free parameters)\n",
        "\n",
        "**Output**: 100 diverse, plausible completions for each test sample\n",
        "\n",
        "**Why 100 Samples?**\n",
        "- Engineers want to explore multiple design options, not just one \"best\" solution\n",
        "- Diversity helps discover unexpected but valid design combinations  \n",
        "\n",
        "**Technical Process:**\n",
        "1. For each test2 sample, use the trained model to generate 100 different completions\n",
        "2. Each completion respects the known constraints (observed values)\n",
        "3. Missing values are filled with diverse, model-generated predictions\n",
        "4. Final output: 417 × 100 × 37 array (417 test samples, 100 variants each, 37 features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc755a46",
      "metadata": {
        "id": "fc755a46"
      },
      "outputs": [],
      "source": [
        "id = np.random.randint(1e8, 1e9-1)\n",
        "np.save(f\"{id}.npy\", test2_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2044ff7",
      "metadata": {
        "id": "d2044ff7"
      },
      "source": [
        "### Summary and Tips for CP3\n",
        "\n",
        "The VAE baseline reproduces the dataset distribution well for some features, but others still show substantial discrepancies, indicating significant room for improvement!\n",
        "\n",
        "**Key Observations:**\n",
        "- **Strengths**: The model captures general feature ranges and some distributional patterns\n",
        "- **Weaknesses**: Some features show systematic bias or poor distribution matching\n",
        "- **Opportunities**: Advanced architectures (diffusion models, transformers) or better conditioning strategies could improve performance\n",
        "\n",
        "**For Your Assignment**: Consider these results as a baseline. Think about:\n",
        "- Which features are hardest to predict and why?\n",
        "- How could you modify the architecture or training process?\n",
        "- What additional constraints or domain knowledge could help?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29cae83b",
      "metadata": {},
      "source": [
        "# Fist try with a diffusion model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e87909d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diffusion-based tabular imputer (DDPM-style)\n",
        "# This cell implements a lightweight denoising diffusion probabilistic model (DDPM)\n",
        "# tailored for tabular imputation (conditioned on observed values via the mask).\n",
        "# It's self-contained and does not require external 'diffusers' packages.\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import tqdm\n",
        "\n",
        "# -- Utilities: noise schedule and timestep embedding --\n",
        "def make_beta_schedule(T, start=1e-4, end=0.02, device='cpu'):\n",
        "    return torch.linspace(start, end, T, device=device)\n",
        "\n",
        "def sinusoidal_timestep_embedding(timesteps, dim):\n",
        "    half = dim // 2\n",
        "    emb = math.log(10000) / (half - 1)\n",
        "    emb = torch.exp(torch.arange(half, device=timesteps.device) * -emb)\n",
        "    emb = timesteps[:, None].float() * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if dim % 2 == 1:\n",
        "        emb = F.pad(emb, (0,1))\n",
        "    return emb\n",
        "\n",
        "# -- Simple MLP denoiser with timestep & mask conditioning --\n",
        "class MLPDenoiser(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=512, timesteps_embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden = hidden\n",
        "        self.timesteps_embed_dim = timesteps_embed_dim\n",
        "\n",
        "        # Time embedding MLP -> produces a vector of size `hidden`\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(timesteps_embed_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Project time embedding to match final output dimension so it can be added to network output\n",
        "        self.time_to_output = nn.Linear(hidden, input_dim)\n",
        "\n",
        "        # The network input will be: noisy_x (input_dim) + observed_values (input_dim) + mask (input_dim)\n",
        "        in_dim = input_dim * 3\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, noisy_x, cond_x, mask, t):\n",
        "        # t: tensor of shape (B,) with timestep indices\n",
        "        # Create sinusoidal embedding of timesteps with the requested embedding dim\n",
        "        t_emb = sinusoidal_timestep_embedding(t, self.timesteps_embed_dim)\n",
        "        t_emb = self.time_mlp(t_emb)  # (B, hidden)\n",
        "\n",
        "        # Project time embedding to output dimension so it can be added to net output\n",
        "        t_out = self.time_to_output(t_emb)  # (B, input_dim)\n",
        "\n",
        "        # Concatenate noisy input, conditional observed values and mask\n",
        "        inp = torch.cat([noisy_x, cond_x * mask, mask], dim=1)  # (B, in_dim)\n",
        "        net_out = self.net(inp)  # (B, input_dim)\n",
        "\n",
        "        # Add projected time features (elementwise) to network output\n",
        "        h = net_out + t_out\n",
        "        return h\n",
        "\n",
        "# -- DDPM helper functions --\n",
        "class DiffusionImputer:\n",
        "    def __init__(self, input_dim, T=1000, device='cpu'):\n",
        "        self.device = device\n",
        "        self.input_dim = input_dim\n",
        "        self.T = T\n",
        "        self.model = MLPDenoiser(input_dim).to(device)\n",
        "        self.beta = make_beta_schedule(T, device=device)\n",
        "        self.alpha = 1.0 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=2e-4)\n",
        "\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        # x_start: (B, D) clean data; t: (B,) timesteps\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "        a_bar = self.alpha_bar[t].unsqueeze(1)\n",
        "        return torch.sqrt(a_bar) * x_start + torch.sqrt(1 - a_bar) * noise, noise\n",
        "\n",
        "    def p_losses(self, x_start, cond_x, mask, t):\n",
        "        x_noisy, noise = self.q_sample(x_start, t)\n",
        "        pred_noise = self.model(x_noisy, cond_x, mask, t)\n",
        "        loss = F.mse_loss(pred_noise * (1 - mask), noise * (1 - mask))\n",
        "        # We only compute loss on missing positions (1-mask) so model learns to predict missing values' noise\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, cond_x, mask, n_steps=None):\n",
        "        # cond_x: observed values (B,D), mask: (B,D) with 1=observed, 0=missing\n",
        "        n_steps = self.T if n_steps is None else n_steps\n",
        "        bsz = cond_x.size(0)\n",
        "        x = torch.randn(bsz, self.input_dim, device=self.device)\n",
        "\n",
        "        for i in reversed(range(n_steps)):\n",
        "            t = torch.full((bsz,), i, dtype=torch.long, device=self.device)\n",
        "            pred_noise = self.model(x, cond_x, mask, t)\n",
        "            beta_t = self.beta[i]\n",
        "            alpha_t = self.alpha[i]\n",
        "            alpha_bar_t = self.alpha_bar[i]\n",
        "\n",
        "            # Predict x0 from x_t and predicted noise\n",
        "            x0_pred = (x - torch.sqrt(1 - alpha_bar_t) * pred_noise) / torch.sqrt(alpha_bar_t)\n",
        "\n",
        "            coef1 = beta_t / torch.sqrt(1 - alpha_bar_t)\n",
        "            mean = torch.sqrt(1.0 / alpha_t) * (x - coef1 * pred_noise)\n",
        "\n",
        "            if i > 0:\n",
        "                noise = torch.randn_like(x)\n",
        "                sigma = torch.sqrt(beta_t)\n",
        "                x = mean + sigma * noise\n",
        "            else:\n",
        "                x = mean\n",
        "\n",
        "            # Re-impose observed values so conditioning always holds exactly\n",
        "            x = cond_x * mask + x * (1 - mask)\n",
        "\n",
        "        return x.cpu().numpy()\n",
        "\n",
        "# -- Training helpers (simple epoch loop) --\n",
        "def train_diffusion(imputer, train_loader, val_loader=None, epochs=20, device='cpu', ckpt_path='diffusion_imputer.pth'):\n",
        "    imputer.model.train()\n",
        "    for epoch in range(epochs):\n",
        "        pbar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "        epoch_loss = 0.0\n",
        "        for batch_x, batch_mask in pbar:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Random timesteps for each sample\n",
        "            t = torch.randint(0, imputer.T, (batch_x.size(0),), device=device, dtype=torch.long)\n",
        "            loss = imputer.p_losses(batch_x, batch_x, batch_mask, t)\n",
        "\n",
        "            imputer.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            imputer.optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * batch_x.size(0)\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
        "\n",
        "        avg_loss = epoch_loss / (len(train_loader.dataset))\n",
        "        # print(f'Epoch {epoch+1}/{epochs} - Avg Loss: {avg_loss:.6f}')\n",
        "        torch.save(imputer.model.state_dict(), ckpt_path)\n",
        "\n",
        "    print('✓ Diffusion training complete')\n",
        "\n",
        "# -- Usage example (copy into a cell and run to train) --\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# imputer = DiffusionImputer(input_dim=config['input_dim'], T=200, device=device)\n",
        "# train_diffusion(imputer, train_loader, val_loader=val_loader, epochs=10, device=device)\n",
        "# # Generate imputations: returns numpy array (B, D)\n",
        "# sample = imputer.sample(cond_x=batch_data.to(device), mask=batch_mask.to(device), n_steps=200)\n",
        "\n",
        "# Notes:\n",
        "# - This implementation is a compact DDPM variant for tabular data. It conditions on observed values by concatenation\n",
        "#   and re-imposes the observed features during sampling to guarantee constraints.\n",
        "# - The model trains only on missing positions (loss masked) so it focuses on reconstructing omitted values.\n",
        "# - You can tune T (timesteps), model hidden size, and training schedule. Use smaller T (e.g., 200) for fast experiments.\n",
        "# - If you want the Hugging Face `diffusers` pipelines instead, I can add instructions to install it and adapt a tabular\n",
        "#   pipeline using their Unet/ModelMlp components, but that will add dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "721607bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train diffusion imputer (short run) and compare to the VAE baseline\n",
        "print('\\n' + '='*70)\n",
        "print('DIFFUSION TRAIN & COMPARE')\n",
        "print('='*70)\n",
        "\n",
        "# Device (should already be defined earlier, but keep fallback)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate the imputer (use smaller T for quicker experiments)\n",
        "imputer = DiffusionImputer(input_dim=config['input_dim'], T=1000, device=device)\n",
        "\n",
        "# Quick training run - adjust epochs as needed (start small for a smoke test)\n",
        "quick_epochs = 100\n",
        "train_diffusion(imputer, train_loader, val_loader=val_loader, epochs=quick_epochs, device=device, ckpt_path='diffusion_imputer.pth')\n",
        "\n",
        "# Evaluation helper for diffusion imputer (generates multiple samples per input)\n",
        "def evaluate_diffusion_imputer(imputer, data_loader, device, n_samples=20, n_steps=200):\n",
        "    imputer.model.eval()\n",
        "    all_imputations = []  # will hold (B, n_samples, D) blocks\n",
        "    all_originals = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in data_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            samples = []\n",
        "            for _ in range(n_samples):\n",
        "                out = imputer.sample(batch_data, batch_mask, n_steps=n_steps)  # numpy (B, D)\n",
        "                samples.append(out)\n",
        "\n",
        "            samples = np.stack(samples, axis=1)  # (B, n_samples, D)\n",
        "            all_imputations.append(samples)\n",
        "            all_originals.append(batch_data.cpu().numpy())\n",
        "            # keep numeric mask (1=observed,0=missing) so callers can compute missing positions via (mask==0)\n",
        "            all_masks.append(batch_mask.cpu().numpy())\n",
        "\n",
        "    imputations = np.concatenate(all_imputations, axis=0)\n",
        "    originals = np.vstack(all_originals)\n",
        "    masks = np.vstack(all_masks)\n",
        "    return imputations, originals, masks\n",
        "\n",
        "# Run evaluation on the test set (this may take time depending on n_samples)\n",
        "print('Evaluating diffusion imputer on test set...')\n",
        "diff_imputations, diff_originals, diff_masks = evaluate_diffusion_imputer(imputer, test_loader, device, n_samples=20, n_steps=200)\n",
        "print('✓ Diffusion imputations generated')\n",
        "\n",
        "# Summarize metrics similar to VAE evaluation (use mean across samples)\n",
        "diff_mean_imputed = diff_imputations.mean(axis=1)  # (N, D)\n",
        "# missing_mask: True where values were missing (mask==0 in model tensors)\n",
        "missing_mask = (diff_masks == 0)\n",
        "feature_metrics_diff = {}\n",
        "for i, fname in enumerate(feature_names):\n",
        "    if missing_mask[:, i].sum() > 0:\n",
        "        imputed_missing = diff_mean_imputed[missing_mask[:, i], i]\n",
        "        ground_truth_missing = diff_originals[missing_mask[:, i], i]\n",
        "\n",
        "        mse = mean_squared_error(ground_truth_missing, imputed_missing)\n",
        "        mae = mean_absolute_error(ground_truth_missing, imputed_missing)\n",
        "        try:\n",
        "            corr = np.corrcoef(ground_truth_missing, imputed_missing)[0,1]\n",
        "        except:\n",
        "            corr = np.nan\n",
        "        mean_diff, js_div = calculate_jsd_and_mean_diff(imputed_missing, ground_truth_missing, fname)\n",
        "\n",
        "        feature_metrics_diff[fname] = {'n_missing': missing_mask[:, i].sum(), 'mse': mse, 'mae': mae, 'correlation': corr, 'mean_difference': mean_diff, 'js_divergence': js_div}\n",
        "\n",
        "print(f'\\n✓ Diffusion metrics calculated for {len(feature_metrics_diff)} features with missing values')\n",
        "\n",
        "# Print summary statistics (same as earlier VAE cell)\n",
        "all_mse = [m['mse'] for m in feature_metrics_diff.values() if not np.isnan(m['mse'])]\n",
        "all_mae = [m['mae'] for m in feature_metrics_diff.values() if not np.isnan(m['mae'])]\n",
        "all_corr = [m['correlation'] for m in feature_metrics_diff.values() if not np.isnan(m['correlation'])]\n",
        "all_mean_diff = [m['mean_difference'] for m in feature_metrics_diff.values() if not np.isnan(m['mean_difference'])]\n",
        "all_js_div = [m['js_divergence'] for m in feature_metrics_diff.values() if not np.isnan(m['js_divergence'])]\n",
        "\n",
        "print('\\nDiffusion Summary Statistics Across All Features:')\n",
        "print(f'  - Average MSE: {np.mean(all_mse):.4f} ± {np.std(all_mse):.4f}')\n",
        "print(f'  - Average MAE: {np.mean(all_mae):.4f} ± {np.std(all_mae):.4f}')\n",
        "print(f'  - Average Correlation: {np.mean(all_corr):.3f} ± {np.std(all_corr):.3f}')\n",
        "print(f'  - Average Mean Difference: {np.mean(all_mean_diff):.4f} ± {np.std(all_mean_diff):.4f}')\n",
        "print(f'  - Average JS Divergence: {np.mean(all_js_div):.4f} ± {np.std(all_js_div):.4f}')\n",
        "\n",
        "# If VAE test imputations exist in the notebook namespace, compute the same score and compare\n",
        "if 'test_imputations' in globals():\n",
        "    print('\\nComparing to VAE results (if available)')\n",
        "    # VAE's test_imputations may have shape (N, D) or (N, n_samples, D) depending on how generated; handle both\n",
        "    vae_samples = test_imputations\n",
        "    if vae_samples.ndim == 3:\n",
        "        vae_mean = vae_samples.mean(axis=1)\n",
        "    else:\n",
        "        vae_mean = vae_samples\n",
        "\n",
        "    # Compute a simple average MSE over missing positions for VAE mean vs diffusion mean\n",
        "    # Use the same missing positions mask computed above\n",
        "    vae_errors = ((vae_mean - X_test_original) ** 2)[missing_mask]\n",
        "    diff_errors = ((diff_mean_imputed - diff_originals) ** 2)[missing_mask]\n",
        "    print(f'  - VAE mean MSE on missing positions: {vae_errors.mean():.6f}')\n",
        "    print(f'  - Diffusion mean MSE on missing positions: {diff_errors.mean():.6f}')\n",
        "else:\n",
        "    print('VAE test imputations variable `test_imputations` not found in the notebook namespace. Run the VAE evaluation cell first to compare.')\n",
        "\n",
        "# Optionally compute the same final metric used for the challenge if compute_score is available\n",
        "if 'compute_score' in globals():\n",
        "    try:\n",
        "        print('\\nComputing challenge-style score for diffusion samples (may take time)')\n",
        "        diff_score = compute_score(generated_samples=diff_imputations, set_name='test')\n",
        "        print('Diffusion score:', diff_score)\n",
        "    except Exception as e:\n",
        "        print('compute_score failed:', e)\n",
        "\n",
        "print('\\n✓ Diffusion training + comparison cell complete. Review outputs above.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce07fe4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate multiple samples from the trained diffusion imputer (N x S x D)\n",
        "def generate_samples_diffusion(imputer, X_test, test_loader, device, n_samples_per_test=100, n_steps=200):\n",
        "    \"\"\"Generate multiple imputations per test sample using the diffusion imputer.\n",
        "\n",
        "    Returns:\n",
        "        test_samples: numpy array shape (N, n_samples_per_test, D)\n",
        "    \"\"\"\n",
        "    imputer.model.eval()\n",
        "\n",
        "    N = X_test.shape[0]\n",
        "    D = X_test.shape[1]\n",
        "    test_samples = np.zeros((N, n_samples_per_test, D))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (batch_data, batch_mask) in enumerate(tqdm.tqdm(test_loader, desc='Generating Diffusion Samples')):\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            start_idx = batch_idx * test_loader.batch_size\n",
        "            end_idx = min(start_idx + test_loader.batch_size, N)\n",
        "            actual_batch_size = end_idx - start_idx\n",
        "\n",
        "            # Generate n_samples_per_test samples for this batch\n",
        "            for j in range(n_samples_per_test):\n",
        "                out = imputer.sample(batch_data, batch_mask, n_steps=n_steps)  # numpy (B, D)\n",
        "                # If imputer.sample returns a larger batch (shouldn't), trim to actual_batch_size\n",
        "                out = np.asarray(out)\n",
        "                if out.shape[0] != actual_batch_size:\n",
        "                    out = out[:actual_batch_size]\n",
        "                test_samples[start_idx:end_idx, j, :] = out\n",
        "\n",
        "    print(f\"✓ Generated samples shape: {test_samples.shape}\")\n",
        "    return test_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de341ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "test2_samples = generate_samples_diffusion(imputer, X_test2, test2_loader, device, n_samples_per_test=100)\n",
        "id = np.random.randint(1e8, 1e9-1)\n",
        "np.save(f\"{id}.npy\", test2_samples)\n",
        "print(f\"Saved diffusion test2 samples to {id}.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c177a76d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#dzyfusion model  result\n",
        "# n Successful!\n",
        "# Name: patryk\n",
        "# Nickname: patryk\n",
        "# Score: -0.16938360168742916\n",
        "# Submission ID: 11f7cdbd-2ede-4450-b86c-02316439e1da"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f595735",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute covariance matrix for `train_original.csv` and show top covarying feature pairs\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "path = r\"c:\\sources\\2155-CP3\\dataset\\train_original.csv\"\n",
        "print('Loading:', path)\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Select feature columns (expect names like 'Feature 1'..'Feature 37')\n",
        "feature_cols = [c for c in df.columns if 'Feature' in c]\n",
        "if len(feature_cols) < 37:\n",
        "    # fallback: use numeric columns\n",
        "    feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "df_feat = df[feature_cols].apply(pd.to_numeric, errors='coerce')\n",
        "print('Raw data shape:', df.shape, '-> feature matrix shape:', df_feat.shape)\n",
        "\n",
        "# Drop rows with all-NaN in features (if any)\n",
        "df_feat = df_feat.dropna(how='all')\n",
        "\n",
        "# Compute covariance and correlation\n",
        "cov = df_feat.cov()\n",
        "corr = df_feat.corr()\n",
        "\n",
        "print('Covariance matrix shape:', cov.shape)\n",
        "\n",
        "# Top absolute corr pairs (upper triangle, excluding diagonal)\n",
        "n = corr.shape[0]\n",
        "pairs = []\n",
        "for i in range(n):\n",
        "    for j in range(i+1, n):\n",
        "        pairs.append((feature_cols[i], feature_cols[j], corr.iat[i, j]))\n",
        "pairs_sorted = sorted(pairs, key=lambda x: abs(x[2]), reverse=True)\n",
        "print('\\nTop 15 absolute correlation pairs:')\n",
        "for a, b, v in pairs_sorted[:15]:\n",
        "    print(f'{a} <-> {b}: {v:.6f}')\n",
        "\n",
        "# Ensure outputs dir exists\n",
        "out_dir = r\"c:\\sources\\2155-CP3\\outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "cov_path = os.path.join(out_dir, 'train_covariance.csv')\n",
        "cov.to_csv(cov_path)\n",
        "print('\\nSaved covariance matrix to', cov_path)\n",
        "\n",
        "# Plot heatmap (show correlation and covariance side-by-side)\n",
        "# Use numeric tick labels (1..N) and smaller font size for clarity\n",
        "labels = [str(i+1) for i in range(len(feature_cols))]\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(cov, xticklabels=labels, yticklabels=labels, cmap='vlag', center=0)\n",
        "plt.title('Covariance (train_original)')\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(corr, xticklabels=labels, yticklabels=labels, cmap='vlag', center=0)\n",
        "plt.title('Correlation (train_original)')\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.13.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
