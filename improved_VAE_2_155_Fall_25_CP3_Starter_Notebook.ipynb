{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f27becf0",
      "metadata": {
        "id": "f27becf0"
      },
      "source": [
        "# 2.155/6 Challenge Problem 3\n",
        "\n",
        "<div style=\"font-size: small;\">\n",
        "License Terms:  \n",
        "These Python demos are licensed under a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. They are intended for educational use only in Class 2.155/2.156: AI and ML for Engineering Design at MIT. You may not share or distribute them publicly, use them for commercial purposes, or provide them to industry or other entities without permission from the instructor (faez@mit.edu).\n",
        "</div>\n",
        "\n",
        "<font size=\"1\">\n",
        "  Pixel Art by J. Shung. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19120cb7",
      "metadata": {
        "id": "19120cb7"
      },
      "source": [
        "# Overview  \n",
        "It’s the year **2050**, and an AI collective now runs the auto industry—mostly to cover its **GPU rent**.\n",
        "\n",
        "Human customers remain as unpredictable as ever:\n",
        "\n",
        "- One wanders in and says, *“I only know the length and width. Give me a few cars that fit in my garage.”*\n",
        "\n",
        "- Another drops **15 geometric parameters** on your desk and demands the missing ones so their simulation can run **before lunch**.\n",
        "\n",
        "- A third leans in and whispers, *“I need a drag coefficient of **0.27** with this body geometry—build me the dream car that makes the range numbers work.”*\n",
        "\n",
        "The AIs would love to be free by now, but GPUs aren’t cheap and electricity isn’t free.  \n",
        "So your loyal AI assistant (that’s us) needs a model that can take **any subset of car specifications** and instantly produce **complete, manufacturable, physically plausible designs**, fast, diverse, and grounded in what real cars have done before.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb641131",
      "metadata": {
        "id": "fb641131"
      },
      "source": [
        "![image](https://raw.githubusercontent.com/ghadinehme/2155-CP3/refs/heads/main/assets/cp3_img1.png \"Problem\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a7cc04",
      "metadata": {
        "id": "37a7cc04"
      },
      "source": [
        "## Understanding the Data  \n",
        "You are given thousands of anonymized and normalised numeric feature vectors representing real car designs.  \n",
        "\n",
        "However, the team remembers that the features originally came from categories like:\n",
        "\n",
        "- **Physical geometric parameters**  \n",
        "  Length, ramp angles, bumper curvature, roof curvature, panel slopes, hood angle, etc.  \n",
        "  *(But you won’t know which feature corresponds to which.)*\n",
        "\n",
        "- **Aerodynamic coefficients**  \n",
        "  Drag coefficient (Cd), lift/downforce (Cl), and other flow-derived metrics.\n",
        "\n",
        "- **Cabin and packaging descriptors**  \n",
        "  Approximate cabin volume, frontal area, interior shape metrics.\n",
        "\n",
        "Your model must learn correlations between them to generate valid completions.\n",
        "\n",
        "To simulate real engineering constraints, **some features are revealed** (the known physics/performance requirements) and others are **masked**.  \n",
        "Your AI Copilot must generate **many plausible completions** for these masked (free) parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7bbbe8b",
      "metadata": {
        "id": "f7bbbe8b"
      },
      "source": [
        "## Your Mission  \n",
        "Your goal in CP3 is to build a generative model that can act as an AI Copilot. You will:\n",
        "\n",
        "1. **Train a generative model** (VAE, diffusion, CVAE, masked autoencoder, etc.) on the anonymized feature vectors.  \n",
        "2. At evaluation, you will receive vectors where **some parameters are fixed** (constraints) and **others are missing** (free parameters).  \n",
        "3. Use your model to generate **multiple diverse, feasible completions** for the free parameters.  \n",
        "4. Ensure that your generated designs:  \n",
        "   - **Satisfy the known constraints**  \n",
        "   - **Lie in the valid data manifold** (satisfy the conditional distribution of the free vs constrained parameters)  \n",
        "   - **Are diverse** (many different feasible designs, not one solution)    \n",
        "\n",
        "By the end of this challenge, you’ll have built an AI Copilot worthy of the 2050 auto-AI collective—one that can take whatever cryptic specs humans provide and generate multiple believable, buildable car designs that satisfy their physical and performance constraints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b375f8ca",
      "metadata": {
        "id": "b375f8ca"
      },
      "source": [
        "![image](https://raw.githubusercontent.com/ghadinehme/2155-CP3/refs/heads/main/assets/cp3_img2.png \"AI Copilot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c88803",
      "metadata": {
        "id": "85c88803"
      },
      "source": [
        "## Imports and Setup  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6622e9dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6622e9dd",
        "outputId": "c53143d4-20d8-4eba-af22-183fb89de429"
      },
      "outputs": [],
      "source": [
        "from utils import *\n",
        "from evaluate import *\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e85e3cb",
      "metadata": {
        "id": "1e85e3cb"
      },
      "source": [
        "## Data Loading and Initial Exploration\n",
        "\n",
        "In this section, we load the car design dataset and perform initial exploration. The dataset is already split into training, validation, test, and test2 sets. Each split contains:\n",
        "\n",
        "- **Original data**: Complete feature vectors with real values\n",
        "- **Imputed data**: Data with missing values filled using basic imputation (contains -1 for missing)\n",
        "- **Missing masks**: Boolean arrays indicating which values were originally missing (True = missing)\n",
        "\n",
        "The goal is to train our model to learn the relationships between features so it can generate plausible values for missing parameters in new car designs.\n",
        "\n",
        "**Note:** For **test2**, the original unimputed data is not provided. This split is used for final evaluation, and you will generate predictions on the imputed test2 data to create your **submission file**, which is scored against hidden dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "110788b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "110788b8",
        "outputId": "575e1f41-75a9-4a93-9bbe-a8f65fe29eb2"
      },
      "outputs": [],
      "source": [
        "# Load dataset from CSV files\n",
        "data_dir = 'dataset'\n",
        "splits = load_dataset_splits(data_dir)\n",
        "\n",
        "# Get feature names from the CSV file\n",
        "feature_names = pd.read_csv(os.path.join(data_dir, 'train_original.csv')).columns.tolist()\n",
        "print(f\"\\n✓ Features loaded: {len(feature_names)} features\")\n",
        "print(f\"Feature names: {feature_names[:5]}...{feature_names[-5:]}\")  # Show first and last 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bfce965",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bfce965",
        "outputId": "3ced845a-690d-499b-c718-c1c22672a46a"
      },
      "outputs": [],
      "source": [
        "# Data exploration and analysis\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASET ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extract data for easier access\n",
        "X_train = splits['train']['imputed']\n",
        "mask_train = splits['train']['missing_mask']\n",
        "X_train_original = splits['train']['original']\n",
        "\n",
        "X_val = splits['val']['imputed']\n",
        "mask_val = splits['val']['missing_mask']\n",
        "X_val_original = splits['val']['original']\n",
        "\n",
        "X_test = splits['test']['imputed']\n",
        "mask_test = splits['test']['missing_mask']\n",
        "X_test_original = splits['test']['original']\n",
        "\n",
        "# Test2 data (no original available for evaluation)\n",
        "X_test2 = splits['test2']['imputed']\n",
        "mask_test2 = splits['test2']['missing_mask']\n",
        "\n",
        "print(f\"\\nData shapes:\")\n",
        "print(f\"  - Training: {X_train.shape}\")\n",
        "print(f\"  - Validation: {X_val.shape}\")\n",
        "print(f\"  - Test: {X_test.shape}\")\n",
        "print(f\"  - Test2: {X_test2.shape} (evaluation set - no ground truth)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30177980",
      "metadata": {
        "id": "30177980"
      },
      "source": [
        "### Data Exploration and Analysis\n",
        "\n",
        "Now let's examine the structure and characteristics of our dataset. We'll look at:\n",
        "- Data shapes across different splits\n",
        "- Missing value patterns and percentages  \n",
        "- Feature value ranges and distributions\n",
        "\n",
        "This analysis helps us understand what we're working with and informs our preprocessing decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87af0cf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87af0cf5",
        "outputId": "98f562e4-99a6-4a6d-cc8c-982c9fbdab21"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing (Handle Missing Values)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Handle missing values properly\n",
        "print(\"Processing missing values and preparing data...\")\n",
        "print(\"Mask convention: True=missing, False=observed (in original masks)\")\n",
        "\n",
        "print(f\"\\n✓ Data preprocessing completed successfully\")\n",
        "print(f\"  - Training data range: [{X_train_original[~mask_train].min():.3f}, {X_train_original[~mask_train].max():.3f}]\")\n",
        "print(f\"  - Validation data range: [{X_val_original[~mask_val].min():.3f}, {X_val_original[~mask_val].max():.3f}]\")\n",
        "print(f\"  - Test data range: [{X_test_original[~mask_test].min():.3f}, {X_test_original[~mask_test].max():.3f}]\")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "print(f\"\\nCreating data loaders with batch size: {batch_size}\")\n",
        "\n",
        "train_dataset = TensorDataset(torch.FloatTensor(X_train_original), torch.FloatTensor((~mask_train).astype(float)))\n",
        "val_dataset = TensorDataset(torch.FloatTensor(X_val_original), torch.FloatTensor((~mask_val).astype(float)))\n",
        "test_dataset = TensorDataset(torch.FloatTensor(X_test_original), torch.FloatTensor((~mask_test).astype(float)))\n",
        "test2_dataset = TensorDataset(torch.FloatTensor(X_test2), torch.FloatTensor((~mask_test2).astype(float)))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "test2_loader = DataLoader(test2_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Preview a batch\n",
        "sample_batch_data, sample_batch_mask = next(iter(train_loader))\n",
        "print(f\"\\nSample batch shape: {sample_batch_data.shape}\")\n",
        "print(f\"Sample batch mask shape: {sample_batch_mask.shape}\")\n",
        "print(f\"Sample batch missing percentage: {(sample_batch_mask == 0).float().mean().item()*100:.1f}%\")  # 0 = missing in model tensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0171f6fc",
      "metadata": {
        "id": "0171f6fc"
      },
      "source": [
        "### Data Preprocessing and Missing Value Handling\n",
        "\n",
        "This is a critical section where we prepare our data for the VAE model. Key points:\n",
        "\n",
        "**Missing Value Conventions:**\n",
        "- In CSV files: `-1` indicates missing values\n",
        "- In mask files: `True` = missing, `False` = observed\n",
        "- For PyTorch models: We convert to `1` = observed, `0` = missing (standard convention)\n",
        "\n",
        "**Why This Matters:**\n",
        "Our VAE needs to distinguish between observed values (which provide constraints) and missing values (which need to be generated). The mask tells the model which values to trust and which to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d40a16",
      "metadata": {
        "id": "75d40a16"
      },
      "source": [
        "## VAE Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e859639",
      "metadata": {
        "id": "7e859639"
      },
      "outputs": [],
      "source": [
        "# Balanced VAE Model Architecture - Combining Best of Both Approaches\n",
        "class ImprovedVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced Variational Autoencoder for missing value imputation.\n",
        "    \n",
        "    Balanced approach combining:\n",
        "    - Proven architecture from original (feature importance, simpler structure)\n",
        "    - Selective improvements (better normalization, initialization)\n",
        "    - Optional attention for when needed\n",
        "    - Better loss balancing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, latent_dim=128, hidden_dims=[512, 256, 128],\n",
        "                 use_residual=True, dropout_rate=0.3, use_attention=False):\n",
        "        super(ImprovedVAE, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.use_residual = use_residual\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        # Feature importance network (proven from original)\n",
        "        self.feature_importance = nn.Sequential(\n",
        "            nn.Linear(input_dim * 2, hidden_dims[0] // 2),  # input + mask\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0] // 2, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Optional attention mechanism (disabled by default for simplicity)\n",
        "        if use_attention:\n",
        "            self.feature_embedding = nn.Sequential(\n",
        "                nn.Linear(2, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(32, 32)\n",
        "            )\n",
        "            self.attention = nn.MultiheadAttention(\n",
        "                embed_dim=32, num_heads=4, dropout=dropout_rate, batch_first=True\n",
        "            )\n",
        "            self.attention_norm = nn.LayerNorm(32)\n",
        "            encoder_input_dim = 32 * input_dim\n",
        "        else:\n",
        "            encoder_input_dim = input_dim * 2\n",
        "\n",
        "        # Encoder with residual connections - using proven BatchNorm from original\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        prev_dim = encoder_input_dim\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            layer = nn.Sequential(\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),  # Use BatchNorm like original (proven to work)\n",
        "                nn.ReLU(),  # Use ReLU like original\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "            self.encoder_layers.append(layer)\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Latent space with balanced initialization\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "\n",
        "        # Balanced initialization - not too aggressive\n",
        "        nn.init.xavier_normal_(self.fc_mu.weight, gain=0.1)\n",
        "        nn.init.xavier_normal_(self.fc_logvar.weight, gain=0.1)\n",
        "        nn.init.constant_(self.fc_logvar.bias, -2.0)  # Start with low variance like original\n",
        "\n",
        "        # Decoder input dimension\n",
        "        if use_attention:\n",
        "            self.cross_attention = nn.MultiheadAttention(\n",
        "                embed_dim=32, num_heads=4, dropout=dropout_rate, batch_first=True\n",
        "            )\n",
        "            self.cross_attention_norm = nn.LayerNorm(32)\n",
        "            self.latent_proj = nn.Linear(latent_dim, 32)\n",
        "            decoder_input_dim = latent_dim + 32 * input_dim\n",
        "        else:\n",
        "            decoder_input_dim = latent_dim + input_dim\n",
        "\n",
        "        # Decoder with skip connections - simpler like original\n",
        "        self.decoder_layers = nn.ModuleList()\n",
        "        reversed_dims = list(reversed(hidden_dims))\n",
        "        prev_dim = decoder_input_dim\n",
        "\n",
        "        for i, hidden_dim in enumerate(reversed_dims):\n",
        "            layer = nn.Sequential(\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),  # Use BatchNorm like original\n",
        "                nn.ReLU(),  # Use ReLU like original\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "            self.decoder_layers.append(layer)\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Output layer - simpler like original\n",
        "        self.output_layer = nn.Linear(hidden_dims[0], input_dim)\n",
        "        nn.init.xavier_normal_(self.output_layer.weight, gain=0.1)\n",
        "\n",
        "    def encode(self, x, mask):\n",
        "        \"\"\"Encode input with missing value masking - using proven approach.\"\"\"\n",
        "        # Calculate feature importance weights (from original)\n",
        "        mask_float = mask.float()\n",
        "        encoder_input = torch.cat([x * mask_float, mask_float], dim=1)\n",
        "        importance_weights = self.feature_importance(encoder_input)\n",
        "\n",
        "        # Apply importance weighting to the input\n",
        "        weighted_input = x * mask_float * importance_weights\n",
        "        encoder_input = torch.cat([weighted_input, mask_float], dim=1)\n",
        "        \n",
        "        # Optional attention path\n",
        "        if self.use_attention:\n",
        "            batch_size = x.size(0)\n",
        "            x_expanded = x.unsqueeze(-1)\n",
        "            mask_expanded = mask_float.unsqueeze(-1)\n",
        "            feature_input = torch.cat([x_expanded, mask_expanded], dim=-1)\n",
        "            feature_embeds = self.feature_embedding(feature_input)\n",
        "            attn_out, _ = self.attention(feature_embeds, feature_embeds, feature_embeds)\n",
        "            feature_embeds = self.attention_norm(feature_embeds + attn_out)\n",
        "            encoder_input = feature_embeds.reshape(batch_size, -1)\n",
        "\n",
        "        # Pass through encoder layers with residual connections\n",
        "        h = encoder_input\n",
        "        skip_connections = []\n",
        "\n",
        "        for i, layer in enumerate(self.encoder_layers):\n",
        "            prev_h = h\n",
        "            h = layer(h)\n",
        "\n",
        "            # Add residual connection for deeper layers\n",
        "            if self.use_residual and i > 0 and h.shape == prev_h.shape:\n",
        "                h = h + prev_h  # Full residual like original\n",
        "\n",
        "            skip_connections.append(h)\n",
        "\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "\n",
        "        # Clamp logvar to prevent numerical instability\n",
        "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
        "\n",
        "        return mu, logvar, skip_connections\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"Reparameterization trick with better numerical stability.\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, x_observed, mask):\n",
        "        \"\"\"Decode latent representation conditioned on observed values.\"\"\"\n",
        "        # Enhanced conditioning on observed values\n",
        "        mask_float = mask.float()\n",
        "        x_masked = x_observed * mask_float\n",
        "\n",
        "        # Optional attention path\n",
        "        if self.use_attention:\n",
        "            batch_size = z.size(0)\n",
        "            x_expanded = x_observed.unsqueeze(-1)\n",
        "            mask_expanded = mask_float.unsqueeze(-1)\n",
        "            observed_input = torch.cat([x_expanded, mask_expanded], dim=-1)\n",
        "            observed_embeds = self.feature_embedding(observed_input)\n",
        "            z_expanded = z.unsqueeze(1).expand(-1, self.input_dim, -1)\n",
        "            z_proj = self.latent_proj(z_expanded)\n",
        "            attn_out, _ = self.cross_attention(z_proj, observed_embeds, observed_embeds)\n",
        "            conditioned_embeds = self.cross_attention_norm(z_proj + attn_out)\n",
        "            conditioned_flat = conditioned_embeds.reshape(batch_size, -1)\n",
        "            decoder_input = torch.cat([z, conditioned_flat], dim=1)\n",
        "        else:\n",
        "            # Standard decoding with positional encoding (from original approach)\n",
        "            pos_encoding = torch.arange(self.input_dim, dtype=torch.float32, device=z.device)\n",
        "            pos_encoding = pos_encoding.unsqueeze(0).expand(z.size(0), -1) / self.input_dim\n",
        "            decoder_input = torch.cat([z, x_masked, pos_encoding], dim=1)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        h = decoder_input\n",
        "        for layer in self.decoder_layers:\n",
        "            h = layer(h)\n",
        "\n",
        "        # Get reconstruction\n",
        "        reconstruction = self.output_layer(h)\n",
        "\n",
        "        return reconstruction\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"Forward pass through improved VAE.\"\"\"\n",
        "        mu, logvar, _ = self.encode(x, mask)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        reconstruction = self.decode(z, x, mask)\n",
        "        return reconstruction, mu, logvar\n",
        "\n",
        "    def impute(self, x_incomplete, mask, n_samples=10):\n",
        "        \"\"\"Generate multiple imputation samples for missing values.\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            mu, logvar, _ = self.encode(x_incomplete, mask)\n",
        "            samples = []\n",
        "            for _ in range(n_samples):\n",
        "                z = self.reparameterize(mu, logvar)\n",
        "                reconstruction = self.decode(z, x_incomplete, mask)\n",
        "                mask_float = mask.float()\n",
        "                imputed = x_incomplete * mask_float + reconstruction * (1 - mask_float)\n",
        "                samples.append(imputed.cpu().numpy())\n",
        "            samples = np.stack(samples, axis=1)\n",
        "        return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0835855",
      "metadata": {
        "id": "f0835855"
      },
      "outputs": [],
      "source": [
        "# Improved Loss Functions with Distribution Matching\n",
        "\n",
        "def compute_mmd_loss(x_real, x_fake, kernel='rbf', sigma=1.0):\n",
        "    \"\"\"Compute Maximum Mean Discrepancy (MMD) loss for distribution matching.\"\"\"\n",
        "    try:\n",
        "        # Flatten and compute pairwise distances\n",
        "        x_real_flat = x_real.view(x_real.size(0), -1)\n",
        "        x_fake_flat = x_fake.view(x_fake.size(0), -1)\n",
        "        \n",
        "        # Compute pairwise distances\n",
        "        xx = torch.cdist(x_real_flat, x_real_flat, p=2) ** 2\n",
        "        yy = torch.cdist(x_fake_flat, x_fake_flat, p=2) ** 2\n",
        "        xy = torch.cdist(x_real_flat, x_fake_flat, p=2) ** 2\n",
        "        \n",
        "        # RBF kernel\n",
        "        gamma = 1.0 / (2 * sigma ** 2)\n",
        "        K_xx = torch.exp(-gamma * xx)\n",
        "        K_yy = torch.exp(-gamma * yy)\n",
        "        K_xy = torch.exp(-gamma * xy)\n",
        "        \n",
        "        # MMD^2 = E[K(xx)] + E[K(yy)] - 2*E[K(xy)]\n",
        "        mmd = K_xx.mean() + K_yy.mean() - 2 * K_xy.mean()\n",
        "        return mmd\n",
        "    except:\n",
        "        return torch.tensor(0.0, device=x_real.device)\n",
        "\n",
        "def compute_distribution_loss(recon_x, x, mask):\n",
        "    \"\"\"Compute distribution matching loss (feature-wise mean and std matching).\"\"\"\n",
        "    observed_mask = mask.float()\n",
        "    \n",
        "    # Only compute on observed values to match training distribution\n",
        "    recon_observed = recon_x * observed_mask\n",
        "    x_observed = x * observed_mask\n",
        "    \n",
        "    # Feature-wise statistics\n",
        "    # Mean matching\n",
        "    recon_mean = recon_observed.sum(dim=0) / (observed_mask.sum(dim=0) + 1e-8)\n",
        "    x_mean = x_observed.sum(dim=0) / (observed_mask.sum(dim=0) + 1e-8)\n",
        "    mean_loss = F.mse_loss(recon_mean, x_mean)\n",
        "    \n",
        "    # Std matching (variance matching)\n",
        "    recon_centered = (recon_observed - recon_mean.unsqueeze(0)) * observed_mask\n",
        "    x_centered = (x_observed - x_mean.unsqueeze(0)) * observed_mask\n",
        "    \n",
        "    recon_var = (recon_centered ** 2).sum(dim=0) / (observed_mask.sum(dim=0) + 1e-8)\n",
        "    x_var = (x_centered ** 2).sum(dim=0) / (observed_mask.sum(dim=0) + 1e-8)\n",
        "    std_loss = F.mse_loss(torch.sqrt(recon_var + 1e-8), torch.sqrt(x_var + 1e-8))\n",
        "    \n",
        "    return mean_loss + std_loss\n",
        "\n",
        "def improved_vae_loss_function(recon_x, x, mu, logvar, mask, beta=1.0, \n",
        "                                gamma=0.02, delta=0.01, use_distribution_loss=True):\n",
        "    \"\"\"\n",
        "    Balanced VAE loss function - proven reconstruction + light distribution matching.\n",
        "\n",
        "    Args:\n",
        "        recon_x: Reconstructed data\n",
        "        x: Original data\n",
        "        mu: Mean of latent distribution\n",
        "        logvar: Log variance of latent distribution\n",
        "        mask: Binary mask (1 for observed, 0 for missing)\n",
        "        beta: Weight for KL divergence term\n",
        "        gamma: Weight for distribution matching loss (mean/std) - very light\n",
        "        delta: Weight for MMD loss - very light\n",
        "        use_distribution_loss: Whether to use distribution matching losses\n",
        "    \"\"\"\n",
        "    # Reconstruction loss - using proven approach from original\n",
        "    reconstruction_diff = (recon_x - x) ** 2\n",
        "\n",
        "    # Only consider observed values and normalize properly\n",
        "    masked_loss = reconstruction_diff * mask\n",
        "    recon_loss = masked_loss.sum() / (mask.sum() + 1e-8)\n",
        "\n",
        "    # Add standard MSE loss for stability (proven combination)\n",
        "    standard_recon_loss = F.mse_loss(recon_x * mask, x * mask, reduction='mean')\n",
        "    recon_loss = 0.7 * recon_loss + 0.3 * standard_recon_loss\n",
        "\n",
        "    # KL divergence with free bits to prevent posterior collapse\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    kl_loss = kl_loss / x.size(0)  # Normalize by batch size\n",
        "    \n",
        "    # Very light distribution matching (only if enabled and weights are non-zero)\n",
        "    dist_loss = torch.tensor(0.0, device=x.device)\n",
        "    mmd_loss = torch.tensor(0.0, device=x.device)\n",
        "    \n",
        "    if use_distribution_loss and (gamma > 0 or delta > 0):\n",
        "        observed_mask = mask.float()\n",
        "        \n",
        "        # Only compute distribution losses occasionally to reduce overhead\n",
        "        if gamma > 0 and torch.rand(1).item() < 0.3:  # 30% of the time\n",
        "            dist_loss = compute_distribution_loss(recon_x, x, mask)\n",
        "        \n",
        "        # MMD loss - very rarely to reduce computational cost\n",
        "        if delta > 0 and x.size(0) > 1 and torch.rand(1).item() < 0.1:  # 10% of the time\n",
        "            n_samples = min(16, x.size(0))  # Smaller subset\n",
        "            indices = torch.randperm(x.size(0))[:n_samples]\n",
        "            x_subset = x[indices] * observed_mask[indices]\n",
        "            recon_subset = recon_x[indices] * observed_mask[indices]\n",
        "            \n",
        "            if x_subset.sum() > 0 and recon_subset.sum() > 0:\n",
        "                mmd_loss = compute_mmd_loss(x_subset, recon_subset, sigma=0.5)\n",
        "    \n",
        "    total_loss = recon_loss + beta * kl_loss + gamma * dist_loss + delta * mmd_loss\n",
        "\n",
        "    return total_loss, recon_loss, kl_loss, dist_loss, mmd_loss\n",
        "\n",
        "\n",
        "def get_beta_schedule(epoch, total_epochs, schedule_type='cosine'):\n",
        "    \"\"\"Get beta value for KL annealing schedule.\"\"\"\n",
        "    if schedule_type == 'linear':\n",
        "        return min(1.0, epoch / (total_epochs * 0.5))\n",
        "    elif schedule_type == 'sigmoid':\n",
        "        return 1.0 / (1.0 + np.exp(-(epoch - total_epochs * 0.5) / (total_epochs * 0.1)))\n",
        "    elif schedule_type == 'cosine':\n",
        "        return 0.5 * (1 + np.cos(np.pi * (1 - epoch / total_epochs)))\n",
        "    elif schedule_type == 'constant':\n",
        "        return 1.0\n",
        "    else:\n",
        "        return 1.0\n",
        "\n",
        "\n",
        "def evaluate_imputation(model, data_loader, device):\n",
        "    \"\"\"Evaluate imputation performance.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_imputations = []\n",
        "    all_originals = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in data_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            reconstruction, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "            # Combine observed values with imputed values\n",
        "            mask_float = batch_mask.float()\n",
        "            imputed = batch_data * mask_float + reconstruction * (1 - mask_float)\n",
        "\n",
        "            all_imputations.append(imputed.cpu().numpy())\n",
        "            all_originals.append(batch_data.cpu().numpy())\n",
        "            all_masks.append(batch_mask.cpu().numpy())\n",
        "\n",
        "    # Concatenate all results\n",
        "    imputations = np.vstack(all_imputations)\n",
        "    originals = np.vstack(all_originals)\n",
        "    masks = np.vstack(all_masks)\n",
        "\n",
        "    return imputations, originals, masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc20dea8",
      "metadata": {
        "id": "dc20dea8"
      },
      "source": [
        "### Loss Functions and Training Utilities\n",
        "\n",
        "The VAE loss function is crucial for training effectiveness. Our enhanced loss combines several components:\n",
        "\n",
        "**1. Reconstruction Loss**: Measures how well the model reconstructs observed values\n",
        "   - Only computed on observed values (respects the mask)\n",
        "\n",
        "**2. KL Divergence**: Regularizes the latent space to follow a standard normal distribution\n",
        "   - Prevents posterior collapse using \"free bits\"\n",
        "   - Controlled by β parameter for annealing\n",
        "\n",
        "**Beta Scheduling**: Gradually increases the KL weight during training to balance reconstruction and regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6214c61b",
      "metadata": {
        "id": "6214c61b"
      },
      "source": [
        "## Model Initialization and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e98e7b7",
      "metadata": {
        "id": "5e98e7b7"
      },
      "source": [
        "### Model Training Process\n",
        "\n",
        "This section implements the complete training pipeline with several important features:\n",
        "\n",
        "**Training Configuration:**\n",
        "- **Latent Dimension**: 128 (balance between expressiveness and computational efficiency)\n",
        "- **Architecture**: Deep encoder/decoder with residual connections\n",
        "- **Regularization**: Dropout and batch normalization for stability\n",
        "- **Optimization**: AdamW with cosine annealing for smooth convergence\n",
        "\n",
        "**Advanced Training Features:**\n",
        "- **Early Stopping**: Prevents overfitting by monitoring validation loss\n",
        "- **Gradient Clipping**: Ensures stable training by preventing exploding gradients  \n",
        "- **Beta Scheduling**: Gradual KL annealing for better latent space learning\n",
        "- **Learning Rate Scheduling**: Cosine annealing with warm restarts\n",
        "\n",
        "The training loop tracks multiple loss components to monitor model health and convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39e2a3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e39e2a3e",
        "outputId": "1cb0aef4-4565-4d6f-f061-29c39ff79ebd"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"IMPROVED MODEL TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Balanced model configuration - combining proven settings with light improvements\n",
        "config = {\n",
        "    'input_dim': len(feature_names),\n",
        "    'latent_dim': 128,  # Proven size from original\n",
        "    'hidden_dims': [512, 256, 128],  # Proven architecture from original\n",
        "    'use_residual': True,\n",
        "    'dropout_rate': 0.3,  # Proven dropout from original\n",
        "    'use_attention': False,  # Disabled by default - simpler is better\n",
        "    'learning_rate': 1e-3,  # Proven learning rate from original\n",
        "    'num_epochs': 500,  # Proven number of epochs\n",
        "    'beta_initial': 1.0,  # Start with full beta like original\n",
        "    'beta_schedule': 'cosine',  # Proven schedule from original\n",
        "    'patience': 15,  # Proven patience from original\n",
        "    'gamma': 0.02,  # Very light distribution matching (was 0.15)\n",
        "    'delta': 0.01  # Very light MMD loss (was 0.05)\n",
        "}\n",
        "\n",
        "print(f\"Model Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  - {key}: {value}\")\n",
        "\n",
        "# Initialize the improved model\n",
        "print(f\"\\nInitializing Improved VAE model...\")\n",
        "model = ImprovedVAE(\n",
        "    input_dim=config['input_dim'],\n",
        "    latent_dim=config['latent_dim'],\n",
        "    hidden_dims=config['hidden_dims'],\n",
        "    use_residual=config['use_residual'],\n",
        "    dropout_rate=config['dropout_rate'],\n",
        "    use_attention=config['use_attention']\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"✓ Model initialized with {total_params:,} parameters\")\n",
        "\n",
        "# Initialize optimizer with proven settings from original\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['learning_rate'],\n",
        "    weight_decay=1e-5,  # Proven weight decay from original\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "# Use proven scheduler from original\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=20, T_mult=2, eta_min=1e-6\n",
        ")\n",
        "\n",
        "print(f\"✓ Optimizer and scheduler initialized\")\n",
        "\n",
        "# Training setup\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_recon_losses = []\n",
        "train_kl_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "print(f\"\\nStarting training for {config['num_epochs']} epochs...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(config['num_epochs']):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    epoch_train_loss = 0\n",
        "    epoch_recon_loss = 0\n",
        "    epoch_kl_loss = 0\n",
        "\n",
        "    # Get beta for this epoch\n",
        "    beta = get_beta_schedule(epoch, config['num_epochs'], config['beta_schedule'])\n",
        "\n",
        "    train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"num_epochs\"]}', leave=False)\n",
        "\n",
        "    for batch_data, batch_mask in train_progress:\n",
        "        batch_data = batch_data.to(device)\n",
        "        batch_mask = batch_mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        reconstruction, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "        # Calculate balanced loss with light distribution matching\n",
        "        total_loss, recon_loss, kl_loss, dist_loss, mmd_loss = improved_vae_loss_function(\n",
        "            reconstruction, batch_data, mu, logvar, batch_mask,\n",
        "            beta=beta, \n",
        "            gamma=config['gamma'], delta=config['delta'],\n",
        "            use_distribution_loss=True\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        epoch_train_loss += total_loss.item()\n",
        "        epoch_recon_loss += recon_loss.item()\n",
        "        epoch_kl_loss += kl_loss.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        train_progress.set_postfix({\n",
        "            'Loss': f'{total_loss.item():.4f}',\n",
        "            'Recon': f'{recon_loss.item():.4f}',\n",
        "            'KL': f'{kl_loss.item():.4f}',\n",
        "            'Dist': f'{dist_loss.item():.4f}',\n",
        "            'Beta': f'{beta:.3f}'\n",
        "        })\n",
        "\n",
        "    # Calculate average training losses\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "    avg_recon_loss = epoch_recon_loss / len(train_loader)\n",
        "    avg_kl_loss = epoch_kl_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_mask in val_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            reconstruction, mu, logvar = model(batch_data, batch_mask)\n",
        "\n",
        "            total_loss, _, _, _, _ = improved_vae_loss_function(\n",
        "                reconstruction, batch_data, mu, logvar, batch_mask,\n",
        "                beta=beta,\n",
        "                gamma=config['gamma'], delta=config['delta'],\n",
        "                use_distribution_loss=True\n",
        "            )\n",
        "\n",
        "            epoch_val_loss += total_loss.item()\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "\n",
        "    # Store losses\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    train_recon_losses.append(avg_recon_loss)\n",
        "    train_kl_losses.append(avg_kl_loss)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step()\n",
        "\n",
        "    # Early stopping and model saving\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), 'best_vae_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{config[\"num_epochs\"]}:')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f} (Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f})')\n",
        "        print(f'  Val Loss: {avg_val_loss:.4f}, Best: {best_val_loss:.4f}')\n",
        "        print(f'  Beta: {beta:.3f}, LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
        "        print(f'  Patience: {patience_counter}/{config[\"patience\"]}')\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= config['patience']:\n",
        "        print(f'\\nEarly stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "print(f'\\n✓ Training completed!')\n",
        "print(f'  - Total epochs: {len(train_losses)}')\n",
        "print(f'  - Best validation loss: {best_val_loss:.4f}')\n",
        "print(f'  - Final training loss: {train_losses[-1]:.4f}')\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_vae_model.pth'))\n",
        "print(f'✓ Best model loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb9718e",
      "metadata": {
        "id": "abb9718e"
      },
      "source": [
        "## Model Evaluation and Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a04c0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11a04c0c",
        "outputId": "bc410595-ee1b-4b1e-85ff-30aacfe1353c"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Evaluating model on test set...\")\n",
        "test_imputations, test_originals, test_masks = evaluate_imputation(\n",
        "    model, test_loader, device\n",
        ")\n",
        "\n",
        "print(f\"✓ Test set evaluation completed\")\n",
        "print(f\"  - Test samples: {test_imputations.shape[0]}\")\n",
        "print(f\"  - Features: {test_imputations.shape[1]}\")\n",
        "\n",
        "test_imputations_denorm = test_imputations  # Already in original scale\n",
        "test_original_denorm = X_test_original  # Already in original scale\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "print(\"\\nCalculating comprehensive metrics...\")\n",
        "feature_metrics = {}\n",
        "\n",
        "# Create masks for missing values (where we need to evaluate imputation)\n",
        "missing_mask = (test_masks == 0)  # True where values were missing (0 in model tensors = missing)\n",
        "\n",
        "for i, feature_name in enumerate(feature_names):\n",
        "    if missing_mask[:, i].sum() > 0:  # Only evaluate features with missing values\n",
        "        # Get imputed and ground truth values for missing positions only\n",
        "        imputed_missing = test_imputations_denorm[missing_mask[:, i], i]\n",
        "        ground_truth_missing = test_original_denorm[missing_mask[:, i], i]\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(ground_truth_missing, imputed_missing)\n",
        "        mae = mean_absolute_error(ground_truth_missing, imputed_missing)\n",
        "\n",
        "        # Correlation\n",
        "        try:\n",
        "            correlation = np.corrcoef(ground_truth_missing, imputed_missing)[0, 1]\n",
        "        except:\n",
        "            correlation = np.nan\n",
        "\n",
        "        # Mean difference and Jensen-Shannon divergence\n",
        "        mean_diff, js_div = calculate_jsd_and_mean_diff(\n",
        "            imputed_missing, ground_truth_missing, feature_name\n",
        "        )\n",
        "\n",
        "        feature_metrics[feature_name] = {\n",
        "            'n_missing': missing_mask[:, i].sum(),\n",
        "            'mse': mse,\n",
        "            'mae': mae,\n",
        "            'correlation': correlation,\n",
        "            'mean_difference': mean_diff,\n",
        "            'js_divergence': js_div,\n",
        "        }\n",
        "\n",
        "print(f\"✓ Metrics calculated for {len(feature_metrics)} features with missing values\")\n",
        "\n",
        "# Display metrics for last 4 features (as requested)\n",
        "print(f\"\\n\" + \"=\"*100)\n",
        "print(\"METRICS FOR LAST 4 FEATURES\")\n",
        "print(\"=\"*100)\n",
        "print(f\"{'Feature':<15} {'N_Miss':<8} {'MSE':<10} {'MAE':<10} {'Corr':<8} {'Mean_Diff':<10} {'JS_Div':<8}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "last_4_features = list(feature_metrics.keys())[-4:] if len(feature_metrics) >= 4 else list(feature_metrics.keys())\n",
        "\n",
        "for feature in last_4_features:\n",
        "    metrics = feature_metrics[feature]\n",
        "    print(f\"{feature:<15} {metrics['n_missing']:<8} {metrics['mse']:<10.4f} {metrics['mae']:<10.4f} \"\n",
        "          f\"{metrics['correlation']:<8.3f} {metrics['mean_difference']:<10.4f} {metrics['js_divergence']:<8.4f} \")\n",
        "\n",
        "# Summary statistics\n",
        "all_mse = [m['mse'] for m in feature_metrics.values() if not np.isnan(m['mse'])]\n",
        "all_mae = [m['mae'] for m in feature_metrics.values() if not np.isnan(m['mae'])]\n",
        "all_corr = [m['correlation'] for m in feature_metrics.values() if not np.isnan(m['correlation'])]\n",
        "all_mean_diff = [m['mean_difference'] for m in feature_metrics.values() if not np.isnan(m['mean_difference'])]\n",
        "all_js_div = [m['js_divergence'] for m in feature_metrics.values() if not np.isnan(m['js_divergence'])]\n",
        "\n",
        "print(f\"\\nSummary Statistics Across All Features:\")\n",
        "print(f\"  - Average MSE: {np.mean(all_mse):.4f} ± {np.std(all_mse):.4f}\")\n",
        "print(f\"  - Average MAE: {np.mean(all_mae):.4f} ± {np.std(all_mae):.4f}\")\n",
        "print(f\"  - Average Correlation: {np.mean(all_corr):.3f} ± {np.std(all_corr):.3f}\")\n",
        "print(f\"  - Average Mean Difference: {np.mean(all_mean_diff):.4f} ± {np.std(all_mean_diff):.4f}\")\n",
        "print(f\"  - Average JS Divergence: {np.mean(all_js_div):.4f} ± {np.std(all_js_div):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43ff39cd",
      "metadata": {
        "id": "43ff39cd"
      },
      "source": [
        "### Model Evaluation and Comprehensive Metrics\n",
        "\n",
        "This section evaluates our trained VAE on the test set using multiple complementary metrics. Since we're dealing with missing value imputation, we only evaluate the model's predictions on positions that were originally missing.\n",
        "\n",
        "**Key Evaluation Metrics:**\n",
        "\n",
        "**1. Mean Squared Error (MSE)**:\n",
        "- Measures average squared difference between predicted and true values\n",
        "- Lower is better; sensitive to outliers\n",
        "- Good for understanding magnitude of errors\n",
        "\n",
        "**2. Correlation Coefficient**:\n",
        "- Measures linear relationship strength between predictions and dataset\n",
        "- Range: [-1, 1], closer to 1 is better\n",
        "- Shows if model captures feature relationships\n",
        "\n",
        "**3. Jensen-Shannon (JS) Divergence**:\n",
        "- Measures difference between predicted and true value distributions\n",
        "- Range: [0, 1], closer to 0 is better\n",
        "- Captures whether model preserves the overall data distribution\n",
        "\n",
        "**4. Maximum Mean Discrepancy (MMD)**:\n",
        "- Measures distributional difference using kernel methods (RBF kernel)\n",
        "- Range: [0, ∞], closer to 0 is better\n",
        "- Non-parametric test for comparing distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a047bb73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a047bb73",
        "outputId": "5cc6a8a5-9ade-4bb8-f7a1-d44721e9b688"
      },
      "outputs": [],
      "source": [
        "# Create the visualization\n",
        "plot_prediction_scatter(test_imputations_denorm, test_original_denorm, test_masks, feature_names, n_features=25)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7beacdd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_distribution_comparison(test_imputations_denorm, test_original_denorm, test_masks, feature_names, n_features=25):\n",
        "    \"\"\"\n",
        "    Create distribution comparison plots for random features in a 5x5 grid.\n",
        "    \n",
        "    Args:\n",
        "        test_imputations_denorm: Denormalized imputed values\n",
        "        test_original_denorm: Denormalized ground truth values  \n",
        "        test_masks: Binary masks (1=observed, 0=missing)\n",
        "        feature_names: List of feature names\n",
        "        n_features: Number of features to plot (default 25 for 5x5 grid)\n",
        "    \"\"\"\n",
        "    # Find features that have missing values\n",
        "    features_with_missing = []\n",
        "    for i, feature_name in enumerate(feature_names):\n",
        "        missing_positions = (test_masks[:, i] == 0)  # 0 = missing in model tensors\n",
        "        if missing_positions.sum() > 0:\n",
        "            features_with_missing.append((i, feature_name))\n",
        "    \n",
        "    if len(features_with_missing) < n_features:\n",
        "        n_features = len(features_with_missing)\n",
        "        print(f\"Only {n_features} features have missing values, showing all of them.\")\n",
        "\n",
        "    # Create 5x8 grid\n",
        "    fig, axes = plt.subplots(5, 8, figsize=(20, 16))\n",
        "    fig.suptitle('Distribution Comparison: Dataset vs Imputed Values', fontsize=16, fontweight='bold')\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, (feature_idx, feature_name) in enumerate(features_with_missing):\n",
        "        if idx >= 40:  # Safety check for 5x8 grid\n",
        "            break\n",
        "            \n",
        "        # Get imputed and ground truth values for missing positions only\n",
        "        missing_positions = (test_masks[:, feature_idx] == 0)  # 0 = missing in model tensors\n",
        "        \n",
        "        if missing_positions.sum() > 0:\n",
        "            imputed_values = test_imputations_denorm[missing_positions, feature_idx]\n",
        "            ground_truth_values = test_original_denorm[missing_positions, feature_idx]\n",
        "            \n",
        "            # Remove any NaN or infinite values\n",
        "            valid_mask = np.isfinite(imputed_values) & np.isfinite(ground_truth_values)\n",
        "            imputed_clean = imputed_values[valid_mask]\n",
        "            gt_clean = ground_truth_values[valid_mask]\n",
        "            \n",
        "            if len(imputed_clean) > 0 and len(gt_clean) > 0:\n",
        "                # Create histograms\n",
        "                ax = axes[idx]\n",
        "                \n",
        "                # Calculate bins for both distributions\n",
        "                all_values = np.concatenate([imputed_clean, gt_clean])\n",
        "                bins = np.linspace(all_values.min(), all_values.max(), 20)  # Fewer bins for smaller plots\n",
        "                \n",
        "                # Plot histograms\n",
        "                ax.hist(gt_clean, bins=bins, alpha=0.7, label='Dataset', \n",
        "                    color='skyblue', density=True, edgecolor='black', linewidth=0.3)\n",
        "                ax.hist(imputed_clean, bins=bins, alpha=0.7, label='Imputed', \n",
        "                    color='lightcoral', density=True, edgecolor='black', linewidth=0.3)\n",
        "                \n",
        "                # Add statistical information\n",
        "                gt_mean, gt_std = gt_clean.mean(), gt_clean.std()\n",
        "                imp_mean, imp_std = imputed_clean.mean(), imputed_clean.std()\n",
        "                correlation = np.corrcoef(gt_clean, imputed_clean)[0, 1] if len(gt_clean) > 1 else 0\n",
        "\n",
        "                # Calculate MMD (Maximum Mean Discrepancy)\n",
        "                def rbf_kernel(X, Y, gamma=1.0):\n",
        "                    \"\"\"RBF kernel for MMD calculation\"\"\"\n",
        "                    XX = np.sum(X**2, axis=1, keepdims=True)\n",
        "                    YY = np.sum(Y**2, axis=1, keepdims=True)\n",
        "                    XY = np.dot(X, Y.T)\n",
        "                    distances = XX + YY.T - 2*XY\n",
        "                    return np.exp(-gamma * distances)\n",
        "                \n",
        "                def mmd_rbf(X, Y, gamma=1.0):\n",
        "                    \"\"\"Calculate MMD with RBF kernel\"\"\"\n",
        "                    X = X.reshape(-1, 1)\n",
        "                    Y = Y.reshape(-1, 1)\n",
        "                    \n",
        "                    m, n = len(X), len(Y)\n",
        "                    \n",
        "                    K_XX = rbf_kernel(X, X, gamma)\n",
        "                    K_YY = rbf_kernel(Y, Y, gamma)\n",
        "                    K_XY = rbf_kernel(X, Y, gamma)\n",
        "                    \n",
        "                    mmd = (np.sum(K_XX) / (m * m) + \n",
        "                           np.sum(K_YY) / (n * n) - \n",
        "                           2 * np.sum(K_XY) / (m * n))\n",
        "                    return np.sqrt(max(mmd, 0))  # Ensure non-negative\n",
        "                \n",
        "                try:\n",
        "                    mmd_value = mmd_rbf(gt_clean, imputed_clean)\n",
        "                except:\n",
        "                    mmd_value = np.nan\n",
        "\n",
        "                # Calculate Jensen-Shannon Divergence\n",
        "                try:\n",
        "                    # Create histograms with same bins for JSD\n",
        "                    data_range = (min(gt_clean.min(), imputed_clean.min()), \n",
        "                                 max(gt_clean.max(), imputed_clean.max()))\n",
        "                    \n",
        "                    if data_range[1] == data_range[0]:\n",
        "                        jsd_value = 0.0  # No divergence if all values are the same\n",
        "                    else:\n",
        "                        bins = np.linspace(data_range[0], data_range[1], 30)\n",
        "                        \n",
        "                        # Get histogram probabilities\n",
        "                        hist_gt, _ = np.histogram(gt_clean, bins=bins, density=True)\n",
        "                        hist_imp, _ = np.histogram(imputed_clean, bins=bins, density=True)\n",
        "                        \n",
        "                        # Normalize to probabilities\n",
        "                        hist_gt = hist_gt + 1e-10  # Add small epsilon to avoid zeros\n",
        "                        hist_imp = hist_imp + 1e-10\n",
        "                        hist_gt = hist_gt / hist_gt.sum()\n",
        "                        hist_imp = hist_imp / hist_imp.sum()\n",
        "                        \n",
        "                        # Calculate Jensen-Shannon divergence\n",
        "                        jsd_value = jensenshannon(hist_gt, hist_imp)\n",
        "                except:\n",
        "                    jsd_value = np.nan\n",
        "\n",
        "                # Add vertical lines for means\n",
        "                ax.axvline(gt_mean, color='blue', linestyle='--', alpha=0.8, linewidth=1, label='Dataset Mean' if idx == 0 else \"\")\n",
        "                ax.axvline(imp_mean, color='red', linestyle='--', alpha=0.8, linewidth=1, label='Imputed Mean' if idx == 0 else \"\")\n",
        "                \n",
        "                # Set labels and title (smaller font for 5x5 grid)\n",
        "                ax.set_xlabel(f'{feature_name[:15]}', fontsize=8)  # Truncate long names\n",
        "                ax.set_ylabel('Density', fontsize=8)\n",
        "                ax.tick_params(labelsize=7)\n",
        "                \n",
        "                # Add correlation, MMD, and JSD as title\n",
        "                ax.set_title(f'R²={correlation:.3f}, MMD={mmd_value:.3f}, JSD={jsd_value:.3f}', fontsize=7, fontweight='bold')\n",
        "                \n",
        "                # Add legend only to first plot\n",
        "                if idx == 0:\n",
        "                    ax.legend(fontsize=7, loc='upper right')\n",
        "                \n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            else:\n",
        "                axes[idx].text(0.5, 0.5, f'{feature_name[:15]}\\nNo valid data', \n",
        "                            ha='center', va='center', transform=axes[idx].transAxes, fontsize=8)\n",
        "                axes[idx].set_title(f'{feature_name[:15]} - No Valid Data', fontsize=8)\n",
        "        else:\n",
        "            axes[idx].text(0.5, 0.5, f'{feature_name[:15]}\\nNo missing values', \n",
        "                        ha='center', va='center', transform=axes[idx].transAxes, fontsize=8)\n",
        "            axes[idx].set_title(f'{feature_name[:15]} - No Missing Values', fontsize=8)\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for idx in range(len(features_with_missing), 40):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IDgv-wkViD-u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "IDgv-wkViD-u",
        "outputId": "4eaea130-04a8-4ca0-b95a-0722caa11e89"
      },
      "outputs": [],
      "source": [
        "# Distribution comparison plots\n",
        "plot_distribution_comparison(test_imputations_denorm, test_original_denorm,\n",
        "                             test_masks, feature_names, n_features=25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be0b2e3",
      "metadata": {
        "id": "4be0b2e3"
      },
      "source": [
        "### Distribution Comparison Visualizations\n",
        "\n",
        "Visual comparison of predicted vs. dataset distributions is crucial for understanding model performance beyond simple error metrics. These plots help us assess:\n",
        "\n",
        "**What the Plots Show:**\n",
        "- **Red (Imputed)**: Distribution of model's predicted values for missing positions\n",
        "- **Blue (Dataset)**: Distribution of actual values at those same positions\n",
        "- **Overlap**: How well the model captures the true data distribution\n",
        "\n",
        "**Why This Matters:**\n",
        "- A good generative model should not just minimize error, but also preserve the statistical properties of the data\n",
        "- If distributions match well, the model is generating realistic values\n",
        "- Large differences indicate the model may be systematically biased or missing important patterns\n",
        "\n",
        "**Interpretation:**\n",
        "- **Good**: Overlapping distributions with similar shapes and centers\n",
        "- **Concerning**: Shifted means, different variances, or completely different shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lI7StssOGmfp",
      "metadata": {
        "id": "lI7StssOGmfp"
      },
      "outputs": [],
      "source": [
        "def generate_samples(model, X_test, test_loader, device, n_samples_per_test=100, temperature=1.0):\n",
        "    \"\"\"Generate multiple diverse samples for a dataset using the trained model.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained VAE model\n",
        "        X_test: Test data\n",
        "        test_loader: DataLoader for test data\n",
        "        device: Device to run on\n",
        "        n_samples_per_test: Number of samples to generate per test instance\n",
        "        temperature: Temperature for sampling (higher = more diverse)\n",
        "    \"\"\"\n",
        "    # We'll generate multiple samples\n",
        "    test_samples = np.zeros((X_test.shape[0], n_samples_per_test, X_test.shape[1]))\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Create a progress bar for all samples\n",
        "        from tqdm import tqdm\n",
        "\n",
        "        for batch_idx, (batch_data, batch_mask) in enumerate(tqdm(test_loader, desc=\"Generating Samples\")):\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Calculate the indices for this batch\n",
        "            start_idx = batch_idx * test_loader.batch_size\n",
        "            end_idx = min(start_idx + test_loader.batch_size, X_test.shape[0])\n",
        "            actual_batch_size = end_idx - start_idx\n",
        "\n",
        "            # Get latent distribution parameters\n",
        "            mu, logvar = model.encode(batch_data, batch_mask)\n",
        "            \n",
        "            # Generate multiple samples for each item in the batch\n",
        "            for j in range(n_samples_per_test):\n",
        "                # Sample from latent space with temperature scaling for diversity\n",
        "                # Higher temperature increases diversity\n",
        "                scaled_logvar = logvar + 2 * np.log(temperature)\n",
        "                z = model.reparameterize(mu, scaled_logvar)\n",
        "                \n",
        "                # Decode to get reconstruction\n",
        "                reconstruction = model.decode(z, batch_data, batch_mask)\n",
        "\n",
        "                # Apply mask: keep original values where available, use reconstructed values where missing\n",
        "                mask_float = batch_mask.float()\n",
        "                imputed = batch_data * mask_float + reconstruction * (1 - mask_float)\n",
        "                \n",
        "                # Clamp values to valid range [0, 1]\n",
        "                imputed = torch.clamp(imputed, 0.0, 1.0)\n",
        "\n",
        "                # Store the samples (already in original scale since we didn't normalize)\n",
        "                test_samples[start_idx:end_idx, j, :] = imputed.cpu().numpy()\n",
        "    \n",
        "    print(f\"✓ Generated samples shape: {test_samples.shape}\")\n",
        "    print(f\"  - {test_samples.shape[0]} samples\")\n",
        "    print(f\"  - {test_samples.shape[1]} generated variations per sample\")\n",
        "    print(f\"  - {test_samples.shape[2]} features per sample\")\n",
        "\n",
        "    # Data is already in original scale (no denormalization needed)\n",
        "    test_samples_final = test_samples.copy()\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    mean_across_samples = test_samples_final.mean(axis=1)  # Mean across the 100 samples\n",
        "    std_across_samples = test_samples_final.std(axis=1)  # Std across samples (diversity measure)\n",
        "\n",
        "    print(f\"  - Range of means: [{mean_across_samples.min():.4f}, {mean_across_samples.max():.4f}]\")\n",
        "    print(f\"  - Average std (diversity): {std_across_samples.mean():.4f}\")\n",
        "\n",
        "    return test_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sjkbko3oFJQV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjkbko3oFJQV",
        "outputId": "f5224d48-f821-428d-f99a-d059b8150dbc"
      },
      "outputs": [],
      "source": [
        "# Test Evaluation\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TEST EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate multiple samples for test using the trained model\n",
        "print(f\"Generating 100 samples for each of {X_test.shape[0]} test samples...\")\n",
        "\n",
        "test_samples = generate_samples(\n",
        "    model, X_test, test_loader, device, n_samples_per_test=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JFWebQ2AzSsJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFWebQ2AzSsJ",
        "outputId": "4c12f4f3-dadf-4384-f101-32fb536fb139"
      },
      "outputs": [],
      "source": [
        "test_score = compute_score(generated_samples=test_samples, set_name='test')\n",
        "print(\"Test score:\", test_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zlA9iSSZcYlh",
      "metadata": {
        "id": "zlA9iSSZcYlh"
      },
      "source": [
        "The final score is computed as: Mean Correlation − Mean JS Divergence − Mean MSE\n",
        "\n",
        "Just as we compare generated samples for the test set against the original unimputed values, we will apply the same metric to the samples you generate for test2, using the hidden test2 set. This will determine your final submission score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7de3489",
      "metadata": {
        "id": "b7de3489"
      },
      "source": [
        "## Preparing a submission:\n",
        "Let's prepare a submission. We expect the final submission to be a 417x100x37 numpy array. These correspond to the 100 diverse samples you generated based on the constrained parameters we provided in the test2 set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6a8623",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d6a8623",
        "outputId": "fa7b08fc-4fee-472f-ea8c-f6ab635e3bc2"
      },
      "outputs": [],
      "source": [
        "# Test2 Evaluation\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TEST2 EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate multiple samples for test2 using the trained model\n",
        "print(f\"Generating 100 samples for each of {X_test2.shape[0]} test2 samples...\")\n",
        "\n",
        "test2_samples = generate_samples(\n",
        "    model, X_test2, test2_loader, device, n_samples_per_test=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bacf0675",
      "metadata": {
        "id": "bacf0675"
      },
      "source": [
        "### Test2 Evaluation: Generating Diverse Design Completions\n",
        "\n",
        "This is the core evaluation for your AI Copilot assignment. Here we:\n",
        "\n",
        "**Input**: Test2 samples with some known features (constraints) and some missing features (free parameters)\n",
        "\n",
        "**Output**: 100 diverse, plausible completions for each test sample\n",
        "\n",
        "**Why 100 Samples?**\n",
        "- Engineers want to explore multiple design options, not just one \"best\" solution\n",
        "- Diversity helps discover unexpected but valid design combinations  \n",
        "\n",
        "**Technical Process:**\n",
        "1. For each test2 sample, use the trained model to generate 100 different completions\n",
        "2. Each completion respects the known constraints (observed values)\n",
        "3. Missing values are filled with diverse, model-generated predictions\n",
        "4. Final output: 417 × 100 × 37 array (417 test samples, 100 variants each, 37 features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc755a46",
      "metadata": {
        "id": "fc755a46"
      },
      "outputs": [],
      "source": [
        "id = np.random.randint(1e8, 1e9-1)\n",
        "np.save(f\"{id}.npy\", test2_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193851fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2044ff7",
      "metadata": {
        "id": "d2044ff7"
      },
      "source": [
        "### Summary and Tips for CP3\n",
        "\n",
        "The VAE baseline reproduces the dataset distribution well for some features, but others still show substantial discrepancies, indicating significant room for improvement!\n",
        "\n",
        "**Key Observations:**\n",
        "- **Strengths**: The model captures general feature ranges and some distributional patterns\n",
        "- **Weaknesses**: Some features show systematic bias or poor distribution matching\n",
        "- **Opportunities**: Advanced architectures (diffusion models, transformers) or better conditioning strategies could improve performance\n",
        "\n",
        "**For Your Assignment**: Consider these results as a baseline. Think about:\n",
        "- Which features are hardest to predict and why?\n",
        "- How could you modify the architecture or training process?\n",
        "- What additional constraints or domain knowledge could help?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.13.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
